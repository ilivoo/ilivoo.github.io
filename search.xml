<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux-shell-debug]]></title>
    <url>%2F2018%2F07%2F15%2Flinux-shell-debug%2F</url>
    <content type="text"><![CDATA[bash -x教程 设置PS4 1export PS4=&apos;($&#123;BASH_SOURCE&#125;:$&#123;LINENO&#125;): $&#123;FUNCNAME[0]&#125; - [$&#123;SHLVL&#125;,$&#123;BASH_SUBSHELL&#125;, $?]&apos; bashdb教程安装1sudo apt install bashdb 命令详解 原始代码 bashdb.sh sub_bashdb.sh bashdb.sh 12345#!/bin/bashecho &quot;hello world&quot;name=&quot;feng xiang&quot;age=28. sub_bashdb.sh $name $age sub_bashdb.sh 12345#!/bin/bashecho &quot;welcome &quot;name=&quot;wang pei&quot;age=27echo &quot;welcome&quot; &gt;&gt; hello.txt 断点 12345678910111213141516#断点, 指定行号或者当前行号break [LINENO]#瞬时断点, 指定行号或者当前行号， 断点一次后失效tbreak#条件断点, 断点编号， 断点条件condition BRKPT-NUM condition#删除一个或多个断点delete &#123;BRKPT-NUM&#125;...#清除某行所有断点clear LINENO#开启一个或多个断点enable BPNUM1 [BPNUM2 ...]#关闭一个或多个断点disable BPNUM1 [BPNUM2 ...]#显示断点info b[reakpoints] 12345678#继续执行到断点continue [LINENO | - ]#1. 执行到下一个断点continue#2. 执行到指定的行(首先给指定的行加零时断点，再执行continue命令)continue LINENO#关闭debug并执行脚本continue - 12345#前进1. 前进一步或者多步， 不进入方法next [COUNT]2. 前进一步或者多步， 进入方法或者source(.)命令执行的文件step [COUNT] 12#跳过一行或多行skip [COUNT] 1234567891011#执行完当前方法finish#从当前方法中返回return#向当前debug的进行发送信号kill [SIGNAL]signal SIGNAL#退出debuggerquit [EXIT-CODE [SHELL-LEVELS]]#重新运行或者指定重新运行的命令, 如果指定args必须为完整的命令， 不指定则使用原先的命令run [args] 查看变量 1234#打印出表达式值print EXPRESSIONprint $@ $name 123456#每次debug停止到一行的时候显示或者直接执行命令display [EXPRESSION]display $@ $name#删除display的命令undisplay NUM [NUM2 ...] 显示源代码, list显示的时候有个移动往下翻页的概念， 并记录当前浏览到了哪一个位置 12345678910list[&gt;] [LOC|.|-] [NUMBER]list . # 显示到当前栈的行list # 显示当前翻页的行前后list - # 显示当前翻页的行前面list&gt; . # 显示到当前栈的行list 10 3 # 显示10附近的三行 9-11list&gt; 10 3 # 显示是后面的三行 10-12list 10 13 # 显示10-13行list 10 -5 # 显示10行到倒数第五行 显示和设置当前debugger的设置信息 1show/set [param] 查看当前信息 123456789bashdb&lt;1&gt; infoInfo subcommands are: args display functions line signals stack watchpoints breakpoints files handle program source warranty #args 显示参数#display 显示display表达式#line 显示当前行号#breakpoints 显示所有断点... debug其它脚本， 并保留当前现场 12debug [SCRIPT]#不指定脚本， 执行当前行的命令 进入一个嵌套的shell， 不是子shell 12345678910shell [options]options: --no-fns | -F : don&apos;t copy in function definitions from parent shell --no-vars | -V : don&apos;t copy in variable definitions --shell SHELL_NAME --posix : corresponding shell option --login | l : corresponding shell option --noprofile : corresponding shell option --norc : corresponding shell option 执行一个命令 1eval CMD 9 设置环境变量 1export VAR1 [VAR2 ...]]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs架构]]></title>
    <url>%2F2018%2F04%2F29%2Fhadoop-hdfs-architecture%2F</url>
    <content type="text"><![CDATA[介绍 hdfs是被设计运行在廉价的机器上的分布式文件系统， 它和现有的文件系统有这很多相似之处， 然而也存在这最为重要的区别， hdfs具有高容错性并且有非常高的吞吐量， 用于处理大量的数据集， hdfs遵循posix使用流的方式访问存储在hdfs中的文件， 它最初设计目标是nurch搜索引擎的底层基础设施， 现在作为hadoop中重要的组成部分。 假设和目标 硬件的错误总是存在的， hdfs被设计运行在成百上千的机器上， 每一个存储都作为hdfs的一个部分，事实上如此大型的系统， 每一个组件都可能存在错误， 所以自动检测并恢复错误就是hdfs设计的核心目标。 从另一个方面来说， 分布式的存储远比单硬件提供持久化要可靠地多。 应用需要以流的形式访问数据， 它和通常的文件系统设计目标不同， hdfs被设计成批处理而不是与用户交互的处理， 它的主要目标是大的吞吐量而不是访问延迟。 应用运行在hdfs上都有非常大的数据集， 通常一个文件都是GB到TB级， 它应该提供高聚合的数据带宽并且在单个集群容易扩展上百个节点， 它应该能在单个实例中支持上亿个文件。 hdfs的文件需要一个一次写入多次读取的访问模型， 一个文件一旦创建、写入并且关闭， 那么它就不能改变， 除非append或者truncate， Append内容到文件的结尾被允许， 但是不能在任意位置修改， 这种简单的数据模型能够支撑非常高的吞吐量， MapReduce应用或者Web crawler应用相当适合这种模型。 移动计算比移动数据更加的廉价， 这意味着， 应用的计算数据与计算操作离得更近会更加有效， 这对于大型数据来说就非常正确了， 通过小量的网路带宽来增加系统的吞吐量， 这个假设是聚合计算到距离数据更近的节点上而不是移动数据， hdfs为应用提供了这种接口移动计算到距离数据更近位置的接口。 hdfs被设计成平台无关的， 并且非常简单的从一个平台迁移到另外一个平台。 NameNode和DataNodes]]></content>
      <categories>
        <category>hadoop</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F2018%2F04%2F28%2Ftools-git%2F</url>
    <content type="text"><![CDATA[概念 git存在工作区， 暂存区， 本地仓库， 远程仓库 工作区: 当前项目root目录 暂存区: 新文件通过git add添加， 或者已经关联的文件都在暂存区当中 本地仓库： 本地的仓库， ！，通过commit将暂存区中的内容提交到本地仓库 远程仓库： 远程的仓库， 通过push将本地的commit的版本push到远程 对于已经新添加的文件或者已经关联的文件修改， 都必须通过git add将其添加到暂存区当中， 再通过commit提交到本地，再通过push到远程仓库当中 git checkout123git checkout file 放弃工作空间中改变的文件git checkout branch 切换到指定分支, 可以添加 -b参数添加一个新的分支git checkout -b newbranch branch 从branch分支创建一个新的分支， 并切换到新的分支 git add1git add . 将工作空间中的改变文件添加到暂存区 git reset1234git reset用户回退一个已经commit但是并没有push的代码， 常见的参数： --hard --merge --mixed --patch --softgit reset --mixed version_hash 会保留源码， 只是将git commit和index信息回退到某个版本 --mixed是git reset的默认模式， git reset --mixed 等价于 git resetgit reset --soft 会保留源码， 只是回退到commit信息到某个版本，不涉及index的回退， 如果还需要提交直接提交就可以了git reset --hard 源代码也会回退到某个版本，commit和index都会回退到某个版本，如果没有指定版本那么就会放弃当前所有的更改回退当上一个commit 注意： 上面reset只是更改本地仓库的代码， 远程仓库代码还没有改变， 所以如果index发生变化， 而线上的index没有发生变化， 那么必定会导致不发进行push， 所以必须pull下来， 再进行push， 此时代码还是回变为线上index的版本， 所以总的来说这种方式不适合回退到线上index版本之前的， 它无法做到， 所以我们经常将其作为本地代码回滚的策略。 git revert12git revert 用于反正提交， 执行时必须要求工作目录必须是干净的git revert 用一个新提交来消除一个历史提交的所做的任何修改， 所以它的index不会回退， 而是继续向前， revert之后本地的代码也会回滚到指定的历史版本， 这个时候再push是不会发生任何冲突的 git reset/revert总结git reset是直接删除制定的commit（不会影响远程机器上的代码， index直接被删除）， git revert是用一次新的commit来回滚之前的commit（会影响远程机器上的代码， index会继续前进） git fetch [远程仓库 [远程仓库分支]]123git fetch origin mastergit merge origin/master， 此时可能需要解决冲突等等， 合并之后我们就可以在当前分支上工作并提交到远程分支如果不希望合并当前分支， 但是希望在远处分支上工作， 可以checkout出来一个新的分支git checkout -b newremotebranch origin/master 可以看到checkout可以直接从一个分支上创建一个新的分支，并不需要工作在origin/master分支上 注意： 下载远程分支， 但并不会合并远程分支到当前分支， 也就是说当前分支可能比下载下来的远程分支滞后几个分支， 所以如果我们希望在远处分支上工作， 那么我们必须将远程分支合并到当前分支 it remote某些场合下， 一个git项目需要同时使用两个甚至更多的远程仓库， 比如国外+国内、测试环境+生成环境+审核环境等等， 这样我们就需要对不同的远程仓库分别提交等操作。git remote 参数： add remove set-branches set-url update prune rename set-head show添加仓库， git remote add remote_name remote_url…..123456案例： 当我们发现github上面存在一个项目我们无法修改， 但是我们希望完全保留其所有的版本信息。操作步骤如下：a. git clone出项目到本地b. 在github上面创建一个空的仓库， 地址如下： https://github.com/ilivoo/spring.gitc. 在本地项目下直接删除原来的仓库， git remote remove remote_named. 将自己的远程仓库添加到项目当中， git remote add origin https://github.com/ilivoo/spring.gite. 将本地的项目直接上传到自己的远程项目当中， git push origin master 注意： 此处不仅仅限制与将其导入到github中， 也可以导入到国内的码云当中， 而且推荐使用这种方式。 并且码云当中还有一个更好的办法， 来完成这个动作， 直接在码云当中创建一个仓库， 创建过程中直接克隆github上的项目， 这样就是一步到位， 而且速度非常快， 再直接从码云中clone到本地， 关机是码云中还可以将其设置为private另外： git存在fork的概念， 可以直接fork项目再自己对其进行修改， 类似上面的操作 git branch123git branch branchName 创建一个新的分支git checkout branchName 切换到新的分支git checkout -b branchName 创建并切换到一个新的分支 git checkoutcheckout出某个分支， 注意如果此时暂存区还有修改没有提交， 如果此时checkout的某个分支可以合并， 那么可以切换到对应的分支， 如果有冲突， 那么此时不允许切换到某个分支， 所以在我们切换分支的时候最好是将工作空间清空。 git merge合并两个分支， 此时被合并的分支必定是commit状态， 而当前分区可以是未commit状态， 但是如果此时合并时存在冲突， 那么此时是不能merge成功的， 所以此时必须要对当前分支commit再合并， 合并后可能存在冲突， 此时将冲突解决就可以了， 重新提交。 也可以在合并冲突之后使用命令git mergetool来调用mergetool来处理冲突问题， 所以可以设置当前mergetool, 如p4merge1234567891011121314C:\Users\ilivoo&gt;cat .gitconfig[user] name = ilivoo email = 316488140@qq.com[merge] tool = p4merge[mergetool &quot;p4merge&quot;] cmd = p4merge \&quot;$BASE\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$MERGED\&quot;[mergetool] trustExitCode = false[diff] tool = p4merge[difftool &quot;p4merge&quot;] cmd = p4merge \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; 注意： 合并是先执行git merge， 合并可能没有成功存在冲突， 此时再调用git mergetool自动调用合并工具来手动合并， 而git difftool则是直接使用就可以, 参数可以添加任意两个版本如果不想合并， 那么可以直接通过 git merge –abort取消当前合并 git clean12345678910# 删除 untracked filesgit clean -f# 连 untracked 的目录也一起删掉git clean -fd# 连 .gitignore文件中untrack 文件/目录也一起删掉 （如：idea生成的.idea，日志logs等等）git clean -xfd# 在用上述 git clean 前，墙裂建议加上 -n 参数来先看看会删掉哪些文件，防止重要文件被误删git clean -nxfd# 注意： 真正的删除是参数f d x， n只是用来查看当前参数可能会删除的那些文件， 如：git clean -nf查看那些未跟踪的文件会被删除， 但是并不会真正执行删除， git clean -f 才是真正删除未跟踪的文件 git 设置代理12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; git 设置无密码提交 使用保存密码方式 git config –global credential.helper store .gitconfig文件中会多出 [credential]helper = store 当输入过一次密码之后会在.gitconfig文件的相同目录下生成.git-credentials https://ilivoo:password@github.com 使用秘钥方式 添加秘钥到github当中 修改.git/config配置文件 123[remote &quot;origin&quot;]url = git@github.com:ilivoo/project.gitfetch = +refs/heads/*:refs/remotes/origin/*]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql_log]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-log%2F</url>
    <content type="text"><![CDATA[mysql 日志 主要包括： 错误日志、查询日志、慢查询日志、事务日志、二进制日志、回放日志 错误日志： 错误日志是用来记录mysql系统在运行的过程中所发生的错误， 如启动过程中存在的问题， 系统在配置主从是报的错误等待 错误日志默认是开启的，并且错误日志无法被禁止， 默认情况下错误日日志存储在mysql数据库的数据文件目录当中， 以主机名作为名字， 并且以.err结尾， 一般情况下 建议更改日志的名字，这样在统一维护的时候会更加方便 配置方式: log_error=error.log 查询日志 查询日志是用来记录用户的所有操作， 其中包括增删改查等所有信息 查询日志默认是关闭的， 一般如果不是为了调试数据库都不会开启查询日志， 会产生大量的磁盘io 配置方式： general_log=log.log 慢查询日志 慢查询日志是用来记录执行时间超过指定的时间的查询语句， 通过慢查询可以找出运行效率低下需要优化的语句 慢查询默认关闭的， 一般它对服务器的性能微乎其微， 所以建议线上根据需要开启 配置方式 123slow_query_log=onslow_query_log_file=slow.loglong_query_time=0.000001 手动开启： 12set global slow_query_log=on;set global long_query_time=0.000001; 可以看到虽然这里设置了， 但是严格它对于当前的session是无法生效的， 如果仅仅只是想对当前session生效， 可以设置， 12set global slow_query_log=on;set long_query_time=0.000001; 如果想对所有的连接有效， 包括已经启动的连接和后面加入的连接有效， 可以设置， 12set global slow_query_log=on;set global long_query_time=0.000001; 查看当前session的配置情况 1select * from performance_schema.variables_by_thread as a, (select THREAD_ID,PROCESSLIST_ID,PROCESSLIST_USER,PROCESSLIST_HOST,PROCESSLIST_COMMAND,PROCESSLIST_STATE from performance_schema.threads where PROCESSLIST_USER&lt;&gt;&apos;NULL&apos;) as b where a.THREAD_ID = b.THREAD_ID and a.VARIABLE_NAME = &apos;long_query_time&apos;; 可以查看所有的连接是否生效， 再通过kill PROCESSLIST_ID来杀手不生效的连接（一般都是使用连接池来连接的所以问题不大）， 也可以通过笨的方法， 对每一个现有的连接执行一次 注意： 通过这里设置全局变量我们可以看到， mysql设置全局变量未必会对现有的连接生效， 所以我们如果想要进行验证， 最简单的办法就是通过上面的语句进行查询。 设置线上慢查询日志时最好指定毫秒数， 这样可以用于长期线上开启 事务日志 事务日志（InnoDB特有的日志）可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把改修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。事务日志采用追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢的刷回到磁盘。目前大多数的存储引擎都是这样实现的，我们通常称之为预写式日志，修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。 1234innodb_flush_log_at_timeout=1 事务日志如果没有每一次从缓冲区中刷新到磁盘， 那么每隔一秒刷新一次innodb_flush_log_at_trx_commit=1 为1表示事务一提交就刷新到磁盘中， 为2表示只有在事务提交时才同步， 有可能丢失整个事务innodb_log_files_in_group=2 至少两个日志组innodb_log_group_home_dir=./ 日志组的位置 二进制日志 二进制日志是MySQL服务器用来记录数据修改事件的，比如INSERT、UPDATE、DELETE等会导致数据发生变化的语句，SELECT语句不会被记录在内。MySQL必须先执行完一条语句才能知道它是否修改了数据，因此写入二进制日志文件的时间是语句执行完成的时间。写入顺序是按语句执行完成的先后顺序，事务中的语句会先被缓存起来，成功提交后才会被写入，回滚则不会被写入。非事务的存储引擎，所有的修改会立刻写入到二进制日志中。 二进制日志顾名思义不是文本而是一种更有效率的二进制格式，它比文本占用更少的空间，但是可读性就很差了，必须使用mysqlbinlog工具才能转换为可读的文本。二进制日志主要用于数据库备份和故障时恢复数据，配置MySQL主从复制必须启用二进制日志。 二进制日志索引文件中会列出所有二进制日志文件，此文件是文本的因此可以直接查看，文件的最后一行就是当前正在使用的二进制日志文件。 配置 12345678910111213141516log_bin=onlog_bin_basename=bin.log //指定bin log的名字， 后面会自动生成xxxxx的数字log_bin_index=bin.log.index //所有bin log文件名的索引// 通过flush logs会导致当前bin log写完， 并刷新出一个新的log// 并且在 index中记录这个文件的名字binlog_do_db //表示只记录指定数据库的二进制日志binlog_ignore_db //表示不记录指定的数据库的二进制日志//不建议使用 do/ignore来进行控制, 安全的替换方案是 在 slave上配置过滤,//使用基于查询中真正涉及到的表的选项, 例如replicate-wild-* 选项replicate-wild-do-table=ljzxdb.%sync_binlog //直接影响mysql的性能和完整性sync_binlog=0：当事务提交后，Mysql仅仅是将binlog_cache中的数据写入Binlog文件，但不执行fsync之类的磁盘 同步指令通知文件系统将缓存刷新到磁盘，而让Filesystem自行决定什么时候来做同步，性能最好。sync_binlog=n，在进行n次事务提交以后，Mysql将执行一次fsync之类的磁盘同步指令，同志文件系统将Binlog文件缓存刷新到磁盘。binglog_format //值有statement（记录逻辑sql语句）、row（记录表的行更动情况）、mixed混合使两种，// row的存在是因为存在很多不确定性的函数， 如果直接使用， 会导致数据不一致， 如 current_time(), 所以直接将值记录下来就更正确 回放日志 bin log从master发送到slave的时候， slave就变成了reply log， 用于回放master中数据发生的变化]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql索引导出]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-index-dump%2F</url>
    <content type="text"><![CDATA[打印删除所有的索引的语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印删除所有的索引的语句，不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; and UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印出所有的所有创建语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印出所有的创建语句， 不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印自增的所有创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)),&apos;) UNSIGNED NOT NULL AUTO_INCREMENT;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印删除自增的创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)), &apos;) UNSIGNED NOT NULL;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印没有主键的表 1SELECT DISTINCT t.table_schema, t.table_name FROM information_schema.tables AS t LEFT JOIN information_schema.columns AS c ON t.table_schema = c.table_schema AND t.table_name = c.table_name AND c.column_key = &quot;PRI&quot; WHERE t.table_schema NOT IN (&apos;information_schema&apos;, &apos;mysql&apos;, &apos;performance_schema&apos;) AND c.table_name IS NULL AND t.table_type != &apos;VIEW&apos;;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks代理]]></title>
    <url>%2F2018%2F04%2F28%2Flinux-shadowsocket%2F</url>
    <content type="text"><![CDATA[proxychain 安装proxychain， 也可以直接使用apt安装 123456git clone https://github.com/rofl0r/proxychains-ng.gitcd proxychains-ng./configuremake &amp;&amp; make installcp ./src/proxychains.conf /etc/proxychains.confcd .. &amp;&amp; rm -rf proxychains-ng 配置proxychain 123sudo vi /etc/proxychains.conf# 将socks4 127.0.0.1 9095改为socks5 127.0.0.1 1080 //1080改为你自己的端口 使用proxychain 1proxychains chrome #在需要进行代理的程序前加proxychains 确保本地的代理已经打开 shadowsocks 安装shadowsocks, apt直接安装可能版本太低无法支持更多的加密算法 12sudo apt-get install python-pipsudo pip install shadowsocks 配置shadowsocks 123456789101112sudo vi /etc/shadowsocks.json&#123; &quot;server&quot;:&quot;服务器地址&quot;, &quot;server_port&quot;:服务器端口号, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;密码&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;加密方式&quot;, &quot;fast_open&quot;: true, #需要服务端开启 &quot;workers&quot;: 1&#125; 启动shadowsocks， 可以直接配置成开机启动启动 1sudo sslocal -c /etc/shadowsocks.json 开启服务端开启fast_open 1https://github.com/shadowsocks/shadowsocks/wiki/TCP-Fast-Open SwitchyOmega 安装SwitchyOmega， chrome如果没有代理可能无法访问商店， 可以开启shadowsocks并使用proxychain代理chrome再安装 配置SwitchyOmega 1https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList shadowsocks服务端搭建 建议使用搬瓦工， 并选择手机运营商对应的直连机房]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux安装配置]]></title>
    <url>%2F2018%2F04%2F28%2Flinux-ubuntu-setup%2F</url>
    <content type="text"><![CDATA[联想r720安装ubuntu 安装的时候一定要使用网络安装， 用iphone提供网络， 安装网卡驱动 安装完成后更改显卡驱动为最新的 123sudo add-apt-repository ppa:xorg-edgers/ppa #添加ppa源sudo add-apt-repository ppa:graphics-drivers/ppa #添加ppa源sudo apt-get update #更新apt-get 然后进入：系统设置-&gt;软件和更新-&gt;附件驱动，选择更新的显卡驱动。 安装bumblebee自动管理集显和独显 设置nvidia-settings-&gt;PRIME Profiles-&gt;Intel(Power Save Mode) 安装bumblebee 和 bbswitch电源管理 1234sudo add-apt-repository ppa:bumblebee/testingsudo apt updatesudo apt install bumblebee-nvidia bumblebee primussudo apt-get install acpidump iasl dmidecode 配置/etc/bumblebee/bumblebee.conf 1234Driver=nvidiaKernelDriver=nvidia-396LibraryPath=/usr/lib/nvidia-396:/usr/lib32/nvidia-396XorgModulePath=/usr/lib/nvidia-396/xorg,/usr/lib/xorg/modules 检测安装 1optirun --status 显示Bumblebee status: Ready (3.2.1). X inactive. Discrete video card is off.就没什么问题 打开nvidia设置 1optirun -b none nvidia-settings -c :8 测试GPU性能 123sudo apt-get install phoronix-test-suitephoronix-test-suite benchmark pts/gputestoptirun phoronix-test-suite benchmark pts/gputest 测试发现使用GPU和不使用的区别就表示完全通过测试 设置desktop文件帮助网站bumlebee、csdn、arch)、FAQ 安装软件使用依赖管理工具, 安装软件更加简单 12sudo apt-get install gdebisudo gdebi netease-cloudmusic_1.0.0_amd64_ubuntu16.04.deb 也可以使用dpkg安装软件 1234567891011# 使用dpkg安装.deb格式文件，这里会提示缺少依赖文件sudo dpkg -i name.deb# 这里使用 apt-get install -f 安装上面一条指令的的依赖文件sudo apt-get install -f# 依赖文件已经安装好了，重新安装一遍sudo dpkg -i name.deb# 如果卸载 -P=--purge, -r=--removeesudo dpkg -P/-r name 安装ubuntu-restricted-extras 获取字体 1sudo apt-get install ubuntu-restricted-extras ubuntu-restricted-addons 配置快捷键 123sudo apt-get install compizconfig-settings-managersettings -&gt; keyboard -&gt; shortcuts#仅仅配置shortcuts有时候可能不生效，需要使用compizconfig来修改 最终快捷键， 保留几个常用的 12345678910Ctrl + Alt + t 打开终端Ctrl + Super + up 放大应用Ctrl + Super + down 缩小应用Super + d 关闭/显示桌面Super + l 锁住桌面Super + w 显示打开的应用Super 显示左边栏并打开搜索Super + Tab 显示左边栏并上下切换打开应用Alt [+ SHift] + [向前]向后切换应用Alt + F4 关闭应用 oh-my-zsh 安装zsh 12sudo apt-get install zsh #安装zshchsh -s /bin/zsh #不需要sudo，重启 安装oh-my-zsh 1https://github.com/robbyrussell/oh-my-zsh 安装zsh插件 123456789vi-modeautojump 需要安装软件zsh-autosuggestions https://github.com/zsh-users/zsh-autosuggestionslast-working-dir d 在终端输入d 显示最近频繁进入的路径，然后输入路径前对应的序号可快速进入对应路径history 用法：在终端输入h即可zsh-syntax-highlighting https://github.com/zsh-users/zsh-syntax-highlightingweb-search 如：google StackOverflow， 指定搜搜引擎就可以了sublime 需要安装软件， [s]st [|文件|文件夹] [管理员]打开 [当前目录|文件|文件夹] vim 安装vim 1https://github.com/amix/vimrc 插件快捷键讲解 12345ack.vim 内容搜索 &lt;leader&gt; + gctrlp.vim 文件搜索 &lt;leader&gt; + jbufexplorer.zip 缓存查看 &lt;leader&gt; + omru.vim 最近文件 &lt;leader&gt; + fNERD Tree 文件目录 &lt;leader&gt; + nn tmux 安装tmux 1https://github.com/gpakosz/.tmux 设置tmux 1234567set-option -g allow-rename off每一次窗口执行不同的命令窗口的名字就会发生变化， 此处是关闭自动命名set-window-option -g pane-base-index 1设置窗口pane的下标从1开始# on Linux, this requires xsel or xcliptmux_conf_copy_to_os_clipboard=false设置tmux剪切板与系统剪切板同步 使用tmux tmux使用来管理回话并提供分屏等功能的软件， 当tmux开启时， 就会启动一个tmux-server的程序，用来管理上面所有的session会话， 每个session会话中可以存在多个windows（窗口）， 每个窗口又存在多个panel（面板） tmux命令 123456789tmux + tab 列出所有tmux可以使用的命令tmux new-session -s name 创建sessiontmux list-session 查看sessiontmux attach-session name attach到某个sessiontmux kill-session name 杀死某个sessiontmux info 查看session, window, pane, 运行的进程号tmux list-keys 列出所有可以的快捷键和其运行的 tmux 命令tmux list-commands 列出所有的 tmux 命令及其参数tmux kill-server 关闭所有 session 注意 所有的上面这些命令在没进入某个session之前直接使用tmux command args来调用， 当进如了tmux之后直接在任何的panel中通过 &lt;pre&gt; + : +command args来激活命令行操作， 此时所有的命令只需要使用 command args就可以调用了 tmux 基本操作 1234567? 列出所有快捷键；按q返回d 脱离当前会话,可暂时返回Shell界面s 选择并切换会话；在同时开启了多个会话时使用: 进入命令行模式；此时可输入支持的命令，例如 kill-server 关闭所有tmux会话, rename 重命名当前session[ 复制模式，光标移动到复制内容位置，空格键开始，方向键选择复制，回车确认，q/Esc退出， 可以设置vi模式复制] 进入粘贴模式，粘贴之前复制的内容，按q/Esc退出t 显示当前的时间 tmux 窗口操作 12345678c 创建新窗口&amp; 关闭当前窗口[0-9] 数字键切换到指定窗口ctrl + h/l 左右切换窗口（安装ctrl， 并快速的按h/l）w 通过窗口列表切换窗口, 重命名当前窗口，便于识别. 修改当前窗口编号，相当于重新排序f 在所有窗口中查找关键词，便于窗口多了切换 tmux 面板操作 123456789101112131415&quot;/- 将当前面板上下分屏%/_ 将当前面板左右分屏x 关闭当前分屏! 将当前面板置于新窗口,即新建一个窗口,其中仅包含当前面板z 最大化当前所在面板ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小H/J/K/L 以1个单元格移动边缘以调整当前面板大小q 显示面板编号o 选择当前窗口中下一个面板方向键/h/j/k/l 移动光标选择对应面板&#123; 向前置换当前面板&#125; 向后置换当前面板alt+o 逆时针旋转当前窗口的面板ctrl+o 顺时针旋转当前窗口的面板 tmux 滚屏操作，由于tmux接管如果输出大量的数据无法看到上面屏幕的字， 如果此时鼠标操作关闭，那么当使用鼠标滑轮的时候显示的执行过命令的显示， 而不能屏幕翻页， 要想达到这个目的有两个办法： 12打开鼠标操作， 使用滑轮上下翻页 &lt;pre&gt; + m打开复制模式， 使用vi的翻滚策略来操作， 此时又会碰到一个问题， 如果前缀是ctrl + b那么不好往下整页翻滚， 建议使用半屏翻滚 ctrl + u / d(f) tmuxinator 安装tmuxinator 1234567891011121314https://github.com/tmuxinator/tmuxinatorsudo apt-get install ruby-fullsudo gem install tmuxinator设置编辑器， 由于tmuxinator编辑项目$EDITOR = vimtmuxinator默认的存放项目位置~/.config/tmuxinator配置zsh命令简写和自动完成 TAB键a. 在.zshrc中添加source ~/.bin/tmuxinator.zshb. 拷贝自动完成脚本cp ~/.bin/tmuxinator.fish ~/.config/fish/completions/注意：如果不存在~/.bin目录，那么自己下载到这个文件夹文件路径 https://github.com/tmuxinator/tmuxinator/tree/master/completion/ 基本命令 1234567tmuxinator new/edit/open project 创建/编辑/打开项目， 可以使用n/e/o简洁表示tmuxinator start [PROJECT] [ARGS] 启动一个项目tmuxinator stop [PROJECT] 停止一个项目tmuxinator list 显示所有项目tmuxinator delete [PROJECT1] [PROJECT2] 删除一个或者多个项目tmuxinator copy [EXISTING] [NEW] 复制一个项目tmuxinator debug [PROJECT] [ARGS] debug一个项目 示例 123456789101112131415161718192021222324252627282930# 项目名称name: sample# 项目主目录root: ~/# 所有的窗口创建之前执行的命令pre_window: echo &quot;hello world&quot;# 所有的windowswindows: # windows 1 editor, 指定名字为myeditor - editor: myeditor # 指定当前window的主目录 root: ~/projects/editor # layout 指定当前window的排版方式， 默认有5种 # 也可以自己定制， 方法在下面 layout: main-vertical # 当前windows下所有的panes， 使用layout指定的排版方式排版 panes: # 面板一， 此面板只有命令vim， 所以这样写就可以了 - vim # 面板二， 此面板存在多一个命令， 将多个命令回车添加 # 必须上一个命令执行成功才会执行下一个命令 - logs: - ssh logs@example.com - cd /var/logs - tail -f development.log # window 2， 可以看到单个窗口一个命令， 直接写就可以了 - server: bundle exec rails s # window 3， 同上 - logs: tail -f log/development.log 使用环境变量 1root: &lt;%= ENV[&quot;MY_CUSTOM_DIR&quot;] %&gt; 使用参数传递 位置参数 123tmuxinator start project foo# 此处的args[0]就是fooroot: ~/&lt;%= @args[0] %&gt; 命名参数 123tmuxinator start project workspace=workspace/todo# 此处的workspace 就是上面设置的值workspace/todoroot: ~/&lt;%= @settings[&quot;workspace&quot;] %&gt; tmux默认5中layout 12345even-horizontaleven-verticalmain-horizontalmain-verticaltiled tmux自定义layout 打印layout 12$ tmux list-windows1: bash* (4 panes) [211x47] [layout 9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125;] @3 (active) 截取layout 19a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125; 配置layout 1layout:9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125; 安装fusuma控制触摸板 安装 1234sudo gpasswd -a $USER input #需要管理员权限执行，配置开机启动也可以不用sudo apt-get install libinput-toolssudo apt-get install xdotoolgem install fusuma #使用ruby安装 配置 12345#确保开启多点触控gsettings set org.gnome.desktop.peripherals.touchpad send-events enabled#创建配置文件mkdir -p ~/config/fusum &amp;&amp; touch ~/.config/fusuma/config.yml#vi ~/.config/fusuma/config.yml 1234567891011121314151617181920212223242526272829303132swipe: 3: left: command: &apos;xdotool key alt+Left&apos; right: command: &apos;xdotool key alt+Right&apos; up: command: &apos;xdotool key ctrl+t&apos; down: command: &apos;xdotool key ctrl+w&apos; 4: left: command: &apos;xdotool key super+Left&apos; right: command: &apos;xdotool key super+Right&apos; up: command: &apos;xdotool key super+a&apos; down: command: &apos;xdotool key super+s&apos;pinch: in: command: &apos;xdotool key ctrl+plus&apos; out: command: &apos;xdotool key ctrl+minus&apos;threshold: swipe: 1 pinch: 1interval: swipe: 1 pinch: 1 配置开机启动， 参考下面的shadowsocks配置 安装网易云音乐 安装 123sudo apt-get install gdebisudo gdebi netease-cloudmusic_1.0.0_amd64_ubuntu16.04.debsudo apt-get install sqlite3 配置 123netease-cloud-music.desktopExec=netease-cloud-music --no-sandbox %U 中在%U 前面添加--no-sandbox 即可解决cd ~/.cache &amp;&amp; sudo chmod -R 777 netease-cloud-music wifi驱动安装后无法启动 查看网络信息 12345678910111213$ rfkill list 0:ideapad_wlan: Wireless LAN Soft blocked: no Hard blocked:yes 表示硬件阻塞没有开启 1:ideapad_bluetooth: Bluetooth Soft blocked: no Hard blocked: yes 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes 表示软件阻塞没有安装驱动 Hard blocked: no 移除ideapad模块， 后后面的没有阻塞的模块可以启动 12345678$ modprobe -r ideapad_laptop$ rfkill list 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes Hard blocke 配置开机启动禁止ideapad模块 12创建/etc/modprobe.d/my_blacklist.conf文件block掉ideapad模块，这样没有重启就不用手动blockblacklist ideapad_laptop 开机自动启动方式一 以systemctl创建shadowsockets开机启动为例子 1sudo vim /etc/systemd/system/shadowsocks.service 编写文件类容 123456789[Unit]Description=Shadowsocks Client ServiceAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/bin/sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json[Install]WantedBy=multi-user.target 把/home/xx/Software/ShadowsocksConfig/shadowsocks.json修改为你的shadowsocks.json路径，如：/etc/shadowsocks.json 生效配置 1sudo systemctl enable /etc/systemd/system/shadowsocks.service 方式二 直接启动Startup Applications程序， 填写需要执行的命令和参数 系统问题修复 查看dmesg日志， 并且google错误信息 1sudo dmesg 查看系统日志 系统卡死现象 12Ctrl + Alt + F1 ~ F6 进入tty1 ~ tty6模式Ctrl + Alt + F7 恢复到桌面模式 idea快捷键冲突问题 设置fcitx 网易云音乐 更改系统快捷键， 不常用的建议都去掉 安装compizconfig-settings-manager继续修改快捷键 关闭关闭tty防止ctrl + alt + F1-12的影响 1234# sudo vi /usr/share/X11/xorg.conf.d/50-novtswitch.confSection &quot;ServerFlags&quot; Option &quot;DontVTSwitch&quot; &quot;true&quot;EndSection 配置idea 博客]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2018%2F04%2F26%2Fhadoop-map-reduce%2F</url>
    <content type="text"><![CDATA[概述 MapReduce是一个简单的计算处理框架, 它能够在一个廉价的大型集群上以一种容错的方式并行的处理TB级的数据。 一个MapRecude任务通常讲输入数据集进行分块, 并且以完全并行的方式处理map任务, 框架对map任务输出进行。排序, 并作为reduce任务的输入, 通常任务的输入和输出都存储在文件系统, 框架负责处理协调、监控并重运行失败的任务。 通常情况下计算节点和存储节点都是在同一个HDFS(如果MapReduce以HDFS作为文件系统)的datenode中， 这种配置允许框架能够有效的协调任务在已经存在数据的节点中运行，这通常在集群中对网络带宽非常友好。 MapReduce也可以通过yarn（分布式资源协调工具）来运行， 并且为每一个MapReduce任务创建一个MRAppMaster负责从yarn的ResourceManager申请协调资源和对MapReduce任务进行监控和管理。 最小的一个MapReduce任务由一个输入输出、实现特定接口的map和reduce方法、以及一个job配置， job客户端提交任务， ResouceManager对任务进行协调监控并提供诊断信息给客户端。 虽然Hadoop框架是使用java运行， 但是MapReduce应用却不要求一定使用java编写， 如Hadoop Streaming允许用户使用任何可执行的shell（能够在shell命令中执行） 创建并运行mapper或者reducer， Hadoop Pipes以一个SWIG兼容的C++ API来实现MapReduce任务（而不是基于JNI） 输入和输出 MapReduce框架操作唯一的&lt;key, value&gt;对，框架输入&lt;key, value&gt;对， 并产生&lt;key, value&gt;对作为输出，key和value需要在框架中通过RPC或者文件的方式进行传输， 所以必须序列化， 框架提供Writable接口作为序列化的基础， 另外框架需要通过key来对&lt;key, value&gt;对进行排序， 所以必须实现WritableComparable接口， 虽然key没有要求必须实现hashcode方法， 但是map任务后的reduce任务通常需要对&lt;key, value&gt;行partition， 而通过hash来进行partition却是默认的方式。 简单的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; //创建单实例的对象， 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; //输入的key其实是一个相对于文件的偏移量， value为每一行的数据 //所以读取文件必须有一个格式， 并且讲文件split给每个map任务，这里由InputFormat提供 StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one);//将split的单词和数量输出&lt;workd, count&gt; &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, "word count"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); //指定Combiner进行本地Reducer任务执行， 主要是这里的reducer任务具有操作无序性质， 所以 //本地的reducer一般用来减小mapper到reducer的带宽， 对于速度上可能并不明显 job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //等待任务完成， 返回job客户端， 也可以执行返回 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 运行应用1$ bin/hadoop jar wc.jar WordCount /user/joe/wordcount/input /user/joe/wordcount/output 为MapReduce任务提供ClassPath和work dir提供资源1$ bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -files dir1/dict.txt#dict1,dir2/dict.txt#dict2 -archives mytar.tgz#tgzdir input output 说明： -files 提供文件到任务work dir中， 通过逗号分割， 使用#号取别名, -archives 提供文件并且解压缩到work dir， 通过都好分割， 使用#号取别名， 两者指定的文件可以在MapReduce任务中获取 Mapper Hadoop MapReduce通过InputFormat将输入的files进行split并创建一任意个InputSplit，并为每个InputSplit创建一个map任务用来执行这个InputSplit表示的逻辑文件， map任务通过RecordReader来读取InputSplit这个逻辑文件， 并生成map的输入对&lt;key1, value1&gt;, InputSplit和RecordReader都是由InputFormat创建。 Mapper类是执行map任务的核心， 它循环的执行RecordReader产生的&lt;key1, value1&gt;对， 并且在执行任何的&lt;key1, value1&gt;对之前可以对Mapper进行初始化操作，如下： 1234Mapper.setup(Context)Mapper.run(Context) # 用于提供map操作的模板方法， Mapper调用的方法Mapper.map(key, value, Context)Mapper.cleanup(Context) MapReduce 输出对不需要和输入对有任何的联系， 输入对可以map出零个或者多个输出对， 通过context.write()输出。 用户可以指定combiner对mapper产生的输出进行聚合， 这样可能会加快速度和减少网络带宽， 其实combiner就是一个reducer， 只是它单纯的对当前map任务的所有输出进行reduce任务操作， 操作完成之后再传给总的reduce来进行处理， 所以可以看到此处的reduce任务必须具有无序性， 其次reducer也可以接受并非只是mapper的中间结果&lt;key2, value2&gt;, 也可以接受&lt;key2, [value2, vlaue3…]&gt; map任务的个数通常是通过输入文件的总大小，也就是总的输入文件的块， 正确的并发map任务通常每个节点10-100个， 通常对于轻量级cpu任务开启300个map任务， 因为一般情况下map需要从磁盘获取数据， 并进行简单操作， 所以通常都是io密集型而对cpu不敏感， map任务的创建需要时间， 所以最好map任务执行超过一分钟。 Partitioner Partitioner是用map产生的中间结果&lt;key1, value1&gt;的key空间来对每个map任务的的中间结果进行分区的， 通常情况下是通过hash来进行分区， 总的分区数量和reduce任务的分区数量是一样的。 ReducerReducer 用来reduce Mapper任务产生的中间结果， 当Mapper产生结果后， 通过Partitioner将具体&lt;key2, value2&gt;或者是&lt;key2, [vlaue2, value3…]&gt;(经过combiner之后的结果)传输到具体的Reducer当中， 具体的某个Reducer收到所有Mapper需要传输到自己的&lt;key2, value2/[value2, value3…]&gt;之后， 对其进行shuffle、sort和最终的reduce Shuffle 讲所有从Mapper中获取的结果进行shuffle成&lt;value2, [value2, value3…]&gt; Sort 对shuffle结果进行排序，shuffle和sort是同时进行的， 排序方式可以通过Job.setSortComparatorClass(Class)来提供， 此处是对key进行排序 Secondary Sort 如果需要对相同key的values进行排序可以指定Job.setGroupingComparatorClass(Class)来对value进行排序 Reduce 对当前Reducer收到并处理的&lt;key2, [value2, value3…]&gt;进行reduce操作， 用来产生&lt;key3, value3&gt;输出到文件系统中，reduce输出没有排序。reduce在操作之前和之后可以和map进行相同的初始化或者结束回调。 reduce的数量通常在[0.95~1.75] 之间， 当maps任务执行完后， 指定倍数为0.95时所有的reducers可以立即启动并且执行传输map的输出， 当为1.75时最快的节点将会完成它们第一轮并且执行第二轮， 有助于负载均衡。 增加的reduces的数量虽然增加了负载， 但是增加了负载均衡的能力， 并且减小了失败造成的消耗。 CounterCounter 是一个用来报告统计的基础设施， Mapper和Reducer实现可以使用Counter来报告统计信息。 Job Configuration Job代表一个MapReduce的任务配置， 通常用来配置InputFormat、Mapper、combiner（如果有）、Partitioner、Reducer、OutputFormat的实现 FileInputFormat和FileOutpuFormat配置输入和输出格式 12FileInputFormat.setInputPaths(Job, Path…) FileInputFormat.addInputPath(Job, Path)FileInputFormat.setInputPaths(Job, String…) FileInputFormat.addInputPaths(Job, String) Job通常需要指定先进的参数， 如Comparator、文件put到DistributedCache、是否使用compressed、是否speculative执行以及map和reduce最大的attempts 当然Job还可以使用Configuration的get/set来获取和设置更多的参数1Configuration.set(String, String) 任务执行和环境变量 MRAppMaster使用隔离jvm的一个子进程去执行Mapper/Reducer任务， 子进程继承父进程的环境变量， 用户可以指定额外的子进程虚拟机启动参数，如下：12345678910111213141516&lt;!-- @taskid@ 表示MapReduce任务id--&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; 任务提交和监控 Job是一个原始的接口， 用于用户job与ResourceManager交互， Job提供的基本功能是提交任务、跟踪任务进程、访问任务的报告和日志和获取MapRecude集群状态等信息， job提交任务涉及到下面几个方面： 检查任务的输入输出 计算InputFormat的InputSplit 为任务分布式缓存设置必要的账号信息 复制任务jar和配置信息到MapReduce系统目录 提交任务到ResourceManager并选择是否监控它的状态 任务历史日志记录在两个指定的目录当中， 如下： 12mapreduce.jobhistory.intermediate-done-dirmapreduce.jobhistory.done-dir 用户可以查看任务运行历史日志总览 1$ mapred job -history [all] output.jhist Job 输入 InputFormat 描述MapReduce任务的输入，主要功能如下： 验证任务输入 split 所有的输入文件为逻辑的InputSplit实例， 每一个InputSplit相当于一个Mapper任务 提供RecordReader实现， 用于Mapper任务启动时从InputSplit中获取单个&lt;key1, value1&gt;记录 默认基于文件的输入是FileInputFortmat， 它通过总文件大小来split出逻辑的InputSplit， 显然通过总大小来分割有时候是无效的， 因为RecordReader无法获取记录的边界 Job输出 OutputFormat 描述MapReduce任务的输出， 主要功能如下： 验证任务输出， 如：检查输出文件是否存在 提供RecorderWriter实现， 用于写入任务输出， Output文件存储到文件系统 OutputCommiter 描述MapReduce任务输出提交， OutputCommiter的主要功能如下： 初始化设置， 如：为任务创建临时输出目录， 任务的初始化设置是在单独的任务中完成的， 初始化之前任务处在PREP状态， 初始化完成后处在RUNNING状态 任务完成后的清理动作， 如：删除零时输出目录， 任务的清理工作也是在单独的任务中， 清理工作完成后任务可能出现SUCCEDED/FAILED/KILLED状态 检查任务是否需要提交 一旦任务完成就提交任务的输出 如果任务失败， 任务输出将会被清理， 并且丢弃任务提交， 如果任务不能清理， 一个相同attempt-id的任务将会单独启动并执行清理动作 RecordWriter 负责讲Reducer的输出&lt;key3, value3&gt;输出到文件系统， 并且负责讲job的outputs输出到文件系统 提交Job到队列 用户提交任务到队列当中， 队列作为一个任务集合允许系统提供指定的功能， 如队列使用ACLs控制那个用户可以提交任务到队列， 在Hadoop中默认就是使用队列作为调度器来调度任务。 Hadoop中存在一个叫做default的队列用于用户默认的任务提交队列， 用户也可以通过Configuration.set(MRJobConfig.QUEUE_NAME, String)指定将任务提交到特定名字的队列当中， 并且Hadoop还提供了一个可插拔的Capacity Scheduler用来支持多租户， 提供资源的隔离， 所以它存在多个队列。 计数器 Counters代表全局的计数器， 可以被MapReduce框架或者应用定义， 每一个Counter可以是一个Enum， 特定的Enum的Counter被集中到特定的Counters.Group当中， 应用可以定义任意的Enum并且Counter.incrCounter(Enum, long)或者Counter.incrCounter(String, String, long)到map或者reduce方法当中， 这些counters被框架自动聚合。 分布式缓存 分布式的缓存用于有效的存储特定应用大型只读的文件， 它是被MapReduce框架提供的基础设施用于缓存应用需要的文本、归档和jar文件等等 Job中的应用通过hdfs://指定被缓的文件， 分布式的缓存假设被指定hdfs://的文件已经存在于文件系统当中 框架将会在任何任务被执行之前，复制必须的文件到worker节点， 事实上它是相当有效的， 文件只会被复制一次， 并且un-archive到work节点 分布式的缓存跟踪缓存文件的修改时间戳， 显然分布式的缓存文件是不能被任何的修改 文件、归档、jar/native lib都可以被缓存， 可以通过property设置， 或者通过Job添加， 如：12Job.addCacheFile(URI) / Job.addCacheArchive(URI)Job.AddArchiveToClassPath(Path) / JOb.addFileToClassPath(Path) URI: hdfs://host:port/absolute-path#link-name 分布式的缓存文件可以是私有或公用， 这决定了它们是怎样被worker节点所共享的， 主要是通过分布式文件的用户和文件的可见性来决定 #### Debugging MapReduce框架提供当任务失败时执行用户指定脚本的功能， 脚本可以访问任务的标准输出、标准错误、系统日志和任务配置， 脚本处理访问的信息并输出错误信息到控制台或者任务UI Debugging脚本的使用步骤： 提交脚本到分布式缓存中 提交脚本到map或者reduce当中， 使用property或者Configuration 脚本访问 $script $stdout $stderr $syslog $jobconf 数据压缩 MapReduce提供map和reduce应用输出压缩功能， 为了性能它使用本地实现 跳过错误的记录 Hadoop可以通过ShipBadRecords类来控制map输入的错误记录， 对于某些应用来说如果少量的错误记录是可以接受的那么可以开启此功能， 使用SkipBadRecords.set[Mapper|Reducer]MaxSkipGroups(Configuration, long)来设置最大的错误记录数量。 复杂的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Counter;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.StringUtils;public class WordCount2 &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; // 提供counter功能 static enum CountersEnum &#123; INPUT_WORDS &#125; // 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); // 提供单词大小写敏感和模式忽略功能 private boolean caseSensitive; private Set&lt;String&gt; patternsToSkip = new HashSet&lt;String&gt;(); private Configuration conf; private BufferedReader fis; // 在setup中处理Mapper的初始化动作 @Override public void setup(Context context) throws IOException, InterruptedException &#123; // 获取配置 conf = context.getConfiguration(); // 获取Job中设置的环境变量 caseSensitive = conf.getBoolean("wordcount.case.sensitive", true); // 解析配置 if (conf.getBoolean("wordcount.skip.patterns", false)) &#123; //获取并解析缓存 URI[] patternsURIs = Job.getInstance(conf).getCacheFiles(); for (URI patternsURI : patternsURIs) &#123; Path patternsPath = new Path(patternsURI.getPath()); String patternsFileName = patternsPath.getName().toString(); parseSkipFile(patternsFileName); &#125; &#125; &#125; private void parseSkipFile(String fileName) &#123; try &#123; fis = new BufferedReader(new FileReader(fileName)); String pattern = null; while ((pattern = fis.readLine()) != null) &#123; patternsToSkip.add(pattern); &#125; &#125; catch (IOException ioe) &#123; System.err.println("Caught exception while parsing the cached file '" + StringUtils.stringifyException(ioe)); &#125; &#125; @Override public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; // 处理单词大小写敏感和模式忽略功能 String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase(); for (String pattern : patternsToSkip) &#123; line = line.replaceAll(pattern, ""); &#125; StringTokenizer itr = new StringTokenizer(line); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); // counter计数， 会在日志中打印 Counter counter = context.getCounter(CountersEnum.class.getName(), CountersEnum.INPUT_WORDS.toString()); counter.increment(1); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 配置解析 Configuration conf = new Configuration(); GenericOptionsParser optionParser = new GenericOptionsParser(conf, args); String[] remainingArgs = optionParser.getRemainingArgs(); if ((remainingArgs.length != 2) &amp;&amp; (remainingArgs.length != 4)) &#123; System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt; [-skip skipPatternFile]"); System.exit(2); &#125; Job job = Job.getInstance(conf, "word count"); // 基本设置 job.setJarByClass(WordCount2.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); List&lt;String&gt; otherArgs = new ArrayList&lt;String&gt;(); for (int i=0; i &lt; remainingArgs.length; ++i) &#123; if ("-skip".equals(remainingArgs[i])) &#123; // 设置缓存 job.addCacheFile(new Path(remainingArgs[++i]).toUri()); // 设置环境变量 job.getConfiguration().setBoolean("wordcount.skip.patterns", true); &#125; else &#123; otherArgs.add(remainingArgs[i]); &#125; &#125; // 设置输入与输出 FileInputFormat.addInputPath(job, new Path(otherArgs.get(0))); FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1))); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 命令行执行123456789101112131415161718192021222324$ bin/hadoop fs -ls /user/joe/wordcount/input//user/joe/wordcount/input/file01/user/joe/wordcount/input/file02$ bin/hadoop fs -cat /user/joe/wordcount/input/file01Hello World, Bye World!$ bin/hadoop fs -cat /user/joe/wordcount/input/file02Hello Hadoop, Goodbye to hadoop.$ bin/hadoop fs -cat /user/joe/wordcount/patterns.txt\.\,\!to$ bin/hadoop jar wc.jar WordCount2 -Dwordcount.case.sensitive=false /user/joe/wordcount/input /user/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt$ bin/hadoop fs -cat /user/joe/wordcount/output/part-r-00000bye 1goodbye 1hadoop 2hello 2horld 2]]></content>
      <categories>
        <category>hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2F2018%2F04%2F26%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[原理 docker 每一条命令都会创建一层， 并且会使用上一层的镜像运行一个docker容器， 再执行本层的命令，执行完成之后就提交停止容器， 并提交当前容器的修改为一个镜像 当容器开始构建的时候， 第一步会将指定的context目录下的所有的文件都传递给docker守护进程（除了.dockerignore文件标识的文件除外), 第二步启动FROM image指定的镜像的容器， 并执行下面一条命令，关闭当前容器并提交一层镜像， 以后各层都是如此，以上一层的镜像运行一个容器并执行命令后提交镜像所以我们可以看到命令越多则层数越多， 建议尽量使用最少的层， 并且将相关的动作描述清晰使其容易维护 注意 妥善使用下面点： 基础镜像选择， 使用两种环境变量， 组合命令 设置基础镜像1FROM ubuntu 设置LABELLABEL用来标示当前镜像， 作为一种元数据存在与镜像当中， 作为一种描述123LABEL com.ilivoo.game=&quot;fengxiang&quot; \ version=&quot;1.0&quot; \ description=&quot;this is my first game!&quot; 设置环境变量设置docker容器运行时的环境变量， 并且可以在构建的过程中使用这些命令， 其实道理很简单， 以后的构建过程中都是使用上一个镜像层作为基础， 而运行的时候启动容器这个环境变量就已经生成了， 所以后面的RUN 等命令当然也是可以获取到这个环境变量的， 所以脚本引用方式与正查的脚本完全一样, 可以看到巧妙的设计环境变量可以使得构建过程非常灵活， 并且运行时容器也非常灵活12ENV VERSION=1.0 \ DEBUG=on 设置ARG参数ARG命令也是用来设置环境变量的， 但是ARG设置的环境变量不会保留到最终容器运行的层当中, 其实实现也非常简单当运行容器的时候设置环境变量， 并且在容器退出的时候就将环境变量删除就可以达到此目的了, 并且此参数可以通过–build-arg参数在构建的时候进行覆盖，这样就可以达到非常灵活的效果了虽然ARG环境变量不会保存到运行是环境变量， 当时也不能使用铭感数据， 因为使用history会查看到其值1ARG REDIS_VERSION=3.2.10 环境变量与ARG参数结合使用 通过在构建的时候覆盖当前–build-arg NAME=FENGXIANG此时后面设置的环境变量FENG就变成了FENGXIANG, 可以看到虽然环境变量不能通过参数的形式进行改变但是这里确提供了灵活的方式来达到环境变量的不同， 所以通过脚本来构建就可以达到非常灵活的效果， 所以在我们的代码当中需要进行灵活变化的变量应该使用环境变量来进行设置， 这样就可以达到任何代码都不需要更改确有一个灵活的生产 测试 线上环境， 这个非常重要 12ARG NAME=XIANGENV FENG=$&#123;NAME&#125; 建议将那些需要进行覆盖的ARG使用这种方式给出， 并且在使用的时候通过字符串获取的方式来获得， 此处标示如果COMPANY不存在那么默认值就是ilivoo通过这种方式能够非常明显标示当前ARG用来进行占位的， 真正使用的时候可以进行指定， 并且最好是通过注释的方式标示可以指定的值 12ARG COMPANYENV COMPANY $&#123;COMPANY:-ilivoo&#125; 指定工作目录指定docker构建的以后的各层当中都使用指定的这个目录作为工作目录， 也就是以后命令执行的当前目录都是这个目录，如：CMD [“./start.sh”], 这个命令默认就去查找这个目录下对应的脚本， 如果不存在则报错， 并且每次设置都会更改以后的所有层， 可以覆盖。 所以建议再构建镜像的时候只有在最终执行命令的时候再去指定这个目录， 构建最终层镜像的时候显示的指定每个工作的目录是一个非常不错的选择1WORKDIR /app 指定执行用户USER和WORKDIR命令类似， 都是改变环境状态并影响以后的层， 而USER则是更改RUN CMD ENTRYPOINT这类动作命的身份， 用户必须事先建立好。如果以root执行脚本， 在执行容器的时候希望使用特定的身份执行容器， 不建议使用sudo命令， 这样会非常麻烦，而且容易出错，建议使用gosu来控制， 借鉴redis的做法。1USER root ADD命令当使用ADD添加本地文件时， 如果是单个文件才会进行解压缩(如果多个文件不会解压缩)如： ADD hello.tar.gz /app如果原路径是一个URL那么docker会试图区下载这个文件， 下载后权限自动设置为600，并且不会解压文件，如果需要更改还需要添加可以看到ADD命令其实限制非常多， 并且语义也并不明确， 如果需要从网络下载还不如使用RUN命令，再使用curl gzip chmod等一次更改当前修改， 这样只需要创建一层就可以完成了。所以在项目中尽量不要使用这个命令来添加文件， 仅当需要解压时才使用， 如将所有的项目文件压缩为一个zip包， 再通过一条ADD命令将其添加到项目目录当中12ADD http://download.redis.io/releases/redis-$&#123;REDIS_VERSION&#125;.tar.gz /appADD hello.tar.gz /app COPY命令注意docker构建的context， 也就是docker构建的上下文再docker构建的过程中是没有当前目录的概念的，比如：不能指定../clear.sh， 因为docker引擎不知道这个目录是哪里，所以获取不到这个文件当使用docker build进行构建的时候， 会设置Dockerfile和Context上下文对于的路径， 如果没有设置Dockerfile的位置， 那么默认会在context中查找Dockerfile, 如果没有那么构建失败而当指定Context的时候，docker客户端会将context指定的目录下所有的文件都传送给docker引擎， docker引擎再根据dockerfile中指定的需要的文件从context中去查找， 如果找不到构建失败由此可见Dockerfile中指定的原路径都是指定当前docker引擎的context路径docker引擎每向下构建一层需要的依赖文件就从docker引擎的context路径传送到启动的docker容器当中， 再提交容器生成一层镜像1COPY . /app RUN命令在构建的过程中运行当前命令， 存在两种形式shell格式和exec形式, shell格式就像直接运行shell命令一样执行 而exec格式则是只按照当前给出的命令和参数的形式进行执行此处必须写成./start.sh， 而不能直接写成start.sh， 因为在shell下直接写成start.sh也是不能运行的RUN ./start.sh而此处exec格式，必须添加上执行的shell1RUN [&quot;/bin/bash&quot;, &quot;start.sh&quot;] CMD命令指定容器启动的主进程启动命令， 也就是操作系统挂载文件系统之后启动的init进程， 而此处的主进程就是类似init进程, 当且仅当主进程推出那么此时容器内所有的进程都退出， 并且我们知道主进程init是所有的进程的父进程，当产生僵尸进程的时候当父进程死后，而子进程没有死那么为了防止僵尸进程，此时就需要init进程来寄养子进程。这里完全和linux操作系统是一样的。在使用shell格式的时候， 系统会默认为其添加sh -c的参数形式执行容器在其启动的时候可以重新覆盖CMD命令，从而启动其他的命令1CMD echo $HOME ENTRYPOINT命令ENTRYPOINT, 当使用了ENTRYPOINT之前的CMD和ENTRYPOINT就被覆盖， 而定义在ENTRYPOINT后面的CMD则作为ENTRYPOINT的参数, 而且在容器运行的时候还可以覆盖CMD参数， 这样就可以达到一种默认命令， 并且可以更改参数的用途， 这样某些场景会非常有用， 比如redis的启动， ENTRYPOINT指定启动脚本， 并且在真正的主进程启动之间进行一些环境的修改， 如执行用户等等，并且可以设置参数。 第二可以把容器当作命令来执行， 并且可以为其指定参数。容器启动的时候也可以使用 –entrypoint 来覆盖这里的entrypoint命令12ENTRYPOINT [&quot;/bin/bash&quot;, &quot;start.sh&quot;]CMD [&quot;3&quot;] 存储卷VOLUME定义匿名卷， 容器运行时应该尽量（必须）保证容器存储层不发生写操作， 任何的数据要么保存到卷当中， 要么通过数据库服务保存在其它的地方为了防止用户忘记运行时将需要使用的目录挂载为卷， 我们可以在Dockerfile中定义匿名的卷， 如果用户不挂载那么运行是容器自动挂载当前卷， 使用docker守护进程在本地创建一个目录作为此匿名卷的挂载目录， 这样就可以保证容器的存储层不会被写入了. 所以任何时候我们都应该为一个需要写入数据的容器声明卷， 而在docker中， 最终这个卷被挂载到具体那个位置是未知的，docker提供了一个灵活的处理方式， docker的卷可以使用任何第三方实现的插件， 将其保存在分布式文件系统或者云或者本地系统等等， 这样卷就可以作为一种资源来供容器使用了， 这也swarm kubernetes等工具的实现方式。运行时挂载 -v mydata:/data12VOLUME /dataVOLUME [&quot;/pass1&quot;, &quot;/pass2&quot;] 端口映射EXPOSE 声明运行时容器提供服务的端口， 仅仅只是一个声明而已， 运行是并不会因为这个声明就会开启这个端口的服务， 其实容器也是完全做不到这一点的， 主要有两个意图， 第一帮助镜像使用者理解这个镜像的服务的守护端口， 方便配置映射， 另一个用户则是运行时使用随机端口映射， 也就是docker run -P时， 会自动随机映射EXPOSE的端口， 不管当前端口是否有服务在监听1EXPOSE 9092]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 使用教程]]></title>
    <url>%2F2018%2F04%2F25%2Fhexo%2F</url>
    <content type="text"><![CDATA[###常见问题 分类和标签统计错误 1234cd $HEXO_HOMErm db.jsonhexo cleanhexo generate markdown转义 12345678910111213141516171819202122! &amp;#33; — 惊叹号 Exclamation mark” &amp;#34; &amp;quot; 双引号 Quotation mark# &amp;#35; — 数字标志 Number sign$ &amp;#36; — 美元标志 Dollar sign% &amp;#37; — 百分号 Percent sign&amp; &amp;#38; &amp;amp; Ampersand‘ &amp;#39; — 单引号 Apostrophe( &amp;#40; — 小括号左边部分 Left parenthesis) &amp;#41; — 小括号右边部分 Right parenthesis* &amp;#42; — 星号 Asterisk+ &amp;#43; — 加号 Plus sign&lt; &amp;#60; &amp;lt; 小于号 Less than= &amp;#61; — 等于符号 Equals sign&gt; &amp;#62; &amp;gt; 大于号 Greater than? &amp;#63; — 问号 Question mark@ &amp;#64; — Commercial at[ &amp;#91; --- 中括号左边部分 Left square bracket\ &amp;#92; --- 反斜杠 Reverse solidus (backslash)] &amp;#93; — 中括号右边部分 Right square bracket&#123; &amp;#123; — 大括号左边部分 Left curly brace| &amp;#124; — 竖线Vertical bar&#125; &amp;#125; — 大括号右边部分 Right curly brace]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 正则表达式]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-re%2F</url>
    <content type="text"><![CDATA[定义 正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 正则表达式的强大之处在于引入特殊字符来定义字符集、字符集位置、匹配子组、重复模式和扩展表示法。 字符集定义: 符号 描述 正则表达式 匹配案例 literal 匹配字符串的字面值 foo foo re1&#124;re2 匹配正则表达式re1或者re2 foo&#124;bar foo或者bar . 匹配任何字符(除了\n之外) b.b bab […] 匹配来自字符集的任意单个字符 [aeiou] a [^…] 匹配非来自字符集的任何单个字符 [^aeiou] 0 [x-ym-n] 匹配任意来自x~y和m~n范围内的单个字符 [A-Za-z] b \d 匹配任何十进制数字，与[0-9]相同(与\D相反) data\d.txt data1.txt \w 匹配字母数字数字字符,与[A-Za-z0-9]相同(与\w相反) data\w.txt dataa.txt \s 匹配任何空格字符, 与[\n\t\r\v\f]相同(与\S相反) of\sthe of the \x 对特殊字符进行转移 \. . 字符集位置： 符号 描述 正则表达式 匹配 ^ 匹配字符串起始位置 ^Dear Dear $ 匹配字符串终止位置 /bin/*sh$ /bin/bash \A(\Z) 匹配字符串起始(结束)位置 \ADear Dear \b 匹配任何单词边界(与\S相反) \bthe\b the 重复模式： 符号 描述 正则表达式 匹配 * 匹配０次或者多次 [A-Za-z0-9]* abcde + 匹配１次或者多次 [a-z]+.com baidu.com ? 匹配０次或者多次 goo? go {N} 精确匹配Ｎ次 [0-9]3 333 {M,N} 匹配M~N次 [0-9]{5,9} 123456 (*&#124;+&#124;?&#124;{}) 匹配上面重复出现的非贪婪版本 .*?[a-z] abcc 匹配子组： 符号 描述 正则表达式 匹配 (…) 匹配封闭的正则表达式， 然后另存为子组 ([0-9]{3}-)?[0-9]{7-8} 020-888888 \N 配皮上面已经保存的分组 ([0-9]{3})-(\d{7-8}) post is \1 020-12345678 post is 020 扩展表示法： 符号 描述 正则表达式 匹配 贪婪模式正则表达式的默认为贪婪模式，表示竟可能多的匹配， 非贪婪是如果后面正则表达式能够匹配则尽可能少的匹配， 给后面表达式匹配的机会， 如：1234&gt;&gt;&gt; re.match(r&apos;^(\d+)(0*)$&apos;, &apos;102300&apos;).groups() #贪婪(&apos;102300&apos;, &apos;&apos;)&gt;&gt;&gt; re.match(r&apos;^(\d+?)(0*)$&apos;, &apos;102300&apos;).groups() #非贪婪(&apos;1023&apos;, &apos;00&apos;) re 模块123456789101112131415161718192021基础方法compile(pattern, flags = 0) 用任何可选的标记来编译正则表达式，然后返回正则表达式对象match(pattern, string, flags = 0) 尝试使用带可选标记的正则表达式的模式来匹配字符串，如果匹配成功返回匹配对象，否则返回Nonesearch(patter, string, flags = 0) 使用可选的标记搜索字符串中第一次出现正则表达式模式，如果匹配成功返回匹配对象，否则返回None，search是逐个逐个字符的往下匹配findall/finditer(pattern, string[,flags]) 查找字符串中所有(非重复)出现的正则表达式模式，并返回匹配列表split(pattern, string, max = 0) 使用正则表达式分割模式分割字符串，并返回最大max次操作的列表sub(pattern, repl, string, count = 0) 使用repl替换正则表达式的模式在字符串中出现的位置，除非定义count，否则替换所有的位置purge() 清楚隐式编译的正则表达式模式标记re.I | re.IGNORECASE 不区分大小写匹配re.L | re.Locale 根据所使用的本地语言环境通过\w、\W、\b、\B、\s、\S实现匹配re.M | re.MULTILINE ^和$分别匹配目标字符串中行的起始和结束，而不是严格匹配整个字符串本身的起始和结尾re.S | re.DOTALL .号可以匹配所有的字符，包括\nre.X | re.VERBOSE 所有的空格加上#都会被忽略re.T | re.TEMPLATE ...匹配对象的方法group(num = 0) 返回整个匹配对象或者编号为num的特定子组groups(default = None) 返回包含所有匹配子组的元组groupdict(defalt = None) 返回包含所有匹配子组的字典，子组名称作为键]]></content>
      <categories>
        <category>python</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 帮助命令]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-help%2F</url>
    <content type="text"><![CDATA[获取内建信息123dir() 打印当前scopedir(__builtins__) 打印内建scopedir(object) 打印对象scope 获取帮助1help(object) 获取类继承层次1Class.mro() 此方法是type中的方法， 每个Class都是type的实例 获取所有的模块1help('modules') 判断对象类型123456789&gt;&gt;&gt; import types&gt;&gt;&gt; type('abc')==types.StringTypeTrue&gt;&gt;&gt; type(u'abc')==types.UnicodeTypeTrue&gt;&gt;&gt; type([])==types.ListTypeTrue&gt;&gt;&gt; type(str)==types.TypeTypeTrue 获取对象中的常量1234567891011121314151617181920212223242526272829303132import typesbase_type = (types.BooleanType, types.FloatType, types.IntType, types.LongType, types.NoneType,types.StringType, types.TupleType, types.ListType, types.DictType, types.ComplexType)def dir_ck(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value print sorted(_constant.keys())def dir_ok(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value print sorted(_object.keys())def dir_c(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value for key in sorted(_constant.keys()): print key + "\t = \t" + _constant.get(key)def dir_o(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value for key in sorted(_object.keys()): print key + "\t = \t" + str(_object.get(key)) 查找对象中的方法12def dir_f(obj, key): print filter(lambda x: x.lower().find(key) != -1, dir(obj))]]></content>
      <categories>
        <category>python</category>
        <category>帮助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
