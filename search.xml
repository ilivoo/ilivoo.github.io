<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mac效率优化]]></title>
    <url>%2F2020%2F07%2F22%2Fmac-install%2F</url>
    <content type="text"><![CDATA[安装brew 获取一个http和https代理，如局域网中有电脑有代理工具privoxy，设置本地代理 123export http_proxy=http://proxyhost:8118export https_proxy=http://proxyhost:8118export ftp_proxy=http://proxyhost:8118 安装brew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; 更改国内源 12345678cd &quot;$(brew --repo)&quot;git remote set-url origin git://mirrors.ustc.edu.cn/brew.gitcd &quot;$(brew --repo)/Library/Taps/homebrew/homebrew-core&quot;git remote set-url origin git://mirrors.ustc.edu.cn/homebrew-core.gitbrew install caskbrew caskcd &quot;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask&quot;git remote set-url origin git://mirrors.ustc.edu.cn/homebrew-cask.git]]></content>
      <categories>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window-use]]></title>
    <url>%2F2020%2F07%2F17%2Fwindow-use%2F</url>
    <content type="text"><![CDATA[查看网络连接 12netstat -ano协议 本地地址 外部地址 状态 PID 查看指定端口的占用情况 123netstat -aon|findstr &quot;9050&quot;协议 本地地址 外部地址 状态 PIDTCP 127.0.0.1:9050 0.0.0.0:0 LISTENING 2016 查看PID对应的进程 123tasklist|findstr &quot;2016&quot; 映像名称 PID 会话名 会话# 内存使用tor.exe 2016 Console 0 16,064 K 杀死进程 12taskkill /f /t /im tor.exetaskkill -F -PID 2016]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 备份]]></title>
    <url>%2F2020%2F07%2F15%2Fmysql-backup%2F</url>
    <content type="text"><![CDATA[mysqldump 与 xtrabackup 的实现原理 mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVELREPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /!40100 WITH CONSISTENTSNAPSHOT /)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK)，等开启事务后，再记录下数据库此时 binlog 的位置(show master status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务。 xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。 备份计划，100G 内使用 mysqldump 来做，可以每天进行都进行全量备份并压缩。100G 以上使用 xtranbackup 来做，一般一周一个全备，其余每天进行增量备份。 恢复时间，物理备份恢复快，逻辑备份恢复慢，逻辑恢复时间一般是备份时间的5倍以上，当然也和机器有非常大的关系，参考： mysqldump：20G的2分钟，80G的30分钟，111G的30分钟 xtranbackup：288G的3小时，3T的4小时 mysqldump 参数 skip-extended-insert 每一行只有一个insert into 语句 master-data 获取备份数据的Binlog位置和Binlog文件名，用于通过备份恢复的实例之间建立复制关系时使用，该参数会默认开启。 dump-slave 用于在slave上dump数据，建立新的slave。因为我们在使用mysqldump时会锁表，所以大多数情况下，我们的导出操作一般会在只读备库上做，为了获取主库的Relay_Master_Log_File和Exec_Master_Log_Pos，需要用到这个参数，不过这个参数只有在5.7以后的才会有 no-data, -d 不导出任何数据，只导出数据库表结构 –opt命令可选，建议加上。等同于指定 –add-drop-tables –add-locking –create-option –disable-keys –extended-insert –lock-tables –quick –set-charset。它可以给出很快的转储操作并产生一个可以很快装入MySQL服务器的转储文件。 mysqldump全库备份，并只恢复某一个库或表 全库备份 1mysqldump -uroot -p --single-transaction -A --master-data=2 &gt;dump.sql 只还原erp库 1mysql -uroot -pMANAGER erp --one-database &lt; dump.sql 抽取出表t的结构 1sed -e&apos;/./&#123;H;$!d;&#125;&apos; -e &apos;x;/CREATE TABLE `t`/!d;q&apos; dump.sql 抽取出表t的内容 1grep&apos;INSERT INTO `t`&apos; dump.sql mysqldump 压缩 备份并压缩 1mysqldump -h127.0.0.1 -uroot -p test | gzip &gt; ./test.gz 解压缩并还原 1gunzip &lt; ./test.gz | mysql -uroot -p test 直接从a库到b库 1mysqldump --default-character-set=utf8mb4 --host=127.0.0.1 -uusername -ppassword --opt dbname | mysql --host=127.0.0.2 -uusernameb -ppasswordb --default-character-set=utf8mb4 -C dbnamenew –default-character-set=utf8mb4 指定该数据库连接的字符类型。如果服务器默认未utf8的话，导出的数据可能会丢失四字节的unicode信息（表情之类的） -C 客户端和服务器之间启用压缩传递所有信息。 添加自增字段 1alter table tableName add column id int auto_increment primary key first;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-oozie]]></title>
    <url>%2F2020%2F06%2F28%2Fhadoop-oozie%2F</url>
    <content type="text"><![CDATA[oozie 使用技巧 单步调试，在job.properties中设置 oozie.suspend.on.nodes 需要挂起的节点，当运行到该节点是自动挂起，需要手动resume才会继续运行，如： 1oozie.suspend.on.nodes=mr-node,my-pig-action,my-fork 全局配置，通常情况下很多的任务存在相同的配置，如任何的action都是通过MapReduce来执行的，所以可以设置MapReduce公用属性，在每个action中可以覆盖，如： 123456789101112&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;wf-name&quot;&gt;&lt;global&gt; &lt;job-tracker&gt;$&#123;job-tracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;namd-node&#125;&lt;/name-node&gt; &lt;job-xml&gt;job1.xml&lt;/job-xml&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;&lt;/global&gt; 重新尝试，当action运行 ERROR 或 FAILED 状态的时候，用户可以选择是否对任务进行再次尝试，尝试的策略有 periodic 和 exponential 123&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;wf-name&quot;&gt; &lt;action name=&quot;a&quot; retry-max=&quot;2&quot; retry-interval=&quot;1&quot; retry-policy=&quot;exponential&quot;&gt;&lt;/action&gt; workflow job 恢复（rerun），oozie提供了job运行失败后重新提交任务的功能，并且可以指定从任意的action开始执行。 要提交一个恢复的workflow，当前运行的任务的状态必须是 SUCCEEDED 、FAILED 或者 KILLED 有两种方式指定从哪些节点开始运行，两种运行方式是互斥的，也就是不能同时包含两个参数 第一种指定需要跳过的节点 oozie.wf.rerun.skip.nodes=node1,node2 第二种指定是否从失败节点开始运行 oozie.wf.rerun.failnodes=true，如果不从失败节点开始运行（也就是完全从头开始运行），可以通过下面两种命令： 1234#可以重新修改配置文件，主要是延续了job id而已oozie job -rerun 0000411-180116183039102-oozie-hado-W -config job.properties#完全沿用配置文件和job idoozie job -rerun 0000411-180116183039102-oozie-hado-W -D oozie.wf.rerun.failnodes=false 由于每次的运行都是延续上一次运行的配置文件，如果我们使用两种方式重新运行过，如果再切换成另外一种方式就会报错，只能在当前配置文件中更改 配置job和node开始和结束回调，在 job.properties 中配置如下，可以使用 nc -l 39999 简单测试 12oozie.wf.workflow.notification.url=http://hdp1.tepia.com:39999?jobId=$jobId&amp;status=$status&amp;parentId=$parentIdoozie.wf.action.notification.url=http://hdp1.tepia.com:39999?jobId=$jobId&amp;nodeName=$nodeName&amp;status=$status 可以指定代理，默认类型为http，也可以指定socks代理，如： 1oozie.wf.workflow.notification.proxy = xyz:80 or socks@xyz:1080 EL 表达式 通用常量 KB、MB、GB、TB、PB 基本函数 123456789String firstNotNull(String value1, String value2) 返回第一个不为空的值，否则为空String concat(String s1, String s2)String replaceAll(String src, String regex, String replacement)String trim(String s)String urlEncode(String s)String timestamp()String toJsonStr(Map)，参数Map通常来自 wf:actionData(String actionName)String toPropertiesStr(Map)，参数Map通常来自 wf:actionData(String actionName)String toConfigurationStr(Map)，参数Map通常来自 wf:actionData(String actionName) WF 函数 12345678910String wf:id()String wf:name()String wf:user()String wf:group()String wf:appPath() returns the workflow application pathString wf:conf(String name) 返回job的配置属性，或者为空字符串Map wf:actionData(String node) 节点产生输出，获取action节点运行的输出并返回一个MapString wf:lastErrorNode()String wf:errorCode(String node)String wf:errorMessage(String message) hadoop 函数 1Map&lt;String, Map&lt;String, Long&gt;&gt; hadoop:counters(String node) Group 和 Counter name常量 12RECORDS 组名MAP_IN MAP_OUT REDUCE_IN REDUCE_OUT 常量名 HDFS 函数 12345boolean fs:exists(String path)boolean fs:isDir(String path)long fs:dirSize(String path)long fs:fileSize(String path)long fs:blockSize(String path) hcat 函数 1boolean hcat:exists(String uri) 例子： 1hcat://[metastore server]:[port]/[database name]/[table name]/[partkey1]=[value];[partkey2]=[value]]]></content>
      <categories>
        <category>hadoop</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive基本使用]]></title>
    <url>%2F2020%2F05%2F29%2Fhadoop-hive-usage%2F</url>
    <content type="text"><![CDATA[hive基本使用 hive的beeline连接到hiveserver2之后，如果使用load data local inpath，此处的路径指的是hiveserver2上的路径地址，所以要么在hiveserver2的服务器上使用beeline，或者将文件上传到hdfs上，再倒入表 beeline连接到hiveserver2之后，不管beeline使用的是何种keytabs，load data inpath使用的文件地址都需要hive用户有访问权限，也就是说hiveserver2使用的永远都是hive用户，而不是beeline或者其它客户端的登录用户 hive3.0默认开启事务表，对于事务表支持CRUD和merge操作，并且只能是orc数据格式，也可以指定事务表为insert-only，但是支持所有的数据格式都支持，事务表现在不需要必须是分桶的。 事务表为了CUD必须设置动态分区为 nonstrict 事务每次修改都会导致产生大量小文件，默认hive会自动compact小文件，但是也可以手动进行compact 1ALTER TABLE tablename [PARTITION (partition_key=&apos;partition_value&apos; [,...])] COMPACT &apos;compaction_type&apos; DISTRIBUTE BY、SORT BY、CLUSTER BY 如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这可以使用 DISTRIBUTE BY 字句。需要注意的是，DISTRIBUTE BY 虽然能把具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。 如果想让 单个Reducer 上的数据时有序的，可以结合 SORT BY 使用 1SELECT empno, deptno, sal FROM emp DISTRIBUTE BY deptno SORT BY deptno ASC; 如果 SORT BY 和 DISTRIBUTE BY 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 CLUSTER BY 进行替换，同时 CLUSTER BY 可以保证数据在全局是有序的 1SELECT empno, deptno, sal FROM emp CLUSTER BY deptno; with 查询（CTE 公共表表达式，mysql也支持），Hive 可以通过with查询来提高查询性能。先通过with语法将重复使用的数据查询到内存，后面其它查询可以直接使用 12with tmp as (select * from employee where name=&apos;ted&apos;)select * from tmp; hive存在内部、外部、临时、分区、分桶和倾斜表，倾斜表是通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。 12345678CREATE EXTERNAL TABLE emp_skewed( empno INT, ename STRING, job STRING, ) SKEWED BY (empno) ON (66,88,100) --指定 empno 的倾斜值 66,88,100 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; LOCATION &apos;/hive/emp_skewed&apos;; hive merge语法，对于事务表通过定时的ETL，可以将kafka或其它数据源中的数据迁移到hive当中，如增量获取维度数据，先用sqoop导入关系型数据到临时表，再执行下面语句 123456merge into base_table using incremental_table on base.id = incremental_table.id when matched then update set fieldl1=incremental_table.email, modified_date=incremental_table.state when not matched then insert values(incremental_table.id, incremental_table.field1, incremental_table.modified_data); hive索引在3.0中已经取消，并且官方建议使用物化视图和列式存储来对表进行加速，其实对于数据仓库而言索引没有存在的必要，都是全表扫描。 hive调试 hive --hiveconf hive.root.logger=INFO,console 表制作函数和lateral view，lateral view 与用户自定义表生成函数（UDTF）（例如 explode）结合使用，UDTF 为每个输入行生成零个或多个输出行。lateralview 首先将 UDTF 应用于基础表的每一行，然后将结果输出行与输入行连接起来形成具有所提供表别名的虚拟表。from 子句可以具有多个 lateralview 子句，多个laterval view进行笛卡尔乘积。如下： 1234select explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) as col; //直接生成多行，列名为colselect tf.* from (select 0) t lateral view explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) tf;//tf虚拟表名select explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) as (key,value);//直接生产多行，列名为key和valueselect tf.* from (select 0) t lateral view explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) tf; hive增强聚合grouping sets、cube、rollup，在通常的group by中只是对当前指定的维度进行聚合，但是通常情况下可能需要同时对多个维度一次性聚合，那么就可以使用增强聚合，如维度为a和b，如下 | gruoping sets | group || ———————————————————— | ———————————————————— || SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a, b), a, b, ( ) ) | SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b UNION SELECT a, null, SUM( c ) FROM tab1 GROUP BY a, null UNION SELECT null, b, SUM( c ) FROM tab1 GROUP BY null, b UNION SELECT null, null, SUM( c ) FROM tab1 || SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b), a) | SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b UNION SELECT a, null, SUM( c ) FROM tab1 GROUP BY a || SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b) ) | SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b || SELECT a,b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS (a,b) | SELECT a, null, SUM( c ) FROM tab1 GROUP BY a UNION SELECT null, b, SUM( c ) FROM tab1 GROUP BY b | 说明：对于cube是直接进行笛卡尔乘积，如grouping sets ( (a, b), a, b, ( ) ) = cube(a, b)，而rollup是向上，如 grouping sets ( (a, b), a, ( ) ) = rollup(a, b) cost-based optimizer (CBO)，hive默认开启CBO优化，并且自动生成表级别的统计信息，但是hive并不会使用CBO优化，知道为表生成了列统计信息，hive并没有默认执行列统计信息，因为统计列信息可能需要大量的计算。 hive中作业优化 union 和 union all 存在区别，union会去重，是一个全局性的动作，建议对于去重动作使用分组，union all 通常可以减少job数量，但需要设置 hive.exec.parallel=true，如 1select name from (select name from a union all select name from b) group by name; 使用distinct，但是count(distinct)这种不推荐使用 1select distinct name from (select name from a union all select name from b) 上面的语句可以通过explain查看job数量，再就是控制MapReduce的运行，如下 12set hive.merge.mapredfiles = true;//MapReduce运行完成合并小文件set hive.merge.mapfiles = true;//Map only运行完成合并小文件 hive使用案例 hive生成自增id，如：有一个维度表需要往其中添加新的数据维度（不是缓慢渐变维） 1234567dim_goods (sid, id, name)tmp_dim_goods 新加入的表(id, name)select t.*, (row_number() over(order by id) + g.max_sid) as sid from tmp_dim_goods tcross join(select coalesce(max(sid), 0) as max_sid from dim_goods) g hive缓慢渐变维，生成自增id 1 hive事实表装载代理主键维度表 1 维度建模 维度保存方案（代理键和拉链表使用代价非常大，一般不建议使用） 稳定的维度，如：时间、地区（地区也不一定，县市的变化） 缓慢变化的维度 每天保存一份全量快照，适合数据量非常小的场景 在维度表中存放历史字段，仅保存一个值，如：current_address、old_address 拉链表实现缓慢渐变维，注意拉链表的备份，其次拉链表会使用到代理键，需要注意代理件的生成，以及事实表装载代理键 用户画像类，使用elestacsearch等保存 维度表的拆分与合并，如：整合整个公司的人员信息，建议使用大宽表，允许存在空值字段 维度的类型，如：用户基本信息、时间、地区等，还存在另外一种维度数据，用户画像的标签数据，如：用户所属渠道、用户注册时间、用户注册手机号、用户首次下单时间、用户累计订单数、用户累计充值金额等 事实表（也存在拉链表），事实表分为单事实表（单个大表），流程事实表（整个订单流程，每个流程都是一个事实） 明细事实表（DWD) ，包括整个事实，不存在任何的汇总操作，尽量把整个维度都展示出来（完全的星座模型） 聚合事实表（DWS），按日，按周期性的，或者上次下单时间，区域，历史累计（sum、count、首次注册、上次下单、上次登录），如果有需求尽量去做一些维度退化，形成大宽表 通用汇总层（需要花费大量力气去做），汇总力度不能太大，需要满足80%以上的功能，可以进行一些维度退化的动作，通常存在两种，如： 完全明细汇总（流程性订单），但是对整个订单过程进行汇总，订单完成时长（从下单到支付）等 聚合明细（针对单用户当天的操作，如：司机、乘客），登录次数、下单次数、下单总完成时间、整个浏览时长、优惠金额等 维度汇总层（在通用汇总层之上，相当于一个日报（周期汇总），实际上就是指标BI），如： 针对订单主题汇总，按照时间、地区两个维度，如：时间ID，地区ID，下单量，支付金额、优惠金额等 针对用户主题汇总，按照日期、渠道、地区三个维度，如：日期ID，渠道ID，地区ID，pv，uv, 新增，活跃，拉新等 数仓建模 梳理业务流程，画出整个业务流程 梳理数据流程，知道每个数据字段等 数据命名规范（层次_数据域_修饰符/描述_范围/周期） 数据域：订单(ord)，用户(user)，财务(finc) 数仓层次：事实表(fact)，共用维度(dim)，数据集市(dm)，ods(o)，dwd(d)，edw(e，也称作dws(s)) 周期/范围数据：日快照(d)，周快照(w)，增量(d)，拉链表(l)，非分区全量表(a) ods表需要能通过表名知道数据源，如o_业务系统编号_业务系统表名_范围/周期 实现命名汉字与中文的映射元数据，如用户_ID =&gt; user_id，订单_金额 =&gt; ord_amt ETL脚本名称尽可能和产出表同名，ETL脚本通常使用python、shell、sql或者mapreduce jar包，调度任务的任务名称也和脚本名字一样 常见的数仓项目，用户画像、运营、BI、广告投放、推荐系统、反欺诈、风控、大数据杀熟 窗口和分析函数 Hive的分析函数又叫窗口函数，在oracle中就有这样的分析函数，主要用来做数据统计分析的。Lag和Lead分析函数可以在同一次查询中取出同一字段的前N行的数据(Lag)和后N行的数据(Lead)作为独立的列。这种操作可以代替表的自联接，并且LAG和LEAD有更高的效率，其中over()表示当前查询的结果集对象，括号里面的语句则表示对这个结果集进行处理。 窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。 窗口函数over是为了对数据划分窗口（本质上也是分组），over(partition by 列名 order by 列名 rows between 开始位置 and 结束位置) over()函数中的分区、排序、指定窗口范围可组合使用也可以不指定，根据不同的业务需求结合使用 over()函数中如果不指定分区，窗口大小是针对查询产生的所有数据，如果指定了分区，窗口大小是针对每个分区的数据 范围 current row：当前行 unbounded：起点，unbounded preceding 表示从前面的起点， unbounded following表示到后面的终点 n preceding ：往前n行数据 n following：往后n行数据 常用的分析函数 窗口函数 LAG 获取窗口中从当前行往前数，第N行数据 LEAD 获取窗口中从当前行往后数，第N行数据 FIRST_VALUE 获取窗口中的第一行 LAST_VALUE 获取窗口中的最后一行 聚合类 count()、avg()、sum()、max()、min() 分析函数 ROW_NUMBER 按照值排序时产生一个自增编号，不会重复，如：1、2、3、4 RANK 按照值排序时产生一个自增编号，值相等时会重复，会产生空位，如：1、2、2、4 DENSE_RANK 按照值排序时产生一个自增编号，值相等时会重复，不会产生空位，如：1、2、2、3 PERCENT_RANK 分组内当前行的RANK值-1/分组内总行数-1 CUME_DIST 小于等于当前值的行数/分组内总行数 NTILE 将窗口中的数据进行分片，如窗口中有10个数据，划分成3片，则值为（1，1，1，1，2，2，2，3，3，3） 物化视图 物化视图是一种预计算的优化方式，通过预先计算并保存表连接或聚合等耗时较多的操作的结果，这样在查询的时候就可以避免进行这些耗时的操作，从而快速得到结果。 物化视图使用的是查询语句重写机制，不需要修改原有的查询语句，优化引擎自动选择合适的物化视图进行查询，对用户完全透明。它和视图的区别在于，物化视图将存储实际的数据，而视图只是存储SQL语句。 物化视图的使用 创建物化视图，类似创建表可以指定handler，将物化视图存储在其它存储设备，如druid 123456789101112CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [DISABLE REWRITE] [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)]AS SELECT ... 修改物化视图对优化器重写优化plan（全局默认是开启） 12SET hive.materializedview.rewriting=true;ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE; 源数据表增加数据时，物化视图也需要更新以保持数据一致性，目前需要用户主动触发rebuild 1ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD; 物化视图生命周期，可以设定指定的窗口时间内物化视图会起到优化作用，超过时间就设置物化视图无效 1SET hive.materializedview.rewriting.time.window=10min; 物化视图和视图的在数据仓库的使用 DM通常情况下视图使用在DM（数据集市层），下层的DW（DWD/DWS）数据仓库层使用的是维度建模，这样在DM层就是一个完整的视图层，可以通过反范式的方式将DW层的数据进行合并到一张表，从而形成一个数据集市的主体域。由此可见物化视图在消除连接和进行提前聚合方面是非常有用的。 连接 hive、spark join和关系型数据库join Spark join，shuffle join，shuffle sort merge join，broadcast join Hive join, shuffle sort merge join，broadcast join，bucket map join，sort-merge-bucket map join 关系型 join，nested-loop join、hash join、sort-merge join（先排序再merge join） 可以看到spark为了充分利用内存提供了shuffle join，不会对map结果进行排序，而hive尽量节约内存所以只有shuffle sort merge join。又由于hive提供分桶表的支持，所以添加了两种map端jion，bucket map join和sort-merge-bucket map join。 hive支持隐式连接，也就是在from后面添加多张表进行连接操作，并且从hive2.2开始已经支持不等值连接。 hive目前存在三种join实现方式，shuffle sort merge join、broadcast join、bucket map join，sort-merge-bucket map join，分别对于关系型数据库中的三种join，hash join、nested-loop join、sort-merge join（先排序再merge join），当然hive中的sort-merge-bucket join稍微有些扩展，因为hive中支持bucket（每个bucket就可以看做是一个小表），所以如果两个表相同的cluster列的bucket成倍数，那么就可以完全将两个表的bucket对应上，这样就想到于各个小表进行sort-merge排序，当然必须保证bucket上已经进行了排序，设置如下： 1234567set hive.auto.convert.sortmerge.join=true;set hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge = true;set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ(d); = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ 当然每种属性都可以独立设置，可以只进行bucketmapjoin等等 hive只能进行等值连接（hive 2.2 之后可以不等值连接），不能进行不等值连接，只支持等值链接，支持 and，不支持 or，这是因为MapReduce实现不等值连接非常困难，所以对于连接中的不等条件只能提取出来，如： 1select * from (select o.*, u.* from ops_oder as o join dwd_dim_user as u on o.user_id = u.user_id) where create_time &gt;= start_date and create_time &lt;= end_date HiveJoin 分三种：inner join, outer join, semi join，outer join 包括 left join，right join 和 full outer join,主要用来处理 join 中空记录的情况，semi join是用来判断存在的，是in/exist的一种高效实现方式，也就是投影只包含左边表，右边表只是用来等值判断的 hive多表连接时，如果on后面的条件是同样的字段，则会自动优化为一个MapReduce任务 等值连接的每个值都可以使用函数进行返回值（包括不等值函数），如解决数据倾斜 123SELECT * FROM log a LEFT OUTER JOIN bmw_users b ON CASE WHEN a.user_id IS NULL THEN CONCAT(‘dp_hive’,RAND()) ELSE a.user_id END =b.user_id; 在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边。 因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表 可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则。 hive也提供了STREAMTABLE大表标识，如： 1SELECT /*+ STREAMTABLE(d) */ e.*,d.* FROM emp e JOIN dept d ON e.no = d.no map Join 操作在 Map 阶段完成，不再需要Reduce，前提条件是需要的数据在 Map 的过程中可以访问到 123INSERT OVERWRITE TABLE pv_users SELECT /*+ MAPJOIN(pv) */ pv.pageid, u.age FROM page_view pv JOIN user u ON (pv.userid = u.userid); map join还可以设置成自动优化，就不需要设置map join了，配置如下： 12set hive.auto.convert.join.noconditionaltask = true;set hive.auto.convert.join.noconditionaltask.size = 10000000; cross join 笛卡尔乘积，笛卡尔乘积不需要指定连接键，因为两张表会做一个乘积。通常情况下笛卡尔乘积不会使用到，但是对于想要为某个表查询的时候添加一个列，此时就可以使用到了，如生产自增id。 1234insert into tbl_dim select row_number() over (order by tbl_stg.tm) + t2.sk_max, tbl_stg.* from tbl_stg cross join (select coalesce(max(sk),0) sk_max from tbl_dim) t2; 说明：此处是给维度表添加新增加的维度数据，自增的id就是维度表的代理键。 数据倾斜 问题1，日志中常会出现信息丢失，比如每日约为 20 亿的全网日志，其中的 user_id 为主 键，在日志收集过程中会丢失，出现主键为 null 的情况，如果取其中的 user_id 和 bmw_users 关联，就会碰到数据倾斜的问题。原因是 Hive 中，主键为 null 值的项会被当做相同的 Key 而分配进同一个计算 Map 解决方法 1：user_id 为空的不参与关联，子查询过滤 null 123SELECT * FROM log a JOIN bmw_users b ON a.user_id IS NOT NULL AND a.user_id=b.user_id UNION All SELECT * FROM log a WHERE a.user_id IS NULL 解决方法 2 如下所示：函数过滤 null 123SELECT * FROM log a LEFT OUTER JOIN bmw_users b ON CASE WHEN a.user_id IS NULL THEN CONCAT(‘dp_hive’,RAND()) ELSE a.user_id END =b.user_id; 通常解决方法2比解决方法1效果更好，不但IO少了，而且作业数也少了。解决方法1中log读取两次，job 数为2。解决方法2中 job 数是1。这个优化适合无效 id（比如-99、 ‘’，null 等）产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的 数据分到不同的Reduce上，从而解决数据倾斜问题。因为空值不参与关联，即使分到不同 的 Reduce 上，也不会影响最终的结果。附上 Hadoop 通用关联的实现方法是：关联通过二次排序实现的，关联的列为 partion key，关联的列和表的tag 组成排序的 group key，根据 pariton key分配Reduce。同一Reduce内根据group key排序。 动态分区 动态分区插入（或多分区插入）旨在通过动态确定在扫描输入表时应创建和填充哪些分区来解决此问题。在动态分区插入中，将评估输入列的值，以确定应将此行插入哪个分区。如果尚未创建该分区，它将自动创建该分区。使用此功能，您只需一个插入语句即可创建并填充所有必要的分区。另外，由于只有一个insert语句，因此只有一个对应的MapReduce作业。与多次插入的情况相比，这显著提高了性能并减少了Hadoop集群的工作量。 由于动态分区存在一些限制，所以操作之前需要设置为 nostrict 12set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; 特殊数据 json数据 方法一：将json数据存储成text，再使用get_json_object、json_tuple解析json 方法二：将json数据存储成json序列化方式，这样就是正常的创建hive表，只是需要将json的数据映射成hive中的数据类型。hive的json序列化器就是将json对象就是hive中的row，再对json中的字段解析成row中字段，通常情况json的嵌套也对于hive的嵌套，但json中的对象可以解析成map也可以解析出struct。json序列化器有多中选择，hive自带或者第三方，如Hive-JSON-Serde，使用json生成hive schema，可以直接使用hive-json-schema。 正则表达式数据，如nginx日志 函数 hive中存在各种内建操作符和对应类型操作的函数，参考 条件函数 | 返回值 | 方法名 | 描述 || ——- | ———————————————————- | ———————————————————— || boolean | isnull( a ) | Returns true if a is NULL and false otherwise. || boolean | isnotnull ( a ) | Returns true if a is not NULL and false otherwise. || T | if(boolean testCondition, T valueTrue, T valueFalseOrNull) | Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise. || T | nvl(T value, T default_value) | Returns default value if value is null else returns value || T | COALESCE(T v1, T v2, …) | Returns the first v that is not NULL, or NULL if all v’s are NULL. || T | CASE a WHEN b THEN c [WHEN d THEN e] [ELSE f] END | When a = b, returns c; when a = d, returns e; else returns f. || T | CASE WHEN a THEN b [WHEN c THEN d] [ELSE e] END | When a = true, returns b; when c = true, returns d; else returns e. || T | nullif( a, b ) | Returns NULL if a=b; otherwise returns a Shorthand for: CASE WHEN a = b then NULL else a || void | assert_true(boolean condition) | Throw an exception if ‘condition’ is not true, otherwise return null | 数据屏蔽函数（创建视图时候防止信息泄露） 常用的其它函数 | 返回值 | 方法名 || —— | ————————————————— || varies | java_method(class, method[, arg1[, arg2..]]) || varies | reflect(class, method[, arg1[, arg2..]]) || int | hash(a1[, a2…]) || string | current_user() || string | logged_in_user() || string | current_database() || string | md5(string/binary) || string | sha1(string/binary)sha(string/binary) || bigint | crc32(string/binary) || string | sha2(string/binary, int) || binary | aes_encrypt(input string/binary, key string/binary) || binary | aes_decrypt(input binary, key string/binary) || string | version() || bigint | surrogate_key([write_id_bits, task_id_bits]) | get_json_object，支持四种最基本形式 1234$ : Root object. : Child operator[] : Subscript operator for array* : Wildcard for [] 如：SELECT get_json_object(src_json.json, `‘$.owner’) FROM src_json;` 聚合函数 | 返回值 | 方法名 | 描述 || ————————- | ——————————————————– | ———————————————————— || BIGINT | count(), count(expr), count(DISTINCT expr[, expr…]) | count() - Returns the total number of retrieved rows, including rows containing NULL values.count(expr) - Returns the number of rows for which the supplied expression is non-NULL.count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with hive.optimize.distinct.rewrite. || DOUBLE | sum(col), sum(DISTINCT col) | Returns the sum of the elements in the group or the sum of the distinct values of the column in the group. || DOUBLE | avg(col), avg(DISTINCT col) | Returns the average of the elements in the group or the average of the distinct values of the column in the group. || DOUBLE | min(col) | Returns the minimum of the column in the group. || DOUBLE | max(col) | Returns the maximum value of the column in the group. || DOUBLE | variance(col), var_pop(col) | Returns the variance of a numeric column in the group. || DOUBLE | var_samp(col) | Returns the unbiased sample variance of a numeric column in the group. || DOUBLE | stddev_pop(col) | Returns the standard deviation of a numeric column in the group. || DOUBLE | stddev_samp(col) | Returns the unbiased sample standard deviation of a numeric column in the group. || DOUBLE | covar_pop(col1, col2) | Returns the population covariance of a pair of numeric columns in the group. || DOUBLE | covar_samp(col1, col2) | Returns the sample covariance of a pair of a numeric columns in the group. || DOUBLE | corr(col1, col2) | Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group. || DOUBLE/百分位数 | percentile(BIGINT col, p) | Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral. || array | percentile(BIGINT col, array(p1 [, p2]…)) | Returns the exact percentiles p1, p2, … of a column in the group (does not work with floating point types). pi must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral. || DOUBLE | percentile_approx(DOUBLE col, p [, B]) | Returns an approximate pth percentile of a numeric column (including floating point types) in the group. The B parameter controls approximation accuracy at the cost of memory. Higher values yield better approximations, and the default is 10,000. When the number of distinct values in col is smaller than B, this gives an exact percentile value. || array | percentile_approx(DOUBLE col, array(p1 [, p2]…) [, B]) | Same as above, but accepts and returns an array of percentile values instead of a single one. || array&lt;struct {&#39;x&#39;,&#39;y&#39;}&gt; | histogram_numeric(col, b) | Computes a histogram of a numeric column in the group using b non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that represent the bin centers and heights || array | collect_set(col) | Returns a set of objects with duplicate elements eliminated. || array | collect_list(col) | Returns a list of objects with duplicates. (As of Hive 0.13.0.) || INTEGER | ntile(INTEGER x) | Divides an ordered partition into x groups called buckets and assigns a bucket number to each row in the partition. This allows easy calculation of tertiles, quartiles, deciles, percentiles and other common summary statistics. (As of Hive 0.11.0.) | 表生成函数 | 返回值 | 方法名 | 描述 || ——————– | —————————————————— | ———————————————————— || T | explode(ARRAY a) | Explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array. || Tkey,Tvalue | explode(MAP&lt;Tkey,Tvalue&gt; m) | Explodes a map to multiple rows. Returns a row-set with a two columns (key,value) , one row for each key-value pair from the input map. (As of Hive 0.8.0.). || int,T | posexplode(ARRAY a) | Explodes an array to multiple rows with additional positional column of int type (position of items in the original array, starting with 0). Returns a row-set with two columns (pos,val), one row for each element from the array. || T1,…,Tn | inline(ARRAY&lt;STRUCTf1:T1,...,fn:Tn&gt; a) | Explodes an array of structs to multiple rows. Returns a row-set with N columns (N = number of top level elements in the struct), one row per struct from the array. (As of Hive 0.10.) || T1,…,Tn/r | stack(int r,T1 V1,…,Tn/r Vn) | Breaks up n values V1,…,Vn into r rows. Each row will have n/r columns. r must be constant. || string1,…,stringn | json_tuple(string jsonStr,string k1,…,string kn) | Takes JSON string and a set of n keys, and returns a tuple of n values. This is a more efficient version of the get_json_object UDF because it can get multiple keys with just one call. || string 1,…,stringn | parse_url_tuple(string urlStr,string p1,…,string pn) | Takes URL string and a set of n URL parts, and returns a tuple of n values. This is similar to the parse_url() UDF but can extract multiple parts at once out of a URL. Valid part names are: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:. |]]></content>
      <categories>
        <category>hadoop</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-ssl]]></title>
    <url>%2F2020%2F05%2F09%2Fhadoop-ssl%2F</url>
    <content type="text"><![CDATA[常见SSL认知 双认证形式，如：nifi，双方都生产各自的keystone，并导出包含自己证书的truststore，再将truststore互换即可进行通讯，如果服务端会有多个客户端连接，那么服务端的truststore需要加入客户端的证书，并且客户端truststore也需要导入服务端的证书，最终就导致服务端的truststore包含多条信任的证书。 单认证形式，如：flume，服务端需要验证客户端，客户端不需要验证服务端，服务端生成keystone，并导出truststore给客户端即可通讯，如果服务端会有多个客户端连接，那么服务端的keystone需要再导入一个新生成的keystone，并把这个keystone的truststore导入给客户端即可。 在JKS中，keystone和truststore本质上是同一种类型文件，都是用来存放私钥和公钥以及证书的，只是为了方便使用，将keystone用来存放秘钥，将truststore存放信任的证书。 合并Keystone和truststore添加证书不需要重启服务器。 keytool工具使用 生成keystone，其中存放私钥和公钥的证书 1keytool -genkey -alias feng -keyalg RSA -keysize 2048 -validity 10950 -keystore keystore.jks -dname &quot;CN=feng, OU=software, O=tepia.com, L=wuhan, ST=hubei, C=CN&quot; -keypass 123456 -storepass 123456 OU表示组织内的单元，也就是组织的部门 keypass表示秘钥的密码 storepass表示keystone容器的密码 导出证书 1keytool -export -alias feng -keystore keystore.jks -rfc -file feng.cer -storepass 123456 将证书导入到truststore 1keytool -import -alias feng -file feng.cer -keystore truststore.jks 查看keystone或truststore中的秘钥或证书 1keytool -list -v -keystore keystore.jks 查看证书信息 1keytool -printcert -file feng.cer keystone中添加一个keystone（添加私钥和公钥证书） 1keytool -importkeystore -srckeystore xiang/keystore.jks -destkeystore keystore.jks 删除证书或这私钥 1keytool -delete -alias xiang -keystore keystore.jks 更改证书类型 12openssl x509 -inform DER -in certificate.cer -out certificate.crtopenssl x509 -inform PEM -in certificate.cer -out certificate.crt 三方签名证书 由于三方签名证书都存在中间证书，单keytool工具无法导出整个证书链，所以需要先转换为pkcs12，再导出 更改格式为pkcs12 1keytool -importkeystore -srckeystore iot.eblssmart.com.jks -destkeystore iot.eblssmart.com.pfx -deststoretype pkcs12 更改格式为pem 1openssl pkcs12 -in iot.eblssmart.com.pfx -nodes -out iot.eblssmart.com.pem 导出私钥 1openssl rsa -in iot.eblssmart.com.pem -out iot.eblssmart.com.key 导出证书 1openssl x509 -in iot.eblssmart.com.pem -out iot.eblssmart.com.crt]]></content>
      <categories>
        <category>hadoop</category>
        <category>ssl</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>ssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-hortonwork-sandbox]]></title>
    <url>%2F2020%2F04%2F20%2Fhadoop-hortonwork-sandbox%2F</url>
    <content type="text"><![CDATA[安装的过程中会出现端口冲突，直接修改 sandbox/proxy/proxy-deploy.sh 配置就可了， 如： -p 31080 1080 \ 修改docker 的root密码, 直接登录进入docker就会提示更改密码 更改ambari admin 密码， 终端下执行 ambari-admin-password-reset mysql无法启动 mv /usr/sbin/mysqld /usr/bin/mysqld ln -nfs /usr/bin/mysqld /usr/sbin/mysqld 修改hdfs的yarn日志用户, Ambari &gt; HDFS &gt; Configurations&gt;custorm core-site &gt; Add Property hadoop.http.staticuser.user = yarn 修改ranger admin UI密码登入， 保持下面密码全部都是一样的， 并且密码长度大于等于9位 注意： 设置ranger的rangerdba连接的时候要明确指定不需要SSL，如下： jdbc:mysql://hdp1.ilivoo.com:3306?useSSL=false 更改ambari admin 密码， 终端下执行 ambari-admin-password-reset 更改ranger admin for Ambari密码， ranger -&gt; config -&gt; advance -&gt; Ranger Admin user’s password for Ambari(Admin Settings下) 更改ranger Admin 密码， ranger -&gt; config -&gt; advance -&gt; Ranger Admin user’s password(Ranger-env下) 更改mysql root用户以方便以后对mysql的更改， 在my.conf中加入skip-grant-tables跳过授权，更改mysql root用户密码 update mysql.user set authentication_string=password(&#39;mypasswd&#39;) where user=&#39;root&#39;; 更改ranger需要的mysql root账户密码， ranger -&gt; config -&gt; ranger admin -&gt; Database Administrator (DBA) password 更改ranger admin账户的登录密码 update ranger.x_portal_user set password = &#39;ceb4f32325eda6142bd65215f4c0f371&#39; where login_id= &#39;admin&#39;; 此password是有用户名和密码进行MD5求取出来的 echo -n &#39;password{admin}&#39; | md5sum 使用同样(f步骤)的方法更改amb_ranger_admin的登录密码 修改 Atlas admin密码 Configs-&gt; Advanced -&gt; Advanced atlas-env -&gt; Admin password mysql 相关配置 max_connect_errors=10000 validate_password=OFF 修改sandbox-proxy， 让其暴露出kerberos认证需要的三个端口 修改 sandbox/proxy/proxy-deploy.sh 添加如下内容： 123-p 749:749 \-p 464:464 \-p 88:88 \ 修改 sandbox/proxy/conf.stream.d/tcp-hdp.conf 添加如下内容： 123456789101112server &#123; listen 749; proxy_pass sandbox-hdp:749;&#125;server &#123; listen 464; proxy_pass sandbox-hdp:464;&#125;server &#123; listen 88; proxy_pass sandbox-hdp:88;&#125; 注意： 由于kerberos需要的是tcp端口， 所以我们只需要该tcp的配置就可以了， 如果需要http端口则可以修改http配置]]></content>
      <categories>
        <category>hadoop</category>
        <category>hortonwork</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hortonwork</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-aria2]]></title>
    <url>%2F2020%2F04%2F17%2Flinux-aria2%2F</url>
    <content type="text"><![CDATA[安装epel 12wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpm 安装Aria2 1yum install aria2 -y 创建配置文件并编辑 12mkdir /etc/aria2/ #创建目录vi /etc/aria2/aria2.conf #创建配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445#用户名#rpc-user=user#密码#rpc-passwd=passwd#上面的认证方式不建议使用,建议使用下面的token方式#设置tokenrpc-secret=token#允许rpcenable-rpc=true#允许所有来源, web界面跨域权限需要rpc-allow-origin-all=true#允许外部访问，false的话只监听本地端口rpc-listen-all=true#RPC端口, 仅当默认端口被占用时修改rpc-listen-port=6800#最大同时下载数(任务数), 路由建议值: 3max-concurrent-downloads=5#断点续传continue=true#同服务器连接数max-connection-per-server=50#最小文件分片大小, 下载线程数上限取决于能分出多少片, 对于小文件重要min-split-size=10M#单文件最大线程数, 路由建议值: 5split=10#下载速度限制max-overall-download-limit=0#单文件速度限制max-download-limit=0#上传速度限制max-overall-upload-limit=0#单文件速度限制max-upload-limit=0#断开速度过慢的连接#lowest-speed-limit=0#验证用，需要1.16.1之后的release版本#referer=*#文件保存路径, 默认为当前启动位置dir=/root/downloads#文件缓存, 使用内置的文件缓存, 如果你不相信Linux内核文件缓存和磁盘内置缓存时使用#disk-cache=0#另一种Linux文件缓存方式enable-mmap=true#文件预分配, 能有效降低文件碎片, 提高磁盘性能. 缺点是预分配时间较长file-allocation=falloc 安装httpd 1yum -y install httpd 获取前端UI 12git clone https://github.com/ziahamza/webui-aria2mv webui-aria2 /var/www/html/ 测试aria2安装是否成功 启动 aria2c --enable-rpc --rpc-listen-all 浏览器打开 http://localhost/webui-aria2/docs ，成功连接表示没有问题 正式启动 1aria2c --conf-path=/etc/aria2/aria2.conf -D 浏览器正式连接，需要设置token]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 字符编码]]></title>
    <url>%2F2019%2F12%2F23%2Fjava-unicode%2F</url>
    <content type="text"><![CDATA[Unicode基本介绍 Unicode和UTF-8/UTF-16/UTF-32之间就是字符集和编码的关系。字符集的概念实际上包含两个方面，一个是字符的集合，一个是编码方案。 字符集定义了它所包含的所有符号，狭义上的字符集并不包含编码方案，它仅仅是定义了属于这个字符集的所有符号。但通常来说，一个字符集并不仅仅定义字符集合，它还为每个符号定义一个二进制编码。当我们提到GB2312或者ASCII的时候，它隐式地指明了编码方案是GB2312或者ASCII，在这些情况下可以认为字符集与编码方案互等。 但是Unicode具有多种编码方案。Unicode字符集规定的标准编码方案是UCS-2（UTF-16），用两个字节表示一个Unicode字符（UTF-16中两个字节的为基本多语言平面字符，4个字节的为辅助平面字符）。而UCS-4(UTF-32）用4个字节表示一个Unicode字符。另外一个常用的Unicode编码方案–UTF-8用1到4个变长字节来表示一个Unicode字符，并可以从一个简单的转换算法从UTF-16直接得到。所以在使用Unicode字符集时有多种编码方案，分别用于合适的场景。 再通俗一点地讲，Unicode字符集就相当于是一本字典，里面记载着所有字符以及各自所对应的Unicode码（与具体编码方案无关），UTF-8/UTF-16/UTF-32码就是Unicode码经过相应的公式计算得到的并且实际存储、传输的数据。 UTF-16，JVM规范中明确说明了java的char类型使用的编码方案是UTF-16，所以先来了解下UTF-16。 Unicode的编码空间从U+0000到U+10FFFF，共有1112064个码位(code point)可用来映射字符，码位就是字符的数字形式, 一共占int类型数值的低21位。这部分编码空间可以划分为17个平面， 由前面的U+00 - U+10来表示, 每个平面包含2^16（65536）个码位， 由后面的U+0000 - U+FFFF表示。第一个平面称为基本多语言平面（Basic Multilingual Plane, BMP），或称第零平面（Plane 0）。其他平面称为辅助平面（Supplementary Planes）。基本多语言平面内，从U+D800到U+DFFF之间的码位区块是永久保留不映射到Unicode字符。UTF-16就利用保留下来的0xD800-0xDFFF区段的码位来对辅助平面的字符的码位进行编码。 所谓基本语言平面就是将所有语言中比较常用的字符放在这个小的字符集当中， 其它比较复杂的字符使用辅助平面来进行编码， 这样就能保证常见字符的效率， 并且能够兼顾空间。 最常用的字符都包含在BMP中，用2个字节表示。辅助平面中的码位，在UTF-16中被编码为一对16比特长的码元，称作代理对（surrogate pair），具体方法是： 将码位减去0x10000,得到的值的范围为20比特长的0~0xFFFFF。 高位的10比特的值（值的范围为0~0x3FF）被加上0xD800得到第一个码元或称作高位代理（high surrogate），值的范围是0xD800~0xDBFF=1024.由于高位代理比低位代理的值要小，所以为了避免混淆使用，Unicode标准现在称高位代理为前导代理（lead surrogates）。 低位的10比特的值（值的范围也是0~0x3FF）被加上0xDC00得到第二个码元或称作低位代理（low surrogate），现在值的范围是0xDC00~0xDFFF=1024.由于低位代理比高位代理的值要大，所以为了避免混淆使用，Unicode标准现在称低位代理为后尾代理（trail surrogates）。 例如U+10437编码: 0x10437减去0x10000(最小的补充字符， 比基本平面最大值大1， 0x10000 = 0xFFFF + 1), 结果为0x00437, 二进制为0000 0000 0100 0011 0111。 分区它的上10位值(刚好就是高位的1024)和下10位值(刚好就是低位的1024), （使用二进制）:0000000001 and 0000110111。 添加0xD800(高位的起始位置)到上值，以形成高位：0xD800 + 0x0001 = 0xD801。 添加0xDC00(低位的起始位置)到下值，以形成低位：0xDC00 + 0x0037 = 0xDC37。 这就是讲int类型的有效字符， 变成基本平面的char表示形式， 可能为基本平面有效字符， 或者一个高位一个低位的补充字符，由于前导代理、后尾代理、BMP中的有效字符的码位，三者互不重叠，搜索时一个字符编码的一部分不可能与另一个字符编码的不同部分相重叠。所以可以通过仅检查一个码元（构成码位的基本单位，2个字节）就可以判定给定字符的下一个字符的起始码元。 基本平面的显示 |_＿＿＿＿|__＿＿＿|__＿＿|___＿＿＿＿＿| ​ 0 0XD800 0XDBFF 0XDFFF 0XFFFF 从上图可以看到由于基本平面中只使用 0 - 0XD800 &amp;&amp; 0XDFFF - 0XFFFF = 65535 - 1024 2 = 63487, 而高位和地位分别占用 1024, 所以高位以及地位能够表达的值范围就是两个相乘，1024 1024 = 1048576, 所以总共的编码量应该是 63487 + 1048576 = 1112063。 可以看到此设计还是相当巧妙的， 它保证了整个编码来说要么使用2字节基本编码， 要么使用4字节的双字符， 并且任意的2字节单字符都可以显示出下一个字符是否是这个字符的增补。 总共的大小为 0X10FFFF = 1114111 = 1112063 + 1024 * 2(不映射字符)， 一个只需要int类型的21位， 那么如何将这个值映射成unicode 16中的字符就是所谓的unicode编码算法了。 java中的char Character String之间的关系 java的jvm当中字符操作使用Unicode编码， 也就是unicode16编码方式， 并且使用char 2字节来对应Unicode的基本两字节， 也就是说char就是Unicode16的基本平面， 而基本平面 = 基本平面的有效字符 （0 - 0XD800) &amp; (0XDFFF - 0XFFFF) + 高位(0XD800 - 0XDBFF) + 低位(0XDC00 - 0XDFFF) char 的包装类型 Character就是包装一个简单的char值， 要么是一个有效字符， 要么是一个辅助平面的高位， 要么是辅助平面的低位， 并且它提供了很多的方法去判断当前char是否是一个有效字符， 是否是一个高位或者低位。 对于Character中有些方法只支持char类型参数， 表示支持基本平面的操作， 有的方法提供了int类型数和char类型参数表示支持所有的unicode 16字符的操作。 对于String来说它使用char[]数组作为底层的表示形式， 如果一个char是高位， 那么它的下一个char就应该是低位， 不然就不能表示一个字符了。也就是说它是unicode 16编码的， 如果要将其变成其它的编码需要对其进行转换。 通常情况下我们在内存中使用unicode16编码， 内存中空间和时间的一个权衡； 并且在存储的时候转换成对于语言的编码(如GBK)，更加节省磁盘； 在传输的时候使用utf8编码， 方便支持多语言并且更加节省带宽。 所以通常情况下在讲磁盘上的数据读取到内存当中时候，读取的是byte类型的数组， 首先讲数据转换成jvm使用unicode 16编码， 并且在处理之后写入到磁盘中时， 转换成具体编码的字节流并写入到磁盘。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmuxinator安装与使用]]></title>
    <url>%2F2019%2F09%2F26%2Ftools-tmuxinator%2F</url>
    <content type="text"><![CDATA[安装tmuxinator1234567891011121314https://github.com/tmuxinator/tmuxinatorsudo apt-get install ruby-fullsudo gem install tmuxinator设置编辑器， 由于tmuxinator编辑项目$EDITOR = vimtmuxinator默认的存放项目位置~/.config/tmuxinator配置zsh命令简写和自动完成 TAB键a. 在.zshrc中添加source ~/.bin/tmuxinator.zshb. 拷贝自动完成脚本cp ~/.bin/tmuxinator.fish ~/.config/fish/completions/注意：如果不存在~/.bin目录，那么自己下载到这个文件夹文件路径 https://github.com/tmuxinator/tmuxinator/tree/master/completion/ 基本命令1234567tmuxinator new/edit/open project 创建/编辑/打开项目， 可以使用n/e/o简洁表示tmuxinator start [PROJECT] [ARGS] 启动一个项目tmuxinator stop [PROJECT] 停止一个项目tmuxinator list 显示所有项目tmuxinator delete [PROJECT1] [PROJECT2] 删除一个或者多个项目tmuxinator copy [EXISTING] [NEW] 复制一个项目tmuxinator debug [PROJECT] [ARGS] debug一个项目 示例123456789101112131415161718192021222324252627282930# 项目名称name: sample# 项目主目录root: ~/# 所有的窗口创建之前执行的命令pre_window: echo &quot;hello world&quot;# 所有的windowswindows: # windows 1 editor, 指定名字为myeditor - editor: myeditor # 指定当前window的主目录 root: ~/projects/editor # layout 指定当前window的排版方式， 默认有5种 # 也可以自己定制， 方法在下面 layout: main-vertical # 当前windows下所有的panes， 使用layout指定的排版方式排版 panes: # 面板一， 此面板只有命令vim， 所以这样写就可以了 - vim # 面板二， 此面板存在多一个命令， 将多个命令回车添加 # 必须上一个命令执行成功才会执行下一个命令 - logs: - ssh logs@example.com - cd /var/logs - tail -f development.log # window 2， 可以看到单个窗口一个命令， 直接写就可以了 - server: bundle exec rails s # window 3， 同上 - logs: tail -f log/development.log 使用环境变量1root: &lt;%= ENV[&quot;MY_CUSTOM_DIR&quot;] %&gt; 使用参数传递 位置参数 123tmuxinator start project foo# 此处的args[0]就是fooroot: ~/&lt;%= @args[0] %&gt; 命名参数 123tmuxinator start project workspace=workspace/todo# 此处的workspace 就是上面设置的值workspace/todoroot: ~/&lt;%= @settings[&quot;workspace&quot;] %&gt; tmux默认5种layout12345even-horizontaleven-verticalmain-horizontalmain-verticaltiled tmux自定义layout 打印layout 12$ tmux list-windows1: bash* (4 panes) [211x47] [layout 9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125;] @3 (active) 截取layout 19a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125; 配置layout 1layout:9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[tmux安装与使用]]></title>
    <url>%2F2019%2F09%2F26%2Ftools-tmux%2F</url>
    <content type="text"><![CDATA[安装tmux 1https://github.com/gpakosz/.tmux 设置tmux 1234567set-option -g allow-rename off每一次窗口执行不同的命令窗口的名字就会发生变化， 此处是关闭自动命名set-window-option -g pane-base-index 1设置窗口pane的下标从1开始# on Linux, this requires xsel or xcliptmux_conf_copy_to_os_clipboard=true设置tmux剪切板与系统剪切板同步 使用tmux tmux使用来管理回话并提供分屏等功能的软件， 当tmux开启时， 就会启动一个tmux-server的程序，用来管理上面所有的session会话， 每个session会话中可以存在多个windows（窗口）， 每个窗口又存在多个panel（面板） tmux命令 12345678910tmux + tab 列出所有tmux可以使用的命令tmux new-session -s name 创建sessiontmux list-session 查看sessiontmux attach-session name attach到某个sessiontmux detach 脱离某个sessiontmux kill-session name 杀死某个sessiontmux info 查看session, window, pane, 运行的进程号tmux list-keys 列出所有可以的快捷键和其运行的 tmux 命令tmux list-commands 列出所有的 tmux 命令及其参数tmux kill-server 关闭所有 session 注意 所有的上面这些命令在没进入某个session之前直接使用tmux command args来调用， 当进如了tmux之后直接在任何的panel中通过 &lt;pre&gt; + : +command args来激活命令行操作， 此时所有的命令只需要使用 command args就可以调用了 tmux 基本操作 1234567? 列出所有快捷键；按q返回d 脱离当前会话,可暂时返回Shell界面s 选择并切换会话；在同时开启了多个会话时使用: 进入命令行模式；此时可输入支持的命令，例如 kill-server 关闭所有tmux会话, rename 重命名当前session[ 复制模式，光标移动到复制内容位置，空格键开始，方向键选择复制，回车确认，q/Esc退出， 可以设置vi模式复制] 进入粘贴模式，粘贴之前复制的内容，按q/Esc退出t 显示当前的时间 4. tmux 会话操作 123:new 启动新会话s 列出所有会话$ 重命名当前会话 5. tmux 窗口操作 123456789101112c 创建新窗口&amp; 关闭当前窗口[0-9] 数字键切换到指定窗口ctrl + h/l 左右切换窗口（按住ctrl， 并快速的按h/l）w 通过窗口列表切换窗口, 重命名当前窗口，便于识别. 修改当前窗口编号，相当于重新排序f 在所有窗口中查找关键词，便于窗口多了切换swap-window -s 3 -t 1 交换 3 号和 1 号窗口swap-window -t 1 交换当前和 1 号窗口move-window -t 1 移动当前窗口到 1 号 123tmux new-window 添加一个windowtmux select-window -t 0 选择第0个窗口tmux kill-window -t 1 杀死某个窗口 6. tmux 面板操作 123456789101112131415&quot;/- 将当前面板上下分屏%/_ 将当前面板左右分屏x 关闭当前分屏! 将当前面板置于新窗口,即新建一个窗口,其中仅包含当前面板z 最大化当前所在面板ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小H/J/K/L 以1个单元格移动边缘以调整当前面板大小q 显示面板编号o 选择当前窗口中下一个面板方向键/h/j/k/l 移动光标选择对应面板&#123; 向前置换当前面板&#125; 向后置换当前面板alt+o 逆时针旋转当前窗口的面板ctrl+o 顺时针旋转当前窗口的面板 7. tmux 滚屏操作，由于tmux接管如果输出大量的数据无法看到上面屏幕的字， 如果此时鼠标操作关闭，那么当使用鼠标滑轮的时候显示的执行过命令的显示， 而不能屏幕翻页， 要想达到这个目的有两个办法： 12打开鼠标操作， 使用滑轮上下翻页 &lt;pre&gt; + m打开复制模式， 使用vi的翻滚策略来操作， 此时又会碰到一个问题， 如果前缀是ctrl + b那么不好往下整页翻滚， 建议使用半屏翻滚 ctrl + u / d(f) 8. 复制模式 按下 `PREFIX-[` 进入文本复制模式。可以使用方向键在屏幕中移动光标。默认情况下，方向键是启用的。在配置文件中启用 Vim 键盘布局来切换窗口、调整窗格大小。Tmux 也支持 Vi 模式。要是想启用 Vi 模式，只需要把下面这一行添加到 .tmux.conf 中： 1setw -g mode-keys vi 启用这条配置后，就可以使用 h、j、k、l 来移动光标了。 想要退出文本复制模式的话，按下回车键就可以了。 然后按下 `PREFIX-]` 粘贴刚才复制的文本。 一次移动一格效率低下，在 Vi 模式启用的情况下，可以辅助一些别的快捷键高效工作。 例如，可以使用 w 键逐词移动，使用 b 键逐词回退。使用 f 键加上任意字符跳转到当前行第一次出现该字符的位置，使用 F 键达到相反的效果。]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zsh安装与使用]]></title>
    <url>%2F2019%2F09%2F26%2Ftools-zsh%2F</url>
    <content type="text"><![CDATA[安装zsh 12sudo apt-get install zsh #安装zshchsh -s /bin/zsh #不需要sudo，重启 安装oh-my-zsh 1https://github.com/robbyrussell/oh-my-zsh 安装zsh插件 123456789vi-modeautojump 需要安装软件zsh-autosuggestions https://github.com/zsh-users/zsh-autosuggestionslast-working-dir d 在终端输入d 显示最近频繁进入的路径，然后输入路径前对应的序号可快速进入对应路径history 用法：在终端输入h即可zsh-syntax-highlighting https://github.com/zsh-users/zsh-syntax-highlightingweb-search 如：google StackOverflow， 指定搜搜引擎就可以了sublime 需要安装软件， [s]st [|文件|文件夹] [管理员]打开 [当前目录|文件|文件夹]]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim 安装和使用技巧]]></title>
    <url>%2F2019%2F09%2F26%2Ftools-vim%2F</url>
    <content type="text"><![CDATA[vim使用 16进制编辑 123456789使用vim 打开需要编辑的文件， 如： Hello.class vim -b Hello.class进入xxd转换程序模式 :%!xxd查找并修改文件16进制部分退出xxd转换程序模式（必须退出， 不然以xxd显示进行保存） :%!xxd -r推出vim :wq 原生vim 更新vim到最新版本 1sudo add-apt-repository ppa:jonathonf/vim 安装vim 1https://github.com/amix/vimrc 添加自动更新到crontab中 10 */6 * * * cd /home/feng/.vim_runtime &amp;&amp; git pull --rebase &gt; /dev/null 2&gt;&amp;1 插件快捷键讲解 12345ack.vim 内容搜索 &lt;leader&gt; + gctrlp.vim 文件搜索 &lt;leader&gt; + jbufexplorer.zip 缓存查看 &lt;leader&gt; + omru.vim 最近文件 &lt;leader&gt; + fNERD Tree 文件目录 &lt;leader&gt; + nn 使用neovim(vim的替代品,推荐) 安装 1234567891011sudo apt-get install software-properties-commonsudo add-apt-repository ppa:neovim-ppa/stablesudo apt-get updatesudo apt-get install neovimsudo apt-get install python-dev python-pip python3-dev python3-pipsudo update-alternatives --install /usr/bin/vi vi /usr/bin/nvim 60sudo update-alternatives --config visudo update-alternatives --install /usr/bin/vim vim /usr/bin/nvim 60sudo update-alternatives --config vimsudo update-alternatives --install /usr/bin/editor editor /usr/bin/nvim 60sudo update-alternatives --config editor 安装插件 123456789101112131415161718curl -fLo ~/.vim/autoload/plug.vim --create-dirs \ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim# 将已有的 Vim 配置，用于 nvim# .vim 文件夹会存放插键相关内容，比如 vim-plug 的内容ln -s ~/.vim ~/.config/nvimln -s ~/.vimrc ~/.config/nvim/init.vim# 安装插键:PlugInstall# 检查状态:PlugStatus# 删除插键（需要先将 ~/.config/nvim/init.vim 中注释掉相关插键）:PlugClean# 更新插键: PlugUpdate# 升级 vim-plug:PlugUpgrade 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119call plug#begin(&apos;~/.local/share/nvim/plugged&apos;)Plug &apos;vim-airline/vim-airline&apos;Plug &apos;majutsushi/tagbar&apos;Plug &apos;kien/ctrlp.vim&apos;Plug &apos;tacahiroy/ctrlp-funky&apos;Plug &apos;mileszs/ack.vim&apos;Plug &apos;yegappan/mru&apos;Plug &apos;ianva/vim-youdao-translater&apos;Plug &apos;yonchu/accelerated-smooth-scroll&apos;Plug &apos;jlanzarotta/bufexplorer&apos;Plug &apos;mhinz/vim-startify&apos;Plug &apos;scrooloose/nerdtree&apos;, &#123; &apos;on&apos;: &apos;NERDTreeToggle&apos; &#125;Plug &apos;easymotion/vim-easymotion&apos;Plug &apos;Valloric/YouCompleteMe&apos;call plug#end()set nobackupset nowbset noswapfile&quot;set numbernnoremap &lt;F5&gt; :set nonumber!&lt;CR&gt;:set foldcolumn=0&lt;CR&gt; &lt;Paste&gt;set hlsearchset ignorecaseset incsearchset smartcasefiletype plugin indent onset tabstop=4set softtabstop=4set shiftwidth=4set expandtab&quot;快捷键的映射，通过不同的前缀定义快捷键在那种模式下启动&quot;nore前缀 非递归&quot;n前缀 普通实施生效&quot;v前缀 可视模式生效&quot;i前缀 插入模式生效&quot;c前缀 在EX命令模式生效&quot;定义前缀键:let mapleader=&quot;;&quot;&quot;定义NERDTree出发快捷键map &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;autocmd bufenter * if (winnr(&quot;$&quot;) == 1 &amp;&amp; exists(&quot;b:NERDTree&quot;) &amp;&amp; b:NERDTree.isTabTree()) | q | endif&quot;定义文件结构快捷键nmap &lt;F3&gt; :TagbarToggle&lt;CR&gt;&quot;定义文件搜索相关&quot;let g:ctrlp_map = &apos;&lt;c-p&gt;&apos;let g:ctrlp_map = &apos;&lt;F8&gt;&apos;let g:ctrlp_cmd = &apos;CtrlP&apos;let g:ctrlp_working_path_mode = &apos;ra&apos;set wildignore+=*/tmp/*,*.so,*.swp,*.ziplet g:ctrlp_custom_ignore = &#123; \ &apos;dir&apos;: &apos;\v[\/]\.(git|hg|svn)$&apos;, \ &apos;file&apos;: &apos;\v\.(exe|so|dll)$&apos;, \ &apos;link&apos;: &apos;some_bad_symbolic_links&apos;, \ &#125;let g:ctrlp_user_command = [&apos;.git&apos;, &apos;cd %s &amp;&amp; git ls-files -co --exclude-standard&apos;]let g:ctrlp_working_path_mode=0let g:ctrlp_match_window_bottom=1let g:ctrlp_max_height=15let g:ctrlp_match_window_reversed=0let g:ctrlp_mruf_max=500let g:ctrlp_follow_symlinks=1&quot;ctrl + j/k 进行上下选择&quot;ctrl + x 在当前窗口水平分屏打开文件&quot;ctrl + v 同上, 垂直分屏&quot;ctrl + t 在tab中打开&quot;定义文件中搜索函数nnoremap &lt;Leader&gt;f :CtrlPFunky&lt;Cr&gt;nnoremap &lt;Leader&gt;fu :execute &apos;CtrlPFunky &apos; . expand(&apos;&lt;cword&gt;&apos;)&lt;Cr&gt;let g:ctrlp_funky_syntax_highlight = 1let g:ctrlp_extensions = [&apos;funky&apos;]&quot;定义内容搜索快捷键let g:ackprg = &apos;ag --nogroup --nocolor --column&apos;map &lt;F7&gt; :Ack&lt;space&gt;if executable(&apos;ag&apos;) &quot; Use Ag over Grep set grepprg=ag\ --nogroup\ --nocolor &quot; Use ag in CtrlP for listing files. Lightning fast and respects .gitignore let g:ctrlp_user_command = &apos;ag %s -l --nocolor -g &quot;&quot;&apos; &quot; ag is fast enough that CtrlP doesn&apos;t need to cache let g:ctrlp_use_caching = 0endif&quot;定义缓冲文件快捷键nnoremap &lt;silent&gt; &lt;F9&gt; :BufExplorer&lt;CR&gt;&quot;定义最近文件快捷键nnoremap &lt;silent&gt; &lt;F10&gt; :MRU&lt;CR&gt;let MRU_Window_Height = 10&quot;jk # 在历史记录列表中上下移动&quot;Enter # 进入相应文件&quot;o # 水平方向打开&quot;t # 在新标签中打开历史文件&quot;u # 更新文件列表&quot;q # 退出MRU历史记录&quot;v # 以只读模式打开&quot;/ # 在MRU列表中快速查找&quot;定义有道翻译快捷键vnoremap &lt;silent&gt; &lt;C-T&gt; :&lt;C-u&gt;Ydv&lt;CR&gt;nnoremap &lt;silent&gt; &lt;C-T&gt; :&lt;C-u&gt;Ydc&lt;CR&gt;noremap &lt;leader&gt;yd :&lt;C-u&gt;Yde&lt;CR&gt;&quot;定义跳转快捷键map &lt;Leader&gt;w &lt;Plug&gt;(easymotion-bd-w)nmap &lt;Leader&gt;w &lt;Plug&gt;(easymotion-overwin-w)&quot;YouCompleteMe快捷键nnoremap &lt;leader&gt;gl :YcmCompleter GoToDeclaration&lt;CR&gt;nnoremap &lt;leader&gt;gf :YcmCompleter GoToDefinition&lt;CR&gt;nnoremap &lt;leader&gt;gg :YcmCompleter GoToDefinitionElseDeclaration&lt;CR&gt;]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime安装与使用]]></title>
    <url>%2F2018%2F09%2F26%2Ftools-sublime%2F</url>
    <content type="text"><![CDATA[安装直接网上搜索，并对其破解 插件安装 123451. ConvertToUTF8 和 GBK Support2. Pretty Json3. SideBarEnhancements4. vim5. MarkdownPreview 快捷键 12341、使用Ctrl + `：打开或者关闭Sublime Text控制台。SublimeText控制台就是一个python的命令窗口。2、Ctrl+Shift+P：打开或者关闭命令行模式。3、Alt+Shift+2：分屏显示。( Alt+Shift+数字 )表示分几屏。默认即1，最大貌似是5。4、Sublime Text的一大亮点是支持多重选择——同时选择多个区域，然后同时进行编辑。Ctrl + D选择当前光标所在的词并高亮该词所有出现的位置，再次Ctrl + D选择该词出现的下一个位置。]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongo spark partitioner 详解]]></title>
    <url>%2F2018%2F09%2F23%2Fmongo-spark-partitioner%2F</url>
    <content type="text"><![CDATA[DefaultMongoPartitioner 默认partitioner使用mongodb的抽样聚合操作( mongo版本需要大于3.2)，通过抽样来确定分区点, 对所有的mongo部署方式都可以 123partitionKey 分区键partitionSizeMB 每个分区大小（没有太多实质性的意义,所以并不准）samplesPerPartition 每个分区抽样个数 第一步通过 collStats 聚合命令获取集合的统计信息 如果有 match 聚合操作就使用 countDocuments(match) 方法获取集合的数量 抽样获取集合分区键，并对集合进行分区 MongoSplitVectorPartitioner 通过 SplitVector命令来进行分区，需要 ClusterManager 权限， 这个命令的最初意图是对集合进行split，也就是对集群生成chunk分裂计划，所以mongo spark就借鉴这个命令进行分区划分， 主要对单节点或者复制集群使用 注意：适合 match 聚合中只使用 $gt 和 $lt 类型的比较符, 如果不是的那么它使用最小和最大键作为分区，这样分区之后由于它不会使用 match 所以可能导致分区并不是特别合适 12partitionKey 分区键partitionSizeMB 每个分区大小（比较准确的值） MongoShardedPartitioner 直接对sharded集合进行分区，因为sharded集合本身就是分区的，所以mongo spark直接使用 chunk 大小作为分区大小，只能对shard集群使用，并且它还会给出 chunk 的具体位置 1shardKey shard键 MongoPaginateByCountPartitioner 通过分页数量来对分区，由于需要多次执行分页操作，而且越到大的分区其实是越慢的，所以没什么用，可以对任何部署方式 12partitionKey 分区键numberOfPartitions 每个分区数量 MongoPaginateBySizePartitioner 通过分页集合大小来进行分区，和上面道理差不错多 12partitionKey 分区键partitionSizeMB 每个分区大小 从上面的集中分区方式可以，DefaultMongoPartitioner 之所以被当着默认的是因为它满足分和部署方式，并且性能一般也还可以。但是对于平时使用的过程中建议还是根据集群的部署方式以及 match 条件进行选择合适的 partitioner 如果集群是单机或这复制集群，并且 match 中只有使用 $gt 和 $lt 类型的比较符， 那么推荐使用 MongoSplitVectorPartitioner 如果是shard机器，那么直接使用 MongoShardedPartitioner ，并推荐看看 chunk 的具体位置怎么使用, 在现在mongo spark中并没有使用到这个参数 如果不是上面两种情况，那么直接使用使用 DefaultMongoPartitioner 就好了]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase 常用管理命令]]></title>
    <url>%2F2018%2F08%2F05%2Fhadoop-hbase-admin%2F</url>
    <content type="text"><![CDATA[importtsv 命令参数说明 12345678910111213141516171819202122232425Usage: importtsv -Dimporttsv.columns=a,b,c [options] &lt;tablename&gt; &lt;inputdir&gt;importtsv.columns 导入的列常量HBASE_ROW_KEY 行健HBASE_TS_KEY 可选，put时使用的时间，指定这个列后 importtsv.timestamp 将会被忽略HBASE_CELL_TTL cell time to liveHBASE_CELL_VISIBILITY visibility label expression.HBASE_ATTRIBUTES_KEY key=&gt;value导入方式参数-Dimporttsv.bulk.output 使用bulkload方式导入，并指定bulkload的缓存位置其它参数-Dimporttsv.dry.run=true 测试模式，数据并非真正导入到表中，如果表不存在创建，并在最后删除-Dimporttsv.skip.bad.lines=false 是否跳过错误行-Dimporttsv.log.bad.lines=true log错误行-Dimporttsv.skip.empty.columns=false 是否跳过空的列&apos;-Dimporttsv.separator=|&apos; 使用管道代替tab分隔符-Dimporttsv.timestamp=currentTimeAsLong 使用当前时间-Dimporttsv.mapper.class=my.Mapper 自定义mapper代替org.apache.hadoop.hbase.mapreduce.TsvImporterMapper-Dmapreduce.job.name=jobName 指定job名字-Dcreate.table=no 避免创建表，导入的表必须存在hbase中-Dno.strict=false 是否严格模式，默认false忽略列族检查性能参数 -Dmapreduce.map.speculative=false -Dmapreduce.reduce.speculative=false 简单例子 文件中的数据 12r0001 feng 29r0002 xiang 27 直接导入 1hbase org.apache.hadoop.hbase.mapreduce.Driver importtsv -Dimporttsv.columns=HBASE_ROW_KEY,INFO:name,INFO:age user /user/hbase/user.tsv bulkload导入 导出成HFiles 1hbase org.apache.hadoop.hbase.mapreduce.Driver importtsv -Dimporttsv.columns=HBASE_ROW_KEY,INFO:name,INFO:age -Dimporttsv.bulk.output=/user/hbase/user_bulk user /user/hbase/user.tsv 导入HFiles 1hbase org.apache.hadoop.hbase.mapreduce.Driver completebulkload /user/hbase/user_bulk user export 命令参数说明 123456789101112131415161718192021Usage: Export [-D &lt;property=value&gt;]* &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]] [^[regex pattern] or [Prefix] to filter]]扫描行存在两种过滤器，正则表达式和前缀输出SequenceFile的压缩形式 -D mapreduce.output.fileoutputformat.compress=true -D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec -D mapreduce.output.fileoutputformat.compress.type=BLOCK指定Scan的参数 -D hbase.mapreduce.scan.column.family=&lt;family1&gt;,&lt;family2&gt;, ... -D hbase.mapreduce.include.deleted.rows=true -D hbase.mapreduce.scan.row.start=&lt;ROWSTART&gt; -D hbase.mapreduce.scan.row.stop=&lt;ROWSTOP&gt; -D hbase.client.scanner.caching=100 -D hbase.export.visibility.labels=&lt;labels&gt;，指定权限相关 -D hbase.export.scanner.batch=10 -D hbase.export.scanner.caching=100mapreduce参数 -D mapreduce.job.name=jobName -D mapreduce.map.speculative=false -D mapreduce.reduce.speculative=false 注意： 此处将versions和starttime endtime直接放入到命令行参数当中，可见通常情况下我们都是通过时间来进行增量拷贝 从这里就衍生出一个问题，我们真的需要将时间戳添加到rowkey当中吗？如果需要通过时间快速的查找那么通常情况下是需要的，如果仅仅只是备份那么通常情况下也不介意全表扫描。充分利用时间戳和版本信息。 简单例子 1hbase org.apache.hadoop.hbase.mapreduce.Driver export user /user/hbase/user_export import 命令参数说明 123456789101112131415Usage: Import [options] &lt;tablename&gt; &lt;inputdir&gt;导入方式参数 -Dimport.bulk.output=/path/for/output -Dimport.bulk.hasLargeResult=trueWAL参数 -Dimport.wal.durability=&lt;SKIP_WAL/ASYNC_WAL/SYNC_WAL/...&gt;使用Filter -Dimport.filter.class= -Dimport.filter.args= -Dhbase.import.version=0.94 指定export导出的时hbase版本MapReduce参数 -D mapreduce.job.name=jobName -Dmapreduce.map.speculative=false -Dmapreduce.reduce.speculative=false 简单例子 直接导入 1hbase org.apache.hadoop.hbase.mapreduce.Driver import user /user/hbase/user_export bulkload导入 导出成HFiles 1hbase org.apache.hadoop.hbase.mapreduce.Driver import -Dimport.bulk.output=/user/hbase/user_import_bulk user /user/hbase/user_export 导入HFiles 1hbase org.apache.hadoop.hbase.mapreduce.Driver completebulkload /user/hbase/user_import_bulk user sqoop import1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --table user --hbase-table user --column-family INFO --hbase-row-key id --hbase-bulkload mapreduce import 编写MapReduce程序并打包成jar包 使用hadoop运行jar包，跟通用的MapReduce一样，但是需要指定hbase classpath 1HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath` $&#123;HADOOP_HOME&#125;/bin/hadoop jar user.jar 注意： 如果有classpath的问题，可以参照官网 &lt;http://hbase.apache.org/book.html#hbase.mapreduce.classpath&gt; 如果MapReduce程序虚设置相关参数可以参照 org.apache.hadoop.hbase.mapreduce.Driver 中的基本类进行配置。 copytable import 命令参数说明 1234567891011121314151617181920212223242526272829Usage: CopyTable [--options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;Options: 控制参数 startrow the start row stoprow the stop row starttime beginning of the time range (unixtime in millis) without endtime means from starttime to forever endtime end of the time range. Ignored if no starttime specified. versions number of cell versions to copy families comma-separated list of families to copy To copy from cf1 to cf2, give sourceCfName:destCfName. To keep the same name, just give "cfName" all.cells also copy delete markers and deleted cells bulkload Write input into HFiles and bulk load to the destination table 目标集群信息 rs.class hbase.regionserver.class of the peer cluster specify if different from current cluster rs.impl hbase.regionserver.impl of the peer cluster peer.adr hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent new.name new table's name性能参数 -Dhbase.client.scanner.caching=100 通常应该&gt;=100 -Dmapreduce.map.speculative=false 永远为false Examples: To copy 'TestTable' to a cluster that uses replication for a 1 hour window: $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable 本地集群导入 1hbase org.apache.hadoop.hbase.mapreduce.Driver copytable --families=INFO:info --bulkload --new.name=user2 user rowcounterCellCountersnapshot export 在同一个集群中使用snapshot通常是用来数据的备份与恢复，并且它对整个hbase集群几乎没有影响 snapshot export工具是在两个集群中进行备份和恢复的]]></content>
      <categories>
        <category>hadoop</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop使用教程]]></title>
    <url>%2F2018%2F08%2F01%2Fhadoop-sqoop%2F</url>
    <content type="text"><![CDATA[前置设置 由于sqoop需要执行MapReduce任务，并且通常指定输出为 /user/sqoop 目录，所以需要创建并指定权限 12hdfs dfs -mkdir /user/sqoophdfs dfs -chown sqoop:hdfs /user/sqoop 建议使用需要访问目录的用户去执行sqoop命令 所有的sqoop import需要指定参数 -Dorg.apache.sqoop.splitter.allow_text_splitter=true 每次执行MapReduce任务可能需要删除MapReduce任务输出目录 一般RDBMS的导出速度控制在60~80MB/s，每个 map 任务的处理速度5~10MB/s 估算，即 -m 参数一般设置4~8，表示启动 4~8 个map 任务并发抽取。 官网连接：https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html hortonwork 教程： https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.1.0/migrating-data/content/hive_moving_data_from_hdfs_to_hive.html 使用 --skip-dist-cache 防止每次需要将sqoop/lib中的jars上传到分布式缓存，所以如果sqoop/lib中的jars没有变化，那么每次都应该携带这个参数。 sqoop metastore 用来保存用户元数据信息，如使用sqoop job时候，创建job需要配置相关信息，并且需要保存last value。通常情况下每个用户一个metastore，通过文件的方式存储在 ~/.sqoop目录当中。如果我们根据需要希望启动一个或者多个单独的metastore，那么只需要使用配置 sqoop-site.xml 并启动即可 12sqoop.metastore.server.location=/var/lib/sqoop/metastore.dbsqoop.metastore.server.port=15999 服务端启动 创建 /var/lib/sqoop 和 /var/log/sqoop 文件夹，并分配权限 启动: bin/start-metastore.sh -p /var/log/sqoop/metastore.pid -l /var/log/sqoop 客户端使用 方式一：通过命令行参数 --meta-connect 方式二：在sqoop-site.xml中配置 1sqoop.metastore.client.autoconnect.url=jdbc:hsqldb:hsql://metaserver:15999/sqoop sqoop import 参数说明 MySQL连接参数 --connect 指定连接地址 --username, -P|--password|--password-file 关系型数据库密码选项 注意sqoop是分布式方式运行，那么mysql连接需要在各个机器上都能够访问mysql服务器 导入形式 基本形式导入 指定参数--table --columns --where 需要导出的表、列和条件 此处可以不用指定 --target-dir 是因为模式使用 --table 作为 MapReduce输出目录 示例 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --table r_cond_info 查询形式导入 指定参数 --query --split-by|-m 1 --target-dir 查询语句、并行分区键（或者使用一个分区）和MapReduce输出目录 --split-limit 限制分区最大值，防止MapReduce数据发送倾斜 示例 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --query &apos;select * from r_cond_info where $CONDITIONS&apos; --split-by stcd --target-dir &apos;/user/sqoop/r_cond_info_dir&apos; 说明 此处必须指定 $CONDITIONS ，并且它会在运行的时候进行替换成 --split-by 的条件 这种形式不能使用 --as-orcfile 并行控制 默认情况下使用 select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt; where &lt;split-by&gt; 来进行并行度控制，如果没有指定split-by则使用主键作为分区并行键，如果主键是分布不均匀的最好用split-by指定一个分区键，如果没有主键也没有指定分区键，那么默认情况下会报错，除非强制指定--num-mappers 1 或 --autoreset-to-one-mapper 使用单个mapper。 选择好分区键后，就是指定使用多少分区来运行，使用 --num-mapers 指定，如果存在数据倾斜的情况，可以指定 --split-limit 控制单个分区最大的量，从而影响并行度。 --autoreset-to-one-mapper 通常用在import-all-tables工具中全库导入，没有主键使用单个mapper 基本形式和查询形式都可以进行并行度控制 导入hive参数 --hive-import --hive-database cmsw 基本参数 --hive-table 知道导入到hive中的表（修改默认mysql表名） --create-hive-table 如果hive中表存在，则报错 --hive-overwrite 覆盖以及存在的表 --external-table-dir 指定外部表的位置（这个外部表的位置就是MapReduce结果的位置, 注意运行sqoop的用户，因为此时hive需要访问MapReduce输出目录），这样hive表就是一个外部表，通常和 --target-dir 联合使用 示例 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --table r_comm_info --hive-import --hive-database cmsw --as-orcfile sqoop直接迁移数据到hive当中，是先执行MapReduce任务将RDBMS中的数据按照指定的格式（如：–as-orcfile）写入到HDFS当中，再将HDFS中的数据复制到hive当中 类型转换 --map-column-java 转换sql类型到java类型，如：id=String,value=Integer --map-column-hive 转换sql类型到hive类型，转换的过程中需要转义，如：DECIMAL(1%2C%201) 代替 DECIMAL(1, 1) 导出类型控制 --as-textfile 默认为text类型 --as-avrodatafile、--as-orcfile、--as-parquetfile、--as-sequencefile SequenceFile、text和Avro可以进行压缩，通过指定 --compress 或 -z 指定使用gzip进行压缩，或者指定 --compression-codec 设置hadoop的压缩格式 text和sequence文件导出和导入格式（定界符），默认的定界符：字段 , ，行 \n ，无引号，无转码，这可能会导致字段或者行无法准确区分清楚，如某个String字段中存在逗号或者换行符，那么整个的定界就是错误的，所以为了明确的解析，必须指定定界符。 字段含义 --enclosed-by &lt;char&gt; 必填字段包围符，如：通常会使用 &#39; 单引号或者 &quot; 双引号将字符串包围起来 --escaped-by &lt;char&gt; 转义字符，如通常某些字符需要转义，转义字符通常是 \ --fields-terminated-by 在一行中，字段和字段之间的分隔符，如cvs文件中使用 | 或 , --lines-terminated-by 行之间的分隔符，通常使用换行符 \n --optionally-enclosed-by &lt;char&gt; 可选字段包围符 转义和包围符，如数据库中存在，如下数据 | String | int | int || —————————- | —- | —- || Some string, with a comma. | 1 | 2 || Another “string with quotes” | 4 | 5 | 明确定义符号 sqoop import --fields-terminated-by , --escaped-by \\ --enclosed-by &#39;\&quot;&#39; 12&quot;Some string, with a comma.&quot;,&quot;1&quot;,&quot;2&quot;&quot;Another \&quot;string with quotes\&quot;&quot;,&quot;4&quot;,&quot;5&quot; 使用可选包围符 sqoop import --optionally-enclosed-by &#39;\&quot;&#39; 12&quot;Some string, with a comma.&quot;,1,2&quot;Another \&quot;string with quotes\&quot;&quot;,4,5 hive默认是支持转义的，所以如果直接导入到hive当中，就不需要显示指定转义 --mysql-delimiters 使用 mysqldump 默认的定界符，并使用direct模式，指定 --direct ，那么导入会非常快速，mysql默认的定界符 ，字段 , ，行 \n ， 转义 \ ，可选包围符 &#39; sqoop增量迁移数据到hive中 增量导入，数据通常分为事实数据、可变维度数据、缓慢渐变维数据、快照表数据 对于事实数据通常不会更改，只是简单递增，那么数据每次将最新的数据添加到已有的数据即可，使用append模式，通常append列为自增列 对于可变维度数据 数据量比较小直接每次全量导出，不需要增量导出 如果数据量比较大，初始全量导出，以后每次增量导出新增和变化的数据即可，有两种合并方式 方法一：直接使用 lastmodified + merge-key 合并新旧数据 方法而：直接使用hive的merge语句 缓慢渐变维度数据（如：数据T+1），首先初始化一次，后面每次导出T-1的数据，再将老的数据进行过期（更新的数据），再直接添加新的数据，所以只需要使用append模式即可，但append的列是时间列。 sqoop增量迁移 --incremental 增量模式，append 和 lastmodified append 表示添加模式（单调递增主键不断添加数据），递增键可以是时间 lastmodified 最后修改模式 （存在一个单调递增值，修改数据后会自动增加，如表中存在最后修改时间字段），不支持hive，主要是hive默认是不支持更新。所以通常情况下先将数据进入hdfs当中，再创建hive external表进行管理。此模式通常需要合并原先的数据和新数据（新添加和修改的数据），所以通常需要指定 --merge-key 来合并新旧数据，如果在这个过程没有合并，那么也可以后面单独调用 sqoop merge 来进行合并操作。 --check-column 指定检查的列，列不能为字符类型 --last-value 指定最后增量值，或者最后修改的值 MySQL中的表 123456CREATE TABLE `user` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, `last_modify` datetime DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 hive中的表 12345create table base_user( id int, name string, last_modify string) 使用基于查询的形式进行控制 lastmodified 类型 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --query 'select * from user where last_modify &gt; "2019-01-01 00:00:00" and $CONDITIONS' --split-by id --target-dir '/user/hive/user' --hive-import --hive-database cmsw --hive-table increment_user --create-hive-table --external-table-dir '/user/hive/increment_user' 注意： 每次执行前需要删除 /user/hive/user 、/user/hive/increment_user 以及hive表中的 increment_user 表 使用增量迁移参数进行控制 lastmodified 类型 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --table user --target-dir '/user/hive/user' --incremental lastmodified --check-column last_modify --last-value '2019-01-01 00:00:00' 注意： 使用 --incremental lastmodified 不支持 hive import，只能后续多进行一个步骤 执行完成后需要记录 --last-value 的值供后面使用， 推荐使用sqoop job来执行 最终将数据写入到基础hive表中 1merge into base_user using increment_user on base_user.id = increment_user.id when matched then update set name=increment_user.name, last_modify=increment_user.last_modify when not matched then insert values(increment_user.id, increment_user.name, increment_user.last_modify); 注意： 使用merge语句建议为base表的连接字段添加主键索引，这样速度会明显加快 使用基于查询的形式进行控制 append 类型 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --query &apos;select * from user where id &gt; 0 and $CONDITIONS&apos; --split-by id --target-dir &apos;/user/hive/user&apos; --hive-import --hive-database cmsw --hive-table increment_user --create-hive-table --external-table-dir &apos;/user/hive/increment_user&apos; 使用增量迁移参数进行控制 append 类型 1sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://cmsw1.tepia.com:3306/cmsw --username cmsw --password Cmsw@123 --table user --target-dir &apos;/user/hive/user&apos; --incremental append --check-column id --last-value 0 --hive-import --hive-database cmsw --hive-table increment_user --create-hive-table --external-table-dir &apos;/user/hive/increment_user&apos; [选择] 如果已经创建hive外部表则不需要此步骤 12345CREATE EXTERNAL TABLE increment_user ( id INT, name STRING, last_modify STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/user/hive/user'; 说明 通常情况下对于事实性性的表都是需要根据时间进行分区的 对于维度表通常会进行修改和添加，这样lastmodified就是一个不错的选择了 当然最重要的还是hive支持的ACID和merge语句，这样能让我们更加容易维护整个表 对于只插入性质的表，通常可以创建一个 insert-only transactional 表，并且进行分区（不能直接使用删除语句，但是可以直接删除分区） 1CREATE TABLE T2(a int, b int) TBLPROPERTIES (&apos;transactional_properties&apos;=&apos;insert_only&apos;); 不建议直接将数据从关系型数据库导入到hive中，使用hive中间表可以保证数据ACID 现在也可以直接使用hive的 JdbcStorageHandler 直接访问关系型数据库中的数据，这样更加简单 导入案例 sqoop job 使用 1234567891011sqoop job --create default_sales_order \-- \import \--connect &quot;jdbc:mysql://hdp1.tepia.com:3306/source?useSSL=false&amp;user=feng&amp;password=feng@123&quot; \--table sales_order \--where &quot;entry_date &lt; current_date()&quot; \--hive-import \--hive-table default.sales_order \--incremental append \--check-column entry_date \--last-value &apos;1900-01-01&apos; 说明： 日期类型也可以作为一个自增字段 where添加后面控制，不会导入当前一天的数据，也就是说是通过一天为单位进行递增导入 查看job信息，如 last_value 1sqoop job --show default_sales_order|grep last.value]]></content>
      <categories>
        <category>hadoop</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive集成其它系统]]></title>
    <url>%2F2018%2F08%2F01%2Fhadoop-hive%2F</url>
    <content type="text"><![CDATA[hive与其它系统集成说明 现在hive使用StorageHandler只能创建external表, 外部表不能使用load语法 使用hive作为数据仓库，这样和其它系统直接集成，这样可以打通和其它系统直接的交互，直接导入导出数据 其它系统通过hive进行连接，也可以达到不同系统之间互通导数据 hive使用phoenix PhoenixStorageHandler 配置hive访问phoenix jar包 在hive-env.sh中配置 1export HIVE_AUX_JARS_PATH=$&#123;HIVE_AUX_JARS_PATH&#125;:/usr/hdp/current/phoenix-client/phoenix-5.0.0.3.1.0.0-78-hive.jar 在hive-site.xml中配置 1234&lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt;/usr/hdp/current/phoenix-client/phoenix-5.0.0.3.1.0.0-78-hive.jar &lt;/value&gt;&lt;/property&gt; 配置hive用户访问phoenix的系统表权限，通常使用ranger进行配置 在phoenix中创建测试表 12create table test(id integer not null primary key, name string, age integer);upsert into test values(1, &quot;xiang&quot;, 28); 在hive中创建external表 1234567891011create external table test(id int, name string, age int)STORED BY &apos;org.apache.phoenix.hive.PhoenixStorageHandler&apos;TBLPROPERTIES ( &quot;phoenix.table.name&quot; = &quot;test&quot;, &quot;phoenix.zookeeper.quorum&quot; = &quot;cmsw1.tepia.com,cmsw2.tepia.com,cmsw3.tepia.com&quot;, &quot;phoenix.zookeeper.znode.parent&quot; = &quot;/hbase-unsecure&quot;, &quot;phoenix.zookeeper.client.port&quot; = &quot;2181&quot;, &quot;phoenix.rowkeys&quot; = &quot;id&quot;, &quot;phoenix.column.mapping&quot; = &quot;id:ID, name:NAME, age:AGE&quot;);insert into test values(2, &apos;feng&apos;, 30); 配置普通用户访问hive数据库的功能 说明： 将数据存储Phoenix，本质是HBase提供的SQL。用于后端的查询，要求RT在秒级以内。phoenix作为hbase二级索引的最佳组合，测试过上百万级别的数据构建二级索引最快的能在毫秒内返回。另外还有数据的统计分析及数据处理、机器学习建模，当然可以直接操作phoenix或spark+phoenix。对于离线的建模业务，我们希望使用hive做更复杂的处理。 hive使用HBaseStorageHandler 配置hive访问hbase jar包 123456789101112131415161718192021222324252627&lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt; /usr/hdp/3.1.0.0-78/hbase/lib/commons-lang3-3.6.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-zookeeper-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-mapreduce-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/jackson-annotations-2.9.5.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-shaded-miscellaneous-2.1.0.jar, /usr/hdp/3.1.0.0-78/hbase/lib/jackson-databind-2.9.5.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-hadoop-compat-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-metrics-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-client-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-protocol-shaded-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/jackson-core-2.9.5.jar, /usr/hdp/3.1.0.0-78/hbase/lib/protobuf-java-2.5.0.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-shaded-netty-2.1.0.jar, /usr/hdp/3.1.0.0-78/hbase/lib/metrics-core-3.2.1.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-server-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-hadoop2-compat-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-metrics-api-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-common-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-protocol-2.0.2.3.1.0.0-78.jar, /usr/hdp/3.1.0.0-78/hbase/lib/hbase-shaded-protobuf-2.1.0.jar, /usr/hdp/3.1.0.0-78/hbase/lib/htrace-core4-4.2.0-incubating.jar, /usr/hdp/3.1.0.0-78/hbase/lib/zookeeper.jar &lt;/value&gt;&lt;/property&gt; 在创建hive时，需要添加访问hbase的权限，根据创建或者访问添加具体权限 hive可以设置hbase不需要写walset hive.hbase.wal.enabled=false hbase表不存在，需要添加hbase的创建表权限（设置当前用户有创建hbase_table表的权限） 1234CREATE EXTERNAL TABLE hive_table (key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")TBLPROPERTIES ("hbase.table.name" = "default:hbase_table"); 说明： hbase.columns.mapping 是必须的，这将会和 HBase 表的列族进行验证 hbase.table.name 属性是可选的，默认指定 HBase 表名与 Hive 表名一致 当将 hive_table 表删除，对应的 hbase_table 表不受影响，里面依旧有数据 当删除 hbase_table 表后，再查询 hive_table 表数据 hive外部表不支持load语句，所以可以直接将数据导入到hbase中，或者创建hive内部表使用load导入，再通过 insert ... select ... 语句导入到外部表中 Hive 读取的是 HBase 表最新的数据，并且创建的hbase表默认只有一个version, 可以修改最大version hbase存在，需要添加hbase访问权限 hbase中创建表，并添加数据 123create &apos;person&apos;, &apos;cf&apos;put &apos;person&apos;, &apos;r001&apos;, &apos;cf:name&apos;, &apos;feng&apos;put &apos;person&apos;, &apos;r001&apos;, &apos;cf:age&apos;, &apos;10&apos; 创建hive表，并插入数据 12create external table person(id string, name string, age string) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties ("hbase.columns.mapping" = ":key,cf:name,cf:age");insert into person values('r002', 'xiang', '12'); hive使用KafkaStorageHandler 创建默认json序列化表 1create external table kafka_table(`message_type` string, `stcd` string, `tm` string) STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler' TBLPROPERTIES ("kafka.topic" = "cmsw", "kafka.bootstrap.servers"="cmsw1.tepia.com:6667,cmsw2.tepia.com:6667,cmsw3.tepia.com:6667"); 显示表的信息 show create table kafka_table 123456789101112131415161718192021222324252627CREATE EXTERNAL TABLE `kafka_table`( `message_type` string COMMENT 'from deserializer', `stcd` string COMMENT 'from deserializer', `tm` string COMMENT 'from deserializer', `__key` binary COMMENT 'from deserializer', `__partition` int COMMENT 'from deserializer', `__offset` bigint COMMENT 'from deserializer', `__timestamp` bigint COMMENT 'from deserializer') ROW FORMAT SERDE 'org.apache.hadoop.hive.kafka.KafkaSerDe' STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler' WITH SERDEPROPERTIES ( 'serialization.format'='1') LOCATION select * from (select o.*, u.* from ops_oder as o join dwd_dim_user as u on o.user_id = u.user_id) where create_time &gt;= start_date and create_time &lt;= end_date 'hdfs://tepia/warehouse/tablespace/external/hive/kafka_table' TBLPROPERTIES ( 'bucketing_version'='2', 'hive.kafka.max.retries'='6', 'hive.kafka.metadata.poll.timeout.ms'='30000', 'hive.kafka.optimistic.commit'='false', 'hive.kafka.poll.timeout.ms'='5000', 'kafka.bootstrap.servers'='cmsw1.tepia.com:6667,cmsw2.tepia.com:6667,cmsw3.tepia.com:6667', 'kafka.serde.class'='org.apache.hadoop.hive.serde2.JsonSerDe', 'kafka.topic'='cmsw', 'kafka.write.semantic'='AT_LEAST_ONCE', 'transient_lastDdlTime'='1565258896') 说明： 可以看到此处多了几个kafka记录元数据，并且每个元数据都可以用来作为条件过滤 kafka记录的值使用json、avro或者orc进行解码，解析后值的类型必须和hive指定的类型相匹配，不然会报错，通常情况下我们只需要映射需要的字段即可 此处 kafka.write.semantic 指定写入数据到kafka的语义，默认为最少一次，如果kafka版本支持事务，那么此处可以设置为 &quot;kafka.write.semantic&quot;=&quot;EXACTLY_ONCE&quot;精确一次 写入到kafka时，元数据列也必须设置，__key 设置出array以外的值，可以为null，__partition指定partition或者不设置，__offset固定为-1，__timestamp null或者-1（系统自动生产时间）或者有意义的时间（生产者可以指定的时间） 示例 查看最近十分钟数据记录 12SELECT COUNT(*) FROM kafka_table WHERE `__timestamp` &gt; 1000 * to_unix_timestamp(CURRENT_TIMESTAMP - interval '10' MINUTES); 与维度表进行连接 1234567//创建最近10分钟视图CREATE VIEW last_10_minutes_of_kafka_table AS SELECT `stcd`, `message_type`, `tm` FROM kafka_table WHERE `__timestamp` &gt; 1000 * to_unix_timestamp(CURRENT_TIMESTAMP - interval '10' MINUTES) ; CREATE TABLE stcd_table (`stcd` string, `name` string ,addr string) select stcd, name, sum(message_type) from last_10_minutes_of_kafka_table lk join stcd_table st on lk.stcd = st.stcd group by stcd, name order by sum(message_type) hive实现kafka ETL 1234567891011121314151617181920//创建offset记录表CREATE TABLE kafka_table_offsets(partition_id int, max_offset bigint, insert_time timestamp); //初始化offset记录表INSERT OVERWRITE TABLE kafka_table_offsets SELECT `__partition`, min(`__offset`) - 1, CURRENT_TIMESTAMP FROM kafka_table GROUP BY `__partition`, CURRENT_TIMESTAMP;//创建ETL表CREATE TABLE orc_kafka_table (partition_id int, koffset bigint, ktimestamp bigint,`message_type` string, `stcd` string, `tm` string)//单事务中进行ETLFROM kafka_table kt JOIN kafka_table_offsets otON (kt.`__partition` = ot.partition_id AND kt.`__offset` &gt; ot.max_offset )INSERT INTO TABLE orc_kafka_table SELECT `__partition`, `__offset`, `__timestamp`, message_type, stcd, tmINSERT OVERWRITE TABLE kafka_table_offsets SELECT `__partition`, max(`__offset`), CURRENT_TIMESTAMP GROUP BY `__partition`, CURRENT_TIMESTAMP;//重复上面步骤可以实现ETL，注意如果partition添加的时候需要更新offset表 hive使用JdbcStorageHandler 创建表 123456789101112131415CREATE EXTERNAL TABLE user_jdbc( id int, name string, age int)STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'TBLPROPERTIES ( "hive.sql.database.type" = "MYSQL", "hive.sql.jdbc.driver" = "com.mysql.jdbc.Driver", "hive.sql.jdbc.url" = "jdbc:mysql://cmsw1.tepia.com/cmsw", "hive.sql.dbcp.username" = "cmsw", "hive.sql.dbcp.password" = "Cmsw@123", "hive.sql.table" = "user", "hive.sql.dbcp.maxActive" = "1"); Hive 与 ElasticSearch 的数据交互Hive 与 druid 的数据交互]]></content>
      <categories>
        <category>hadoop</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phoenix使用]]></title>
    <url>%2F2018%2F08%2F01%2Fhadoop-phoenix%2F</url>
    <content type="text"><![CDATA[phoenix 读写考虑 phoenix 不可变表，可以进行更新，但是是整体的一行进行替换，如果创建不可变表的索引那么在更新数据之后，会导致索引表中存在脏数据（以往索引的数据），这是因为索引是通过row key进行创建的，无法自动删除，所以对于不可变表最好就是不要进行更新。 phoenix 在每一行中添加一个默认的列，并且设置为 x，每次更新都会自动更新这一列 性能影响最大的因素一定是主键、salt 表(http://phoenix.apache.org/salted.html)以及schema设计 关注读性能，通常是创建多个global index, http://phoenix.apache.org/secondary_indexing.html 关注写性能，预定义分区或使用salt表，使用真实类型代替原始类型，使用local index 为了保证读性能，可以考虑使用覆盖索引， CREATE INDEX index ON table（...）INCLUDE(...) 可选设置 IMMUTABLE_ROWS=true 如果速度更加重要，设置 DISABLE_WAL， 注意可能会丢失数据 row timestamp 将hbase行的时间映射到行键上面，这样在scan的时候就可以使用这个时间了，这个时间可以自己设置，也可以让hbase服务端自动设置, 一旦这个时间写入到行键，那么行健肯定是不会变了。但是如果行进行了update，那么此时默认列 x 的时间戳会更新, 可见每次进行更新的时候，系统都会带上默认列进行更新。从上面的信息可以看出，row_timestamp 仅仅只是提供了一种可以不用设置时间的方式并显示它， 但是在实际使用过程中并没有任何用处，所以如果没有特别的理由，最好不要使用这个时间戳，说明 http://phoenix.apache.org/rowtimestamp.html 如果表的元数据改变不是特别平凡，设置 UPDATE_CACHE_FREQUENCY为15分钟左右 如果数据表不是稀疏表（超过50%列有值），设置 SINGLE_CELL_ARRAY_WITH_OFFSETS 数据编码模式（将整个列族的一行放入到一个hbase cell当中）, http://phoenix.apache.org/columnencoding.html 12345678CREATE IMMUTABLE TABLE T ( a_string varchar not null, col1 integer CONSTRAINT pk PRIMARY KEY (a_string) ) IMMUTABLE_STORAGE_SCHEME = SINGLE_CELL_ARRAY_WITH_OFFSETS, COLUMN_ENCODED_BYTES = 1; 设置块编码 CREATE TABLE … ( … ) DATA_BLOCK_ENCODING=‘FAST_DIFF’ ， FAST_DIFF 和 SNAPPY 都是不错的选择 表的数据如果存在分离，创建多个列族 对于结构体对象，不要使用json（压缩性能不好），可以使用protobuf、avro、bson等等 尽量不要关闭列mapping 表太大使用 ASYNC 创建index， if not exists event_object_id_idx_b on trans.event (object_id) ASYNC UPDATE_CACHE_FREQUENCY=60000 大范围查询，可以使用 GZIP 压缩，但是需要注意写性能，以及在scan的时候，设置 Scan.setCacheBlocks(false) 对于大的查询或者连接查询，推荐使用 Hints 来优化性能 对于大客户端，不要使用 executeBatch ，使用多条进行update and commit，对于瘦客户端，推荐使用 executeBatch，可以减少与query server的RPC调用 1234567891011121314try (Connection conn = DriverManager.getConnection(url)) &#123; conn.setAutoCommit(false); int batchSize = 0; int commitSize = 1000; // number of rows you want to commit per batch. try (Statement stmt = conn.prepareStatement(upsert)) &#123; stmt.set ... while (there are records to upsert) &#123; stmt.executeUpdate(); batchSize++; if (batchSize % commitSize == 0) &#123; conn.commit(); &#125; &#125; conn.commit(); // commit the last batch of records &#125; update ... select ... 插入大量数据时候，通常打开 autocommit 并且设置 phoenix.mutate.batchSize 删除大量数据时候，通常打开 autoCommit，这样region server直接删除，而不需要返回row key到客户端 对于应用来说，快速失败比让客户端等待要好得多，可以设置更改等待时间phoenix.query.timeoutMs，配置设置参考 http://phoenix.apache.org/tuning.html CsvBulkLoadTool 参数 1234567891011121314-a,--array-delimiter &lt;arg&gt; 数组元素分隔符，默认冒号-b,--binaryEncoding &lt;arg&gt; Specifies binary encoding-c,--import-columns &lt;arg&gt; 导入的列，逗号分割-d,--delimiter &lt;arg&gt; 字段分隔符，默认逗号-e,--escape &lt;arg&gt; 转移字符，默认反斜线-g,--ignore-errors Ignore input errors-h,--help Show this help and quit-i,--input &lt;arg&gt; 输入路径，逗号分割-it,--index-table &lt;arg&gt; 当导入索引时，指定索引表-o,--output &lt;arg&gt; 输出临时HFile路径-q,--quote &lt;arg&gt; 自定义短语分隔符，默认双引号-s,--schema &lt;arg&gt; Phoenix schema name (optional)-t,--table &lt;arg&gt; Phoenix table name (mandatory)-z,--zookeeper &lt;arg&gt; zookeeper连接信息 基本命令形式 1sudo -u hbase HADOOP_CLASSPATH=$(hbase mapredcp):/usr/hdp/current/hbase-client/conf hadoop jar /usr/hdp/current/phoenix-client/phoenix-5.0.0.3.1.0.0-78-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool 字段分隔符设置 1-d $‘\t’ 小写表名转义 1--table \&quot;\&quot;t\&quot;\&quot; 注意，还有一个工具可以完成表导入工具 psql.py, 并且它是使用单线程phoenix客户端的方式，每秒2-5万条数据 首先使用 psql.py create_table.sql 再使用 psql.py load_data.cvs JsonBulkLoadToolIndexToolUtilIndexScrutinyTool映射hbase表到phoenix]]></content>
      <categories>
        <category>hadoop</category>
        <category>phoenix</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 命令行工具]]></title>
    <url>%2F2018%2F07%2F19%2Fkafka-shell-command%2F</url>
    <content type="text"><![CDATA[kafka使用kerberos连接zookeeper一定要带上zookeeper上kafka的节点，如： 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --list --zookeeper cmsw1.tepia.com:2181/kafka kafka-console-producer.sh用来往topic中生产消息1bin/kafka-console-producer.sh --broker-list hadoop3.feng.com:9092 --property &quot;parse.key=true&quot; --property &quot;key.separator=:&quot; --topic kafka 说明 –broker-list –topic 指定broker列表和生产消息的topic –property 指定console读取消息的kafka.tools.ConsoleProducer$LineMessageReader的属性， 也可以–line-reader指定默认的行读取器， 可以指定三个参数： parse.key true 表示是否解析key， 如果不解析默认key为null key.separator 指定key分隔符， 默认为 \t ignore.error true 指定是否忽略错误 如果不解析key那么， 生产的消息会均匀的发送到所有的Partition当中 –producer.config –producer-property 通过配置文件或者直接指定key=value去指定生产者的属性，用于在设置命令行中没有提供的参数 –key-serializer –value-serializer –compression-codec 指定key value的序列化和压缩方式 kafka-console-consumer.sh用来消费topic中的消息1bin/kafka-console-consumer.sh --bootstrap-server hadoop2.feng.com:9092 --topic kafka --property print.key=true --property &quot;key.separator=:&quot; --from-beginning 说明 –broker-list –topic 指定broker列表和消费的topic –property 指定console读取消息的kafka.tools.DefaultMessageFormatter的属性， 也可以–formatter指定默认的行读取器， 可以指定三个参数： print.key true 打印key print.value true 打印value key.separator 指定key分隔符 line.separator 指定行分隔符 –from-beginning 指定从日志头开始消费 –group 指定消费者组， 如果不指定那么默认会自动生产一个唯一的消费者组， 如果想要多个不同的consumer消费某个topic那么此处必须指定 –Partition 指定消费的topic的分组， 如果不指定默认消费全部的分组 –offset 指定消费者的位置非负整数或者earliest和latest， 默认为latest –consumer.config –consumer-property 通过配置文件或者直接指定key=value去指定消费者的属性，用于在设置命令行中没有提供的参数 –isolation-level read_uncommitted/read_committed 指定隔离级别， 用于支持读取消息的一致性语义 kafka-consumer-groups.sh用来管理消费者组1bin/kafka-consumer-groups.sh --bootstrap-server hadoop1.feng.com:9092 --group console-consumer-38042 --describe 说明 此命令主要存在四个功能–list, –describe, –delete, –reset-offsets –list 显示所有的消费者组 –describe 显示某个消费者组的详细信息 –delete 删除某个消费者组 –reset-offsets 重新设置消费者组的偏移量 kafka-log-dirs.sh用来查看broker的日志保存信息1bin/kafka-log-dirs.sh --bootstrap-server hadoop1.feng.com:9092 --describe --topic-list kafka --broker-list 2 说明 –topic-list 需要显示的topic –broker-list 需要显示的brokers kafka-delete-records.sh用来删除记录123456789bin/kafka-delete-records.sh --bootstrap-server hadoop1.feng.com:9092 --offset-json-file myconfig/kafka-delete-records.json&#123; &quot;partitions&quot;: [ &#123;&quot;topic&quot;: &quot;kafka&quot;, &quot;partition&quot;: 0,&quot;offset&quot;: 20&#125; ], &quot;version&quot;:1&#125; 说明 对于kafka来说默认存在两种删除策略， 一种基于日志量， 二种基于日志保存的时间， 但是有时候我们需要手动的去指定删除到具体的位置， 这个脚本就比较有用了 日志可能未必立即删除， 但是以及无法消费删除的日志了 kafka-preferred-replica-election.sh用来对某个topic的分区进行选举prefered replica， 分区的leader用来读写的， 从而平衡kafka集群1bin/kafka-preferred-replica-election.sh --zookeeper hadoop1.feng.com:2181 --path-to-json-file myconfig/kafka-preferred-replica-election.json 说明 此处的选举主要是为了kafka能够平衡负载， 并且kafka也保留了auto.leader.rebalance.enable来自动进行分区leader选举， 造成不平衡的原理是因为broker添加或者是broker失败恢复 kafka-reassign-partitions.sh生产分配方案12345678910111213141516bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --generate --topics-to-move-json-file myconfig/kafka-reassign-partitions-generate.json --broker-list 0,1,2Current partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125;Proposed partition reassignment configuration&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125;kafka-reassign-partitions-generate.json&#123; &quot;topics&quot;: [ &#123;&quot;topic&quot;: &quot;kafka&quot;&#125; ], &quot;version&quot;:1&#125; 说明 –generate 生成重新分区方案， –topic-to-move-json-file指定需要移动的topic信息， –broker-list 指定移动到的broker 此处打印出当前的分配信息， 以及建议的分配信息， 可以讲建议信息作为–reassignment-json-file的参数进行手动的执行 这里不需要指定bootstrap-server是因为当重新进行分配分区的时候实际上仅仅只是在zookeeper中更改状态， 当controller获取到状态的时候就异步的去执行当前分配策略 执行分配123bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --execute --reassignment-json-file myconfig/kafka-reassign-partitions-execute.json&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125; 在Partition移动的过程中会有大量的io操作， 所以对于集群中网络带宽这种稀有资源应该加以限制， 使用throttle进行设定 检查分配1bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --verify --reassignment-json-file myconfig/kafka-reassign-partitions-execute.json 检查移动分区的执行情况 移除当前移动分区时设置的网络配额]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell单步调试]]></title>
    <url>%2F2018%2F07%2F15%2Flinux-shell-debug%2F</url>
    <content type="text"><![CDATA[bash -x教程 PS4是在调试的时候脚本打印出的信息前缀 1export PS4=&apos;($&#123;BASH_SOURCE&#125;:$&#123;LINENO&#125;): $&#123;FUNCNAME[0]&#125; - [$&#123;SHLVL&#125;,$&#123;BASH_SUBSHELL&#125;, $?]&apos; bashdb教程安装1sudo apt install bashdb 命令详解 原始代码 bashdb.sh sub_bashdb.sh bashdb.sh 12345#!/bin/bashecho &quot;hello world&quot;name=&quot;feng xiang&quot;age=28. sub_bashdb.sh $name $age sub_bashdb.sh 12345#!/bin/bashecho &quot;welcome &quot;name=&quot;wang pei&quot;age=27echo &quot;welcome&quot; &gt;&gt; hello.txt 进入调试 1bashdb --debug bashdb.sh 断点 12345678910111213141516#断点, 指定行号或者当前行号break [LINENO]#瞬时断点, 指定行号或者当前行号， 断点一次后失效tbreak#条件断点, 断点编号， 断点条件condition BRKPT-NUM condition#删除一个或多个断点delete &#123;BRKPT-NUM&#125;...#清除某行所有断点clear LINENO#开启一个或多个断点enable BPNUM1 [BPNUM2 ...]#关闭一个或多个断点disable BPNUM1 [BPNUM2 ...]#显示断点info b[reakpoints] 12345678#继续执行到断点continue [LINENO | - ]#1. 执行到下一个断点continue#2. 执行到指定的行(首先给指定的行加零时断点，再执行continue命令)continue LINENO#关闭debug并执行脚本continue - 12345#前进1. 前进一步或者多步， 不进入方法next [COUNT]2. 前进一步或者多步， 进入方法或者source(.)命令执行的文件step [COUNT] 12#跳过一行或多行skip [COUNT] 1234567891011#执行完当前方法finish#从当前方法中返回return#向当前debug的进行发送信号kill [SIGNAL]signal SIGNAL#退出debuggerquit [EXIT-CODE [SHELL-LEVELS]]#重新运行或者指定重新运行的命令, 如果指定args必须为完整的命令， 不指定则使用原先的命令run [args] 查看变量 1234#打印出表达式值print EXPRESSIONprint $@ $name 123456#每次debug停止到一行的时候显示或者直接执行命令display [EXPRESSION]display $@ $name#删除display的命令undisplay NUM [NUM2 ...] 显示源代码, list显示的时候有个移动往下翻页的概念， 并记录当前浏览到了哪一个位置 12345678910list[&gt;] [LOC|.|-] [NUMBER]list . # 显示到当前栈的行list # 显示当前翻页的行前后list - # 显示当前翻页的行前面list&gt; . # 显示到当前栈的行list 10 3 # 显示10附近的三行 9-11list&gt; 10 3 # 显示是后面的三行 10-12list 10 13 # 显示10-13行list 10 -5 # 显示10行到倒数第五行 显示和设置当前debugger的设置信息 1show/set [param] 查看当前信息 123456789bashdb&lt;1&gt; infoInfo subcommands are: args display functions line signals stack watchpoints breakpoints files handle program source warranty #args 显示参数#display 显示display表达式#line 显示当前行号#breakpoints 显示所有断点... debug其它脚本， 并保留当前现场 12debug [SCRIPT]#不指定脚本， 执行当前行的命令 进入一个嵌套的shell， 不是子shell 12345678910shell [options]options: --no-fns | -F : don&apos;t copy in function definitions from parent shell --no-vars | -V : don&apos;t copy in variable definitions --shell SHELL_NAME --posix : corresponding shell option --login | l : corresponding shell option --noprofile : corresponding shell option --norc : corresponding shell option 执行一个命令 1eval CMD 设置环境变量 1export VAR1 [VAR2 ...] ##### 通过shell来]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 常用查询]]></title>
    <url>%2F2018%2F07%2F02%2Fmysql-use%2F</url>
    <content type="text"><![CDATA[区间分组统计 123456SELECT elt(INTERVAL(score, 0, 60, 80, 100), &apos;bad&apos;, &apos;good&apos;, &apos;perfect&apos;) as level, count(name) as count FROM classGROUP BY elt(INTERVAL(score, 0, 60, 80, 100), &apos;bad&apos;, &apos;good&apos;, &apos;perfect&apos;); 排序 123456SELECT elt(INTERVAL(score, 0, 60, 80, 100), &apos;1/bad&apos;, &apos;1/good&apos;, &apos;1/perfect&apos;) as level, count(name) as count FROM class GROUP BY elt(INTERVAL(score, 0, 60, 80, 100), &apos;1/bad&apos;, &apos;1/good&apos;, &apos;1/perfect&apos;); 按时间段分组统计 12345678SELECT DATE_FORMAT(trigger_time, &apos;%Y-%m-%d %h&apos;) trigger_time, COUNT(id) triggerCountFROM `job_qrtz_trigger_log`WHERE trigger_time BETWEEN &apos;2018-02-02 09:18:36&apos; AND &apos;2018-03-05 23:18:36&apos;GROUP BY DATE_FORMAT(trigger_time, &apos;%Y-%m-%d %h&apos;) trigger_time 分组统计的说明，可以看到上面都是通过区间进行分组统计， 那么分组统计首先是通过具体的字段进行分组， 分组到具体的桶后，剩下的就是一个对这个list操作了 基本对list操作的函数 12345678910111213141516171819AVGBIT_ANDBIT_ORBIT_XORCOUNTCOUNT DISTINCTGROUP_CONCAT 对整个List的字段进行连接JSON_ARRAYAGGJSON_OBJECTAGGMAXMINSTDSTDDEVSTDDEV_POPSTDDEV_SAMPSUMVARIANCEVAR_POPVAR_SAMP 将纵表变成横表 12345678select name, (case class when &apos;语文&apos; then score else 0 end) as 语文, (case class when &apos;数学&apos; then score else 0 end) as 数学, (case class when &apos;英语&apos; then score else 0 end) as 英语from class_scoregroup by name 将纵表变成横表思路二，对数据进行拼接再到程序中进行处理 123456select name, group_concat(class, &quot;,&quot;, score, &quot;;&quot;)from class_scoregroup by name]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F2018%2F04%2F28%2Ftools-git%2F</url>
    <content type="text"><![CDATA[参考 https://github.com/muwenzi/Program-Blog/labels/1.1%20Git 概念 git存在工作区， 暂存区， 本地仓库， 远程仓库 工作区: 当前项目root目录 暂存区: 新文件通过git add添加， 或者已经关联的文件都在暂存区当中 本地仓库： 本地的仓库， 通过commit将暂存区中的内容提交到本地仓库 远程仓库： 远程的仓库， 通过push将本地的commit的版本push到远程 对于已经新添加的文件或者已经关联的文件修改， 都必须通过git add将其添加到暂存区当中， 再通过commit提交到本地，再通过push到远程仓库当中 git checkout123git checkout file 放弃工作空间中改变的文件git checkout branch 切换到指定分支, 可以添加 -b参数添加一个新的分支git checkout -b newbranch branch 从branch分支创建一个新的分支， 并切换到新的分支 git add1git add . 将工作空间中的改变文件添加到暂存区 git reset1234git reset用户回退一个已经commit但是并没有push的代码， 常见的参数： --hard --merge --mixed --patch --softgit reset --mixed version_hash 会保留源码， 只是将git commit和index信息回退到某个版本 mixed是git reset的默认模式， git reset --mixed 等价于 git resetgit reset --soft 会保留源码， 只是回退到commit信息到某个版本，不涉及index的回退， 如果还需要提交直接提交就可以了git reset --hard 源代码也会回退到某个版本，commit和index都会回退到某个版本，如果没有指定版本那么就会放弃当前所有的更改回退当上一个commit 注意： 上面reset只是更改本地仓库的代码， 远程仓库代码还没有改变， 所以如果index发生变化， 而线上的index没有发生变化， 那么必定会导致不发进行push， 所以必须pull下来， 再进行push， 此时代码还是回变为线上index的版本， 所以总的来说这种方式不适合回退到线上index版本之前的， 它无法做到， 所以我们经常将其作为本地代码回滚的策略。 git revert123git revert 用于反正提交， 执行时必须要求工作目录必须是干净的git revert 用一个新提交来消除一个历史提交的所做的任何修改， 所以它的index不会回退， 而是继续向前， revert之后本地的代码也会回滚到指定的历史版本， 这个时候再push是不会发生任何冲突的git revert commit1 commit2 ... revert 只是回滚某个commit， 所以如果需要回滚多个commit需要指定多个commit git reset/revert总结git reset是直接删除制定的commit（不会影响远程机器上的代码， index直接被删除）， git revert是用一次新的commit来回滚之前的commit（会影响远程机器上的代码， index会继续前进） git fetch [远程仓库 [远程仓库分支]]123git fetch origin mastergit merge origin/master， 此时可能需要解决冲突等等， 合并之后我们就可以在当前分支上工作并提交到远程分支如果不希望合并当前分支， 但是希望在远处分支上工作， 可以checkout出来一个新的分支git checkout -b newremotebranch origin/master 可以看到checkout可以直接从一个分支上创建一个新的分支，并不需要工作在origin/master分支上 注意： 下载远程分支， 但并不会合并远程分支到当前分支， 也就是说当前分支可能比下载下来的远程分支滞后几个分支， 所以如果我们希望在远处分支上工作， 那么我们必须将远程分支合并到当前分支 it remote某些场合下， 一个git项目需要同时使用两个甚至更多的远程仓库， 比如国外+国内、测试环境+生成环境+审核环境等等， 这样我们就需要对不同的远程仓库分别提交等操作。git remote 参数： add remove set-branches set-url update prune rename set-head show添加仓库， git remote add remote_name remote_url…..123456案例： 当我们发现github上面存在一个项目我们无法修改， 但是我们希望完全保留其所有的版本信息。操作步骤如下：a. git clone出项目到本地b. 在github上面创建一个空的仓库， 地址如下： https://github.com/ilivoo/spring.gitc. 在本地项目下直接删除原来的仓库， git remote remove remote_named. 将自己的远程仓库添加到项目当中， git remote add origin https://github.com/ilivoo/spring.gite. 将本地的项目直接上传到自己的远程项目当中， git push origin master 注意： 此处不仅仅限制与将其导入到github中， 也可以导入到国内的码云当中， 而且推荐使用这种方式。 并且码云当中还有一个更好的办法， 来完成这个动作， 直接在码云当中创建一个仓库， 创建过程中直接克隆github上的项目， 这样就是一步到位， 而且速度非常快， 再直接从码云中clone到本地， 关机是码云中还可以将其设置为private另外： git存在fork的概念， 可以直接fork项目再自己对其进行修改， 类似上面的操作 git branch123git branch branchName 创建一个新的分支git checkout branchName 切换到新的分支git checkout -b branchName 创建并切换到一个新的分支 git checkoutcheckout出某个分支， 注意如果此时暂存区还有修改没有提交， 如果此时checkout的某个分支可以合并， 那么可以切换到对应的分支， 如果有冲突， 那么此时不允许切换到某个分支， 所以在我们切换分支的时候最好是将工作空间清空。 git merge合并两个分支， 此时被合并的分支必定是commit状态， 而当前分区可以是未commit状态， 但是如果此时合并时存在冲突， 那么此时是不能merge成功的， 所以此时必须要对当前分支commit再合并， 合并后可能存在冲突， 此时将冲突解决就可以了， 重新提交。 也可以在合并冲突之后使用命令git mergetool来调用mergetool来处理冲突问题， 所以可以设置当前mergetool, 如p4merge1234567891011121314C:\Users\ilivoo&gt;cat .gitconfig[user] name = ilivoo email = 316488140@qq.com[merge] tool = p4merge[mergetool &quot;p4merge&quot;] cmd = p4merge \&quot;$BASE\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$MERGED\&quot;[mergetool] trustExitCode = false[diff] tool = p4merge[difftool &quot;p4merge&quot;] cmd = p4merge \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; 注意： 合并是先执行git merge， 合并可能没有成功存在冲突， 此时再调用git mergetool自动调用合并工具来手动合并， 而git difftool则是直接使用就可以, 参数可以添加任意两个版本如果不想合并， 那么可以直接通过 git merge –abort取消当前合并 也可以使用meld 123456789101112131415161718192021[user] name = ilivoo email = 316488140@qq.com[credential][commit] gpgsign = true[diff] tool = meld[difftool] prompt = false[difftool &quot;meld&quot;] path = /usr/bin/meld[merge] tool = meld conflictstyle = diff3[mergetool &quot;meld&quot;] path = /usr/bin/meld cmd = meld \&quot;$LOCAL\&quot; \&quot;$MERGED\&quot; \&quot;$REMOTE\&quot; --output \&quot;$MERGED\&quot; #cmd = meld \&quot;$LOCAL\&quot; \&quot;$BASE\&quot; \&quot;$REMOTE\&quot; --output \&quot;$MERGED\&quot;[mergetool] prompt = false git clean12345678910# 删除 untracked filesgit clean -f# 连 untracked 的目录也一起删掉git clean -fd# 连 .gitignore文件中untrack 文件/目录也一起删掉 （如：idea生成的.idea，日志logs等等）git clean -xfd# 在用上述 git clean 前，墙裂建议加上 -n 参数来先看看会删掉哪些文件，防止重要文件被误删git clean -nxfd# 注意： 真正的删除是参数f d x， n只是用来查看当前参数可能会删除的那些文件， 如：git clean -nf查看那些未跟踪的文件会被删除， 但是并不会真正执行删除， git clean -f 才是真正删除未跟踪的文件 git 设置代理12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; git 设置无密码提交 使用保存密码方式 git config –global credential.helper store .gitconfig文件中会多出 [credential]​ helper = store 当输入过一次密码之后会在.gitconfig文件的相同目录下生成.git-credentials https://ilivoo:password@github.com 使用秘钥方式, 参考 https://segmentfault.com/a/1190000002645623 添加秘钥到github当中 修改.git/config配置文件 123[remote &quot;origin&quot;]url = git@github.com:ilivoo/project.gitfetch = +refs/heads/*:refs/remotes/origin/* git 配置 commit 签名 参考 https://coderwall.com/p/d3uo3w/git-gpg-know-thy-commits 步骤： 使用gpg生成密钥和公钥 gpg --full-generate-key, 名字和邮箱最好是github的名字和邮箱 导出公钥并上传到github中 git常见用例 clone到一个非空文件夹 1git clone your-repo tmp &amp;&amp; mv tmp/.git . &amp;&amp; rm -rf tmp &amp;&amp; git reset --hard]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fzf安装与使用]]></title>
    <url>%2F2018%2F04%2F28%2Ftools-fzf%2F</url>
    <content type="text"><![CDATA[安装 参考github https://github.com/junegunn/fzf 12git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf~/.fzf/install 配置vim插件 1Plug &apos;junegunn/fzf&apos;, &#123; &apos;dir&apos;: &apos;~/.fzf&apos;, &apos;do&apos;: &apos;./install --all&apos; &#125; 配置oh-my-zsh插件 1fzf 基本使用 快捷键 12345678CTRL-T - 粘贴选择的文件或者文件夹到命令行Set FZF_CTRL_T_COMMAND to override the default commandSet FZF_CTRL_T_OPTS to pass additional optionsCTRL-R - 粘贴选择历史命令到命令行Set FZF_CTRL_R_OPTS to pass additional optionsALT-C - cd到选择的文件夹Set FZF_ALT_C_COMMAND to override the default commandSet FZF_ALT_C_OPTS to pass additional options zsh中配置快捷键命令 12export FZF_DEFAULT_OPTS=&apos;--height 60% --reverse --border&apos;export FZF_CTRL_T_OPTS=&quot;--preview &apos;(highlight -O ansi -l &#123;&#125; 2&gt; /dev/null || cat &#123;&#125; || tree -C &#123;&#125;) 2&gt; /dev/null | head -200&apos;&quot; fzf终端快捷键 123CTRL-J / CTRL-K (or CTRL-N / CTRL-P) 条目的选择Enter key to select the item, CTRL-C / CTRL-G / ESC to 退出On multi-select mode (-m), TAB and Shift-TAB to 标记多条目 fzf终端中进行过滤条件 1234567sbtrkt 模糊匹配包含字符^music 以music开头mp3$ 以mp3结尾&apos;wild 精确匹配包含wild!fire 不保护fire!^music 不以music开头!mp3$ 不以mp3结尾 使用 ** 进入fzf模式，并使用Tab键进入命令行选择 12vim .oh-my-zsh/** vim查看 .oh-my-zsh 中的文件，但是忘记文件名字kill -9 直接进入命令行 命令说明 fzf是一个模糊搜索工具，它可以接受任何输入，包括ag的数据，所以我们在使用的时候一定要灵活适应 fzf的输出可以使用 xargs 进行接收并做操作]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 日志]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-log%2F</url>
    <content type="text"><![CDATA[mysql 日志 主要包括： 错误日志、查询日志、慢查询日志、事务日志、二进制日志、回放日志 错误日志： 错误日志是用来记录mysql系统在运行的过程中所发生的错误， 如启动过程中存在的问题， 系统在配置主从是报的错误等待 错误日志默认是开启的，并且错误日志无法被禁止， 默认情况下错误日日志存储在mysql数据库的数据文件目录当中， 以主机名作为名字， 并且以.err结尾， 一般情况下 建议更改日志的名字，这样在统一维护的时候会更加方便 配置方式: log_error=error.log 查询日志 查询日志是用来记录用户的所有操作， 其中包括增删改查等所有信息 查询日志默认是关闭的， 一般如果不是为了调试数据库都不会开启查询日志， 会产生大量的磁盘io 配置方式： general_log=log.log 慢查询日志 慢查询日志是用来记录执行时间超过指定的时间的查询语句， 通过慢查询可以找出运行效率低下需要优化的语句 慢查询默认关闭的， 一般它对服务器的性能微乎其微， 所以建议线上根据需要开启 配置方式 123slow_query_log=onslow_query_log_file=slow.loglong_query_time=0.000001 手动开启： 12set global slow_query_log=on;set global long_query_time=0.000001; 可以看到虽然这里设置了， 但是严格它对于当前的session是无法生效的， 如果仅仅只是想对当前session生效， 可以设置， 12set global slow_query_log=on;set long_query_time=0.000001; 如果想对所有的连接有效， 包括已经启动的连接和后面加入的连接有效， 可以设置， 12set global slow_query_log=on;set global long_query_time=0.000001; 查看当前session的配置情况 1select * from performance_schema.variables_by_thread as a, (select THREAD_ID,PROCESSLIST_ID,PROCESSLIST_USER,PROCESSLIST_HOST,PROCESSLIST_COMMAND,PROCESSLIST_STATE from performance_schema.threads where PROCESSLIST_USER&lt;&gt;&apos;NULL&apos;) as b where a.THREAD_ID = b.THREAD_ID and a.VARIABLE_NAME = &apos;long_query_time&apos;; 可以查看所有的连接是否生效， 再通过kill PROCESSLIST_ID来杀手不生效的连接（一般都是使用连接池来连接的所以问题不大）， 也可以通过笨的方法， 对每一个现有的连接执行一次 注意： 通过这里设置全局变量我们可以看到， mysql设置全局变量未必会对现有的连接生效， 所以我们如果想要进行验证， 最简单的办法就是通过上面的语句进行查询。 设置线上慢查询日志时最好指定毫秒数， 这样可以用于长期线上开启 事务日志 事务日志（InnoDB特有的日志）可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把改修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。事务日志采用追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢的刷回到磁盘。目前大多数的存储引擎都是这样实现的，我们通常称之为预写式日志，修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。 1234innodb_flush_log_at_timeout=1 事务日志如果没有每一次从缓冲区中刷新到磁盘， 那么每隔一秒刷新一次innodb_flush_log_at_trx_commit=1 为1表示事务一提交就刷新到磁盘中， 为2表示只有在事务提交时才同步， 有可能丢失整个事务innodb_log_files_in_group=2 至少两个日志组innodb_log_group_home_dir=./ 日志组的位置 二进制日志 二进制日志是MySQL服务器用来记录数据修改事件的，比如INSERT、UPDATE、DELETE等会导致数据发生变化的语句，SELECT语句不会被记录在内。MySQL必须先执行完一条语句才能知道它是否修改了数据，因此写入二进制日志文件的时间是语句执行完成的时间。写入顺序是按语句执行完成的先后顺序，事务中的语句会先被缓存起来，成功提交后才会被写入，回滚则不会被写入。非事务的存储引擎，所有的修改会立刻写入到二进制日志中。 二进制日志顾名思义不是文本而是一种更有效率的二进制格式，它比文本占用更少的空间，但是可读性就很差了，必须使用mysqlbinlog工具才能转换为可读的文本。二进制日志主要用于数据库备份和故障时恢复数据，配置MySQL主从复制必须启用二进制日志。 二进制日志索引文件中会列出所有二进制日志文件，此文件是文本的因此可以直接查看，文件的最后一行就是当前正在使用的二进制日志文件。 配置 12345678910111213141516log_bin=onlog_bin_basename=bin.log //指定bin log的名字， 后面会自动生成xxxxx的数字log_bin_index=bin.log.index //所有bin log文件名的索引// 通过flush logs会导致当前bin log写完， 并刷新出一个新的log// 并且在 index中记录这个文件的名字binlog_do_db //表示只记录指定数据库的二进制日志binlog_ignore_db //表示不记录指定的数据库的二进制日志//不建议使用 do/ignore来进行控制, 安全的替换方案是 在 slave上配置过滤,//使用基于查询中真正涉及到的表的选项, 例如replicate-wild-* 选项replicate-wild-do-table=ljzxdb.%sync_binlog //直接影响mysql的性能和完整性sync_binlog=0：当事务提交后，Mysql仅仅是将binlog_cache中的数据写入Binlog文件，但不执行fsync之类的磁盘 同步指令通知文件系统将缓存刷新到磁盘，而让Filesystem自行决定什么时候来做同步，性能最好。sync_binlog=n，在进行n次事务提交以后，Mysql将执行一次fsync之类的磁盘同步指令，同志文件系统将Binlog文件缓存刷新到磁盘。binglog_format //值有statement（记录逻辑sql语句）、row（记录表的行更动情况）、mixed混合使两种，// row的存在是因为存在很多不确定性的函数， 如果直接使用， 会导致数据不一致， 如 current_time(), 所以直接将值记录下来就更正确 回放日志 bin log从master发送到slave的时候， slave就变成了reply log， 用于回放master中数据发生的变化]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql索引导出]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-index%2F</url>
    <content type="text"><![CDATA[Mysql Innodb索引类型（索引使用b+树，非叶子节点不存放数据，叶子节点才存放数据） 聚簇索引：索引和数据存放在一起 非主键索引（二级索引）：二级索引存放主键索引的的值，当在二级索引中找到主键的值再回到主键索引中查找数据 覆盖索引：索引包含所有需要查询的数据 左前缀索引：查询条件根据左前缀原则查找数据 主键联合索引：二级索引包含主键联合索引的两个值，排序顺序也是包含的 索引下推：存在索引(a, b)，查询a like ‘hello%’ and b = 10，此时这个索引会使用，并且不会返回到主表中判断b是否等于10 ESR原则（Equal、Sort、Range）：索引建立需要考虑到排序，如果条件是 a = ‘’ and c &gt; 100 order by b，索引建立（a,b,c），排序即时分组，如果是分组，也需要考虑分组情况 MyISAM的主键索引和二级索引都是存放列值和行号（物理地址），并且使用B+树 打印删除所有的索引的语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印删除所有的索引的语句，不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; and UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印出所有的所有创建语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印出所有的创建语句， 不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印自增的所有创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)),&apos;) UNSIGNED NOT NULL AUTO_INCREMENT;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印删除自增的创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)), &apos;) UNSIGNED NOT NULL;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印没有主键的表 1SELECT DISTINCT t.table_schema, t.table_name FROM information_schema.tables AS t LEFT JOIN information_schema.columns AS c ON t.table_schema = c.table_schema AND t.table_name = c.table_name AND c.column_key = &quot;PRI&quot; WHERE t.table_schema NOT IN (&apos;information_schema&apos;, &apos;mysql&apos;, &apos;performance_schema&apos;) AND c.table_name IS NULL AND t.table_type != &apos;VIEW&apos;;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks代理和全局代理]]></title>
    <url>%2F2018%2F04%2F28%2Flinux-shadowsocket%2F</url>
    <content type="text"><![CDATA[proxychain 安装proxychain， 也可以直接使用apt安装 123456git clone https://github.com/rofl0r/proxychains-ng.gitcd proxychains-ng./configuremake &amp;&amp; make installcp ./src/proxychains.conf /etc/proxychains.confcd .. &amp;&amp; rm -rf proxychains-ng 配置proxychain 123sudo vi /etc/proxychains.conf# 将socks4 127.0.0.1 9095改为socks5 127.0.0.1 1080 //1080改为你自己的端口 使用proxychain 1proxychains chrome #在需要进行代理的程序前加proxychains 确保本地的代理已经打开 如果proxychains不能启动某个程序，可以使用下面方式使得程序使用sockets代理 1chromium-browser --proxy-server=&quot;socks5://127.0.0.1:1080&quot; shadowsocks 客户端 安装混淆工具 https://github.com/shadowsocks/simple-obfs 123456789# Debian / Ubuntusudo apt-get install --no-install-recommends build-essential autoconf libtool libssl-dev libpcre3-dev libev-dev asciidoc xmlto automakegit clone https://github.com/shadowsocks/simple-obfs.gitcd simple-obfsgit submodule update --init --recursive./autogen.sh./configure &amp;&amp; makesudo make install 启动obfs 1obfs-local -s 服务端ip -p 服务端端口号 -l 9996 --obfs http --obfs-host www.ilivoo.com 可选kcptun, 安装方式服务端安装时候会给出 123456789101112131415161718192021sudo ./client_linux_amd64 -c config.json&#123; &quot;localaddr&quot;: &quot;:11443&quot;, #此处端口可以更改 &quot;remoteaddr&quot;: &quot;kcptun服务器地址:kcp服务器端口&quot;, &quot;key&quot;: &quot;kcptun密码&quot;, &quot;crypt&quot;: &quot;none&quot;, &quot;mode&quot;: &quot;fast2&quot;, &quot;mtu&quot;: 1350, &quot;sndwnd&quot;: 1024, &quot;rcvwnd&quot;: 1024, &quot;datashard&quot;: 10, &quot;parityshard&quot;: 3, &quot;dscp&quot;: 0, &quot;nocomp&quot;: false, &quot;quiet&quot;: false, &quot;tcp&quot;: false, &quot;acknodelay&quot;: false, &quot;sockbuf&quot;: 4194304, &quot;smuxbuf&quot;: 4194304, &quot;keepalive&quot;: 10&#125; 安装最新shadowsocks, apt直接安装可能版本太低无法支持更多的加密算法 12sudo apt-get install python-pipsudo pip install https://github.com/shadowsocks/shadowsocks/archive/master.zip -U 配置shadowsocks 123456789101112sudo vi /etc/shadowsocks.json&#123; &quot;server&quot;:&quot;127.0.0.1&quot;, &quot;server_port&quot;:9996, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;密码&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;chacha20-ietf-poly1305&quot;, &quot;fast_open&quot;: false, #需要服务端开启 &quot;workers&quot;: 2&#125; 启动shadowsocks， 可以直接配置成开机启动启动 1sudo sslocal -c /etc/shadowsocks.json SwitchyOmega 安装SwitchyOmega， chrome如果没有代理可能无法访问商店， 可以开启shadowsocks并使用proxychain代理chrome再安装，或者直接百度搜索SwitchyOmega离线安装。 配置SwitchyOmega 1https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList shadowsocks服务端搭建 建议使用搬瓦工， 并选择手机运营商对应的直连机房 安装步骤： 安装Centos 7 x86_64 bbr 使用谷歌网络优化技术 安装shadowsocks1234yum install wgetwget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.shchmod +x shadowsocks-all.sh./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log **安装选择：** 1234561. 安装语言版本 Shadowsocks-libev2. 密码3. 端口 4434. 选择加密算法 chacha20-ietf-poly13054. 开启obfs混淆 yes5. 选择混淆协议 http 开启kcptun加速(可选) 脚本地址 https://github.com/kuoruan/shell-scripts kcptun地址 https://github.com/xtaci/kcptun 12wget --no-check-certificate -O kcptun.sh https://github.com/kuoruan/shell-scripts/raw/master/kcptun/kcptun.shsh kcptun.sh 安装选择: 123451. 选择kcptun服务端绑定的端口, 需要对外暴露2. 选择kcptun转发的IP地址, 此处默认127.0.0.1就可以, 因为kcptun还可以转发到其它服务器3. 选择kcptun转发的端口号, 此处设置上面obfs的端口号4. 剩下的就是密码等设置, 自行设置就好, 建议mode设置为fast2, 加密为none5. 设置好之后会打印出客户端下载地址和客户端配置, 下载客户端并配置设置客户端配置文件, 客户端的localaddr可以自行修改, 后面客户端的obfs会指向这个端口 优化,参照https://github.com/shadowsocks/shadowsocks/wiki/TCP-Fast-Open： 服务端优化tcp_fastopen 12345678910111213&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:443, &quot;password&quot;:&quot;密码&quot;, &quot;timeout&quot;:300, &quot;user&quot;:&quot;nobody&quot;, &quot;method&quot;:&quot;chacha20-ietf-poly1305&quot;, &quot;fast_open&quot;:false, &quot;nameserver&quot;:&quot;8.8.8.8&quot;, &quot;mode&quot;:&quot;tcp_and_udp&quot;, &quot;plugin&quot;:&quot;obfs-server&quot;, &quot;plugin_opts&quot;:&quot;obfs=http&quot;&#125; 服务端系统开启bbr 和 tcp_fastopen, 搬瓦工选择bbr镜像默认已经开启 12#创建文件vi /etc/sysctl.d/local.conf 12345678910111213# 配置如下net.core.default_qdisc=fqnet.ipv4.tcp_congestion_control=bbrnet.ipv4.neigh.default.base_reachable_time_ms = 600000net.ipv4.neigh.default.mcast_solicit = 20net.ipv4.neigh.default.retrans_time_ms = 250net.ipv4.conf.all.rp_filter=0net.ipv4.conf.eth0.rp_filter=0net.ipv4.conf.eth1.rp_filter=0net.core.default_qdisc=fqnet.ipv4.tcp_congestion_control=bbrnet.ipv4.tcp_fastopen=3 12#启动 sysctl --system 客户端系统开启tcp_fastopen 1234# 当前生效echo 3 &gt; /proc/sys/net/ipv4/tcp_fastopen# 永久生效,在/etc/sysctl.conf中添加net.ipv4.tcp_fastopen = 3 全局代理设置 先需要安装genpac工具（基于gfwlist的代理自动配置(Proxy Auto-config)文件生成工具，支持自定义规则， chrome的代理也是使用这种。 123sudo pip install genpacmkdir -p ~/opt/autoproxy &amp;&amp; cd ~/opt/autoproxygenpac --pac-proxy &quot;SOCKS5 127.0.0.1:1080&quot; --gfwlist-proxy=&quot;SOCKS5 127.0.0.1:1080&quot; --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output=&quot;autoproxy.pac&quot; 配置系统代理，成功之后会在该目录下生成一个autoproxy.pac文件。然后打开System Settings-&gt;Hardware-&gt;Network-&gt;Network proxy，将Method设置为Automatic，然后Configuration URL中填写autoproxy.pac文件路径 1file:///home/feng/opt/autoproxy/autoproxy.pac 注意： 安装完之后此时使用浏览器就可以进行使用全局代理了 启动http、https、ftp代理 设置命令行下所有的其它程序使用socks代理，安装一个privoxy代理工具，实现终端内socks5转换为http/https，进而将http/https请求转发给ss，实现终端内的代理 1sudo apt-get install privoxy 配置/etc/privoxy/config文件 123#保证存在下面两行存在, forward-socks5t 如果不行就改成 forward-socks5 listen-address localhost:8118forward-socks5t / 127.0.0.1:1080 . 注意：此处可以将设置 listen-address 0.0.0.0:8118，这样局域网内也可以进行代理 注意：也可以使用另外一种方式配置，如: https://guojing.io/posts/privoxy/ 配置/etc/profile 1234# 在末尾添加以下三行：export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export ftp_proxy=http://127.0.0.1:8118 #ftp的代理可以根据需要添加 注意： source /etc/profile 启动privoxy 1sudo privoxy --user privoxy /etc/privoxy/config 注意：之后每次修改了配置文件后，都要执行sudo service privoxy restart来重启服务 ubuntu18.04 关闭systemd-resolved.service服务 关闭服务 12sudo systemctl disable systemd-resolvedsudo systemctl stop systemd-resolved 修改添加配置文件 /etc/NetworkManager/NetworkManager.conf 中 main 选择 1dns=default 删除 rm /etc/resolv.conf 使其重新生成 重启网络服务 1sudo systemctl restart NetworkManager ubuntu 上dns是一个比较复杂的问题， 还是推荐使用系统默认的 systemd-resolved 来进行dns解析，因为它有缓存功能。如果发现网络的DNS不能解析， 直接在网络哪里更改DNS即可， 并重启网络。 1systemd-resolve --status 查看当前DNS情况]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 正则表达式]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-re%2F</url>
    <content type="text"><![CDATA[定义 正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 正则表达式的强大之处在于引入特殊字符来定义字符集、字符集位置、匹配子组、重复模式和扩展表示法。 字符集定义: 符号 描述 正则表达式 匹配案例 literal 匹配字符串的字面值 foo foo re1&#124;re2 匹配正则表达式re1或者re2 foo&#124;bar foo或者bar . 匹配任何字符(除了\n之外) b.b bab […] 匹配来自字符集的任意单个字符 [aeiou] a [^…] 匹配非来自字符集的任何单个字符 [^aeiou] 0 [x-ym-n] 匹配任意来自x~y和m~n范围内的单个字符 [A-Za-z] b \d 匹配任何十进制数字，与[0-9]相同(与\D相反) data\d.txt data1.txt \w 匹配字母数字数字字符,与[A-Za-z0-9]相同(与\w相反) data\w.txt dataa.txt \s 匹配任何空格字符, 与[\n\t\r\v\f]相同(与\S相反) of\sthe of the \x 对特殊字符进行转移 \. . 字符集位置： 符号 描述 正则表达式 匹配 ^ 匹配字符串起始位置 ^Dear Dear $ 匹配字符串终止位置 /bin/*sh$ /bin/bash \A(\Z) 匹配字符串起始(结束)位置 \ADear Dear \b 匹配任何单词边界(与\S相反) \bthe\b the 重复模式： 符号 描述 正则表达式 匹配 * 匹配０次或者多次 [A-Za-z0-9]* abcde + 匹配１次或者多次 [a-z]+.com baidu.com ? 匹配０次或者多次 goo? go {N} 精确匹配Ｎ次 [0-9]3 333 {M,N} 匹配M~N次 [0-9]{5,9} 123456 (*&#124;+&#124;?&#124;{}) 匹配上面重复出现的非贪婪版本 .*?[a-z] abcc 匹配子组： 符号 描述 正则表达式 匹配 (…) 匹配封闭的正则表达式， 然后另存为子组 ([0-9]{3}-)?[0-9]{7-8} 020-888888 \N 配皮上面已经保存的分组 ([0-9]{3})-(\d{7-8}) post is \1 020-12345678 post is 020 扩展表示法： 符号 描述 正则表达式 匹配 贪婪模式正则表达式的默认为贪婪模式，表示竟可能多的匹配， 非贪婪是如果后面正则表达式能够匹配则尽可能少的匹配， 给后面表达式匹配的机会， 如：1234&gt;&gt;&gt; re.match(r&apos;^(\d+)(0*)$&apos;, &apos;102300&apos;).groups() #贪婪(&apos;102300&apos;, &apos;&apos;)&gt;&gt;&gt; re.match(r&apos;^(\d+?)(0*)$&apos;, &apos;102300&apos;).groups() #非贪婪(&apos;1023&apos;, &apos;00&apos;) re 模块123456789101112131415161718192021基础方法compile(pattern, flags = 0) 用任何可选的标记来编译正则表达式，然后返回正则表达式对象match(pattern, string, flags = 0) 尝试使用带可选标记的正则表达式的模式来匹配字符串，如果匹配成功返回匹配对象，否则返回Nonesearch(patter, string, flags = 0) 使用可选的标记搜索字符串中第一次出现正则表达式模式，如果匹配成功返回匹配对象，否则返回None，search是逐个逐个字符的往下匹配findall/finditer(pattern, string[,flags]) 查找字符串中所有(非重复)出现的正则表达式模式，并返回匹配列表split(pattern, string, max = 0) 使用正则表达式分割模式分割字符串，并返回最大max次操作的列表sub(pattern, repl, string, count = 0) 使用repl替换正则表达式的模式在字符串中出现的位置，除非定义count，否则替换所有的位置purge() 清楚隐式编译的正则表达式模式标记re.I | re.IGNORECASE 不区分大小写匹配re.L | re.Locale 根据所使用的本地语言环境通过\w、\W、\b、\B、\s、\S实现匹配re.M | re.MULTILINE ^和$分别匹配目标字符串中行的起始和结束，而不是严格匹配整个字符串本身的起始和结尾re.S | re.DOTALL .号可以匹配所有的字符，包括\nre.X | re.VERBOSE 所有的空格加上#都会被忽略re.T | re.TEMPLATE ...匹配对象的方法group(num = 0) 返回整个匹配对象或者编号为num的特定子组groups(default = None) 返回包含所有匹配子组的元组groupdict(defalt = None) 返回包含所有匹配子组的字典，子组名称作为键]]></content>
      <categories>
        <category>python</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 帮助命令]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-help%2F</url>
    <content type="text"><![CDATA[获取内建信息123dir() 打印当前scopedir(__builtins__) 打印内建scopedir(object) 打印对象scope 获取帮助1help(object) 获取类继承层次1Class.mro() 此方法是type中的方法， 每个Class都是type的实例 获取所有的模块1help('modules') 判断对象类型123456789&gt;&gt;&gt; import types&gt;&gt;&gt; type('abc')==types.StringTypeTrue&gt;&gt;&gt; type(u'abc')==types.UnicodeTypeTrue&gt;&gt;&gt; type([])==types.ListTypeTrue&gt;&gt;&gt; type(str)==types.TypeTypeTrue 获取对象中的常量1234567891011121314151617181920212223242526272829303132import typesbase_type = (types.BooleanType, types.FloatType, types.IntType, types.LongType, types.NoneType,types.StringType, types.TupleType, types.ListType, types.DictType, types.ComplexType)def dir_ck(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value print sorted(_constant.keys())def dir_ok(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value print sorted(_object.keys())def dir_c(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value for key in sorted(_constant.keys()): print key + "\t = \t" + _constant.get(key)def dir_o(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value for key in sorted(_object.keys()): print key + "\t = \t" + str(_object.get(key)) 查找对象中的方法12def dir_f(obj, key): print filter(lambda x: x.lower().find(key) != -1, dir(obj))]]></content>
      <categories>
        <category>python</category>
        <category>帮助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase、phoenix 创建表]]></title>
    <url>%2F2018%2F04%2F16%2Fhadoop-hbase%2F</url>
    <content type="text"><![CDATA[创建日志表 创建test表 123create &apos;test&apos;, \&#123;NAME =&gt; &apos;f&apos;, COMPRESSION =&gt; &apos;SNAPPY&apos;&#125;, \&#123;SPLITS =&gt; [&apos;40&apos;,&apos;80&apos;,&apos;c0&apos;], SPLIT_POLICY =&gt; &apos;org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy&apos;, CONFIGURATION =&gt; &#123;&apos;KeyPrefixRegionSplitPolicy.prefix_length&apos; =&gt; &apos;2&apos;&#125;&#125; 创建testIndex 1create &apos;testIndex&apos;, &#123;NAME =&gt; &apos;f&apos;, COMPRESSION =&gt; &apos;SNAPPY&apos;&#125;, &#123;SPLITS =&gt; [&apos;40&apos;,&apos;80&apos;,&apos;c0&apos;]&#125; 创建最新一条数据表 1create &apos;testNewest&apos;, &#123;NAME =&gt; &apos;f&apos;, VERSIONS =&gt; 1, BLOCKCACHE =&gt; true, IN_MEMORY =&gt; true&#125; 注意: 此处可以设置 DURABILITY =&gt; &#39;SKIP_WAL&#39;, 不过最好是在程序中控制WAL是否写入. 创建phoenix表 1create table TEST(prefix char(2) not null, time bigint not null, id char(10) not null, message varchar, constraint pk primary key(prefix, time desc row_timestamp, id)) IMMUTABLE_ROWS=true SPLIT ON (&apos;40&apos;, &apos;80&apos;, &apos;c0&apos;) COMPRESSION=&apos;GZ&apos;; 说明： IMMUTABLE_ROWS 不可变表(每一次表的操作都会影响整行，如update会直接替换原来一行，delete会删除整行)，对于创建索引的时候，如果是不可变的那么所以就不需要进行实时维护，对于全局索引如果是不可变的，那么写到索引表是在客户端完成的，而可变的索引是在服务端完成的。不可变索引写性能要高些。 row_timestamp 指定主键中的某个字段为行的timestamp, 那么在插入的时候就指定这个时间为数据的真实timestamp（列上的timestamp都为这个时间， 这个时间可以指定也可以是服务端自动生成，由于是rowkey中的一部分，所以这个时间一定不可能发生变化，以后所有的upsert语句都不会更改这个时间）, 此处必须注意，如果数据删除之后, 再不能插入主键为相同的数据（因为主键相同，那么意味着列的timestamp并没有发生任何变化， 此时hbase已经将此timestamp的版本设置为了删除标识，所以即使插入数据版本也没有发生变化，还是删除的）， 所以对于这样的表，数据一定不能删除。不建议在表中使用这个字段，除非自己知道自己在干什么。此方式并不会减少数据存储，timestamp也会存在与主键之中，仅仅更改了列的版本的处理方式。 phoenix为每一行添加了一个空的列， 别去列mapping可以设置mapping字节数，下面有讲解。 更改表的分区策略 1alter &apos;TEST&apos;, &#123;SPLIT_POLICY =&gt; &apos;org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy&apos;, CONFIGURATION =&gt; &#123;&apos;KeyPrefixRegionSplitPolicy.prefix_length&apos; =&gt; &apos;2&apos;&#125;&#125; 插入数据，插入数据没有主键约束，可以多次插入，会覆盖原先的数据 1upsert into test values(&apos;00&apos;, 1555385043000, &apos;1234567890&apos;, &apos;hello phoenix&apos;); 案例 1create table TEST(PREFIX char(2) not null, TM char(14) not null, STCD char(10) not null, DRP varchar, INTV varchar, PDR varchar, DYP varchar, WTH varchar, constraint pk primary key(PREFIX, TM desc, STCD)) SPLIT ON (&apos;40&apos;, &apos;80&apos;, &apos;c0&apos;); 1create table st_statistics_pptn_day_new(stcd char(10) not null, tm char(8) not null, drp BIGINT, constraint pk primary key(stcd, tm desc)) SALT_BUCKETS = 4; 1create table st_statistics_pptn_hour_new(stcd char(10) not null, tm char(10) not null, drp BIGINT, constraint pk primary key(stcd, tm desc)) SALT_BUCKETS = 4; 说明 : SALT_BUCKETS 大小通常是cpu core (0.5 ~ 2) region server, 例如: 6 0.5 3 = 9 row key 顺序扫描可以优化，通过在 hbase-sites.xml 中指定 phoenix.query.rowKeyOrderSaltedTable=true 禁止用户指定分割点 加盐后会在row key前面添加一个字节（0-255），并且会自动进行分割，如果需要手动split，需要添加上前面的一个盐字节 查询 1scan ‘table’ &#123; STARTROW =&gt; &apos;FixedWidthUsername&apos; LIMIT =&gt; 30&#125; 注意：如果前缀为16进制，则必须使用双引号，如下 1scan &apos;tsdb&apos;, &#123;STARTROW =&gt;&quot;\x00\x00\x3c&quot;, LIMIT =&gt; 6&#125; hbase 列预测限定，因为列进行mapping的时候，如果知道有多少列，那么就可以使用固定大小的mapping值来进行对应 1COLUMN_ENCODED_BYTES = 1 此处1表示一个字节，那么做多可以代表256个数值，也就可以进行转换256个列 不可变表（IMMUTABLE_ROWS）列encoding，通过encoding可以降低存储，提供更好的性能 1IMMUTABLE_STORAGE_SCHEME = SINGLE_CELL_ARRAY_WITH_OFFSETS 此处表示，单个cell存储一行的一个列族，建议不可变表使用这种设置， 并且设置此参数的时候列的mapping必须设定，也就是说需要添加 COLUMN_ENCODED_BYTES 参数 注意：这种优化策略不用应用与稀疏表，行中有50%的数据存在则不是稀疏表，与之对应的设定就是 IMMUTABLE_STORAGE_SCHEME = ONE_CELL_PER_COLUMN 动态列，由于hbase的列的灵活性，所以phoenix也提供了这种动态添加列的方式， 也就是在create schema初期并不会提供列名，而是在插入和查询的时候指定需要查询的列和列对应的类型。从这里可以看到后期绑定也是非常实用的功能。 修改表类型 ALTER TABLE my_table SET IMMUTABLE_ROWS=true,DISABLE_WAL=true; HBase minor compact只进行文件的合并，Major compact除了文件的合并还包括让HFile和RegionServer本地化(这通常是因为Region迁徙造成的) hbase读优化 Get本质上也是Scan来进行操作 在进行数据获取的时候，尽量只获取需要的数据 控制Scan之后RegionServer是否缓存Block，默认缓存，但对于MapReduce应用来说通常不需要缓存块 可以设置Cache用来客户端缓存数据 可以设置Batch用来控制每次从服务端获取的数量，减少rpc 开启集群自动负载均衡，设置Compact策略 控制BlockCache的比率 hfile.block.cache.size 默认0.4 建表常用的参数 123456789101112131415161718192021222324252627282930313233343536373839404142CREATE IMMUTABLE TABLE IF NOT EXISTS cmsw.t_monitor_data ( code varchar(32) not null, tm date not null, station_type tinyint, real_time_flow decimal(6,2), voltage decimal(6,2), electric_current decimal(6,2), energy_consumption decimal(6,2), efficiency decimal(6,2), cod decimal(6,2), runtime date, day_flow decimal(10,2), year_flow decimal(10,2), station_status smallint, device_code varchar(64), is_fault smallint, is_running smallint, conductivity decimal(8,2), is_alarm_node tinyint, created_time date, liquid_flow decimal(6,2), notify_type varchar(100), device_id varchar(64), gateway_id varchar(64), data_signal varchar(10), snr integer, ecl integer, CONSTRAINT pk PRIMARY KEY (code, tm desc)) SALT_BUCKETS = 18,VERSIONS = 1, IMMUTABLE_STORAGE_SCHEME = SINGLE_CELL_ARRAY_WITH_OFFSETS, COLUMN_ENCODED_BYTES = 1;//TTL = 60IMMUTABLE_ROWS=trueSPLIT ON (&apos;40&apos;, &apos;80&apos;, &apos;c0&apos;)COMPRESSIONDISABLE_WALSTORE_NULLS phoenix覆盖所有导致数据不一致问题，如： 1234567主表：a b c d e，a b为主键覆盖索引: a b c d业务插入：1 1 1 1 11 1 2 1 1结果：主表中只有一条 1 1 2 1 1，覆盖索引中有两条 1 1 1，1 1 2查询：select a b d from table，此时返回两条相同的数据 phoenix 添加 query server 连接问题，注意查看官方文档，如果配置kerberos，在连接的时候需要指定对应的kerberos 1jdbc:phoenix:thin:url=http://bd1.dhls.com:8765;authentication=SPNEGO;principal=HTTP/bd1.dhls.com@DHLS.COM;keytab=/Users/fengxiang/spnego.service.keytab;serialization=protobuf 注意：ranger中需要添加HTTP用户，并且设置好权限]]></content>
      <categories>
        <category>hadoop</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn客户端和服务端安装]]></title>
    <url>%2F2018%2F03%2F15%2Flinux-openvpn%2F</url>
    <content type="text"><![CDATA[openvpn服务端安装 安装openvpn 12345yum install -y epel-releaseyum update -yyum install -y openssl lzo pam openssl-devel lzo-devel pam-devel expectyum install -y easy-rsayum install -y openvpn 配置easy-rsa 12345678cp -rf /usr/share/easy-rsa/3.0.3 /etc/openvpn/server/easy-rsacd /etc/openvpn/server/easy-rsa./easyrsa init-pki./easyrsa build-ca nopass./easyrsa build-server-full server nopass./easyrsa build-client-full client nopass./easyrsa gen-dhopenvpn --genkey --secret ta.key 配置openvpn 1234mkdir -p /var/log/openvpn/chown openvpn:openvpn /var/log/openvpnmkdir -p /etc/openvpn/server/usermkdir -p /etc/openvpn/server/ccd 编辑/etc/openvpn/server/server.conf文件，并写入以下内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152################################################## This file is for the server side ## of a many-clients &lt;-&gt; one-server ## OpenVPN configuration. ## ## Comments are preceded with '#' or ';' ##################################################port 1194proto tcp-server## Enable the management interface# management-client-auth# management localhost 7505 /etc/openvpn/user/management-filedev tap # TUN/TAP virtual network deviceuser openvpngroup openvpnca /etc/openvpn/server/easy-rsa/pki/ca.crtcert /etc/openvpn/server/easy-rsa/pki/issued/server.crtkey /etc/openvpn/server/easy-rsa/pki/private/server.keydh /etc/openvpn/server/easy-rsa/pki/dh.pemtls-auth /etc/openvpn/server/easy-rsa/ta.key 0## Using System user auth.# plugin /usr/lib64/openvpn/plugins/openvpn-plugin-auth-pam.so login## Using Script Pluginsauth-user-pass-verify /etc/openvpn/server/user/checkpsw.sh via-envscript-security 3# client-cert-not-required # Deprecated optionverify-client-certusername-as-common-name## Connecting clients to be able to reach each other over the VPN.client-to-client## Allow multiple clients with the same common name to concurrently connect.# duplicate-cnclient-config-dir ccd# ifconfig-pool-persist ipp.txtserver 10.8.0.0 255.255.255.0push "dhcp-option DNS 223.5.5.5"push "dhcp-option DNS 114.114.114.114"push "dhcp-option DNS 8.8.8.8"# push "route 10.93.0.0 255.255.255.0"# comp-lzo - DEPRECATED This option will be removed in a future OpenVPN release. Use the newer --compress instead.compress lzo# cipher AES-256-CBCncp-ciphers "AES-256-GCM:AES-128-GCM"## In UDP client mode or point-to-point mode, send server/peer an exit notification if tunnel is restarted or OpenVPN process is exited.# explicit-exit-notify 1keepalive 10 120persist-keypersist-tunverb 3log /var/log/openvpn/server.loglog-append /var/log/openvpn/server.logstatus /var/log/openvpn/status.log 启动配置文件 12cd /etc/openvpn/server/ln -sf server.conf .service.conf 创建用户密码文件 12345tee /etc/openvpn/server/user/psw-file &lt;&lt; EOFmytest mytestpassEOFchmod 600 /etc/openvpn/server/user/psw-filechown openvpn:openvpn /etc/openvpn/server/user/psw-file 注意 : 用户名和密码可以多行, 表示多个用户. 这里的用户名和密码, 和上面的 client 不是一回事, client 表示密钥, 所有的用户都可以使用同一个密钥, 也可以使用不同的, 所有通常为了简单只创建一个密钥, 并通过用户名来区分. 创建密码检查脚本 /etc/openvpn/server/user/checkpsw.sh 1234567891011121314151617181920212223242526272829#!/bin/sh############################################################ checkpsw.sh (C) 2004 Mathias Sundman &lt;mathias@openvpn.se&gt;## This script will authenticate OpenVPN users against# a plain text file. The passfile should simply contain# one row per user with the username first followed by# one or more space(s) or tab(s) and then the password.PASSFILE="/etc/openvpn/server/user/psw-file"LOG_FILE="/var/log/openvpn/password.log"TIME_STAMP=`date "+%Y-%m-%d %T"`###########################################################if [ ! -r "$&#123;PASSFILE&#125;" ]; then echo "$&#123;TIME_STAMP&#125;: Could not open password file \"$&#123;PASSFILE&#125;\" for reading." &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiCORRECT_PASSWORD=`awk '!/^;/&amp;&amp;!/^#/&amp;&amp;$1=="'$&#123;username&#125;'"&#123;print $2;exit&#125;' $&#123;PASSFILE&#125;`if [ "$&#123;CORRECT_PASSWORD&#125;" = "" ]; then echo "$&#123;TIME_STAMP&#125;: User does not exist: username=\"$&#123;username&#125;\", password=\"$&#123;password&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiif [ "$&#123;password&#125;" = "$&#123;CORRECT_PASSWORD&#125;" ]; then echo "$&#123;TIME_STAMP&#125;: Successful authentication: username=\"$&#123;username&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125; exit 0fiecho "$&#123;TIME_STAMP&#125;: Incorrect password: username=\"$&#123;username&#125;\", password=\"$&#123;password&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125;exit 1 赋予写权限 1chmod +x /etc/openvpn/server/user/checkpsw.sh 启动服务 12345678# 查看service名rpm -ql openvpn |grep service/usr/lib/systemd/system/openvpn-client@.service/usr/lib/systemd/system/openvpn-server@.service/usr/lib/systemd/system/openvpn@.service# 启动systemctl start openvpn-server@.service.servicesystemctl enable openvpn-server@.service.service 问题总结: 添加用户 1echo user password &gt;&gt; /etc/openvpn/server/user/psw-file 为用户固定IP地址 123tee /etc/openvpn/server/ccd/$&#123;user&#125; &lt;&lt; EOFifconfig-push 10.8.0.30 255.255.255.0EOF 注意 : 替换此处的用户名, 和IP地址 obfsproxy服务端安装 安装python, 版本最好是大于2.7.5，pip版本最好为9.0.*版本 1yum install -y python-devel bzip2 bzip2-devel zlib gcc 安装pip和对应工具 12345wget https://bootstrap.pypa.io/get-pip.pypython get-pip.pypip install --upgrade setuptoolspip install --upgrade incrementalpip install psutil 安装Twisted 123wget https://twistedmatrix.com/Releases/Twisted/18.9/Twisted-18.9.0.tar.bz2tar -jxvf Twisted-18.9.0.tar.bz2python setup.py install 安装obfsproxy 1234git clone https://github.com/dounine/obfsproxy.gittar -xzf obfsproxy-0.2.13.tar.gzpython setup.py installln -s /usr/bin/obfsproxy /usr/local/bin/obfsproxy 注意 : 中间可能还会出现其它python问题, google解决即可 启动obfsproxy 1obfsproxy obfs3 --dest=127.0.0.1:1194 server 0.0.0.0:10102 说明 : dest表示obfsproxy接收到的数据传输到哪里, 此处接收到数据传输到openvpn, server表示它是服务端, 0.0.0.0:10102表示监听的端口号. 添加obfsproxy的防火墙配置 12firewall-cmd --permanent --add-port=10102/tcpfirewall-cmd --reload obfsproxy客户端安装 安装obfsproxy客户端和服务端完全一样 启动obfsproxy 1obfsproxy obfs3 --dest=服务端IP地址:10102 client 127.0.0.1:9997 openvpn客户端安装 安装openvpn客户端和服务端完全一样 从server上下载文件到客户端 1234/etc/openvpn/server/easy-rsa/pki/ca.crt/etc/openvpn/server/easy-rsa/pki/issued/client.crt/etc/openvpn/server/easy-rsa/pki/private/client.key/etc/openvpn/server/easy-rsa/ta.key 配置openvpn，并添加配置文件client.ovpn, 内容如下: 12345678910111213141516clientproto tcp-clientdev tapauth-user-passremote 127.0.0.1 9997ca ca.crtcert client.crtkey client.keytls-auth ta.key 1remote-cert-tls serverauth-nocachepersist-tunpersist-keycomp-lzoverb 4mute 10 注意 : 此处的remote并没有设置为openvpn的服务端地址, 而是设置为obfsproxy客户端监听的地址, 这样openvpn就先把数据发送给obfsproxy客户端, obfsproxy客户端再发送给obfsproxy服务端, obfsproxy服务端再发送给openvpn服务端. openvpn经常会出现闪断 为openvpn客户端和服务端都创建启动脚本，并加入到crontab中，这样就不怕断开连接 1https://github.com/ilivoo/tools-openvpn]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn 动态资源分配]]></title>
    <url>%2F2018%2F01%2F23%2Fhadoop-yarn%2F</url>
    <content type="text"><![CDATA[spark动态资源分配 开启External shuffle service Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle数据，给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。通过该服务来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他Executor的任务运行。 NodeManager中启动External shuffle Service 在yarn-site.xml中添加如下配置 12345678910&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 可选配置--&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 拷贝${SPARK_HOME}/lib/spark-${SPARK_VERSION}-yarn-shuffle.jar 到${HADOOP_HOME}/share/hadoop/yarn/lib/目录下, 并重启NodeManager进程，也就启动了External shuffle Service。 Spark应用中使用External shuffle Service， 在spark-defaults.conf中添加配置或者直接使用–conf配置 12spark.shuffle.service.enabled truespark.shuffle.service.port 7337 注意： 如果 yarn.nodemanager.aux-services配置项已存在，则在 value 中添加spark_shuffle，且用逗号和其他值分开。 spark.shuffle.service.port的值需要和上面yarn-site.xml中的值一样， 可以都直接使用默认值。 实际上现在hdp3.0 以上版本都已经自动开启了spark的External shuffle Service, 只需要设置在spark-defaults 和 yarn-site中设置 spark.shuffle.service.port=7337 spark程序开启动态资源分配 1234567spark.dynamicAllocation.enabled truespark.dynamicAllocation.minExecutors 1spark.dynamicAllocation.initialExecutors 1spark.dynamicAllocation.maxExecutors 9spark.dynamicAllocation.executorIdleTimeout 60spark.dynamicAllocation.cachedExecutorIdleTimeout 60spark.executor.cores 2 注意： cores尽量设置为 &lt;= 3, 也就是说通常都是增大maxExecutors, 减小cores yarn 资源分配建议 设置Executor最大可用cores 和 memery， 这样能防止单个机器压力过大， 对于总的物理cores和memery来说需要减去操作系统以及hbase等其它的存储需要消耗的资源才为yarn最大可用资源， 通常需要预留生效资源的10%左右]]></content>
      <categories>
        <category>hadoop</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kerberos的使用以及各种权限说明]]></title>
    <url>%2F2018%2F01%2F13%2Fhadoop-kerberos%2F</url>
    <content type="text"><![CDATA[kerberos客户端配置 krb5.conf配置选项参考： https://linux.die.net/man/5/krb5.conf 配置kerberos多用户ticket缓存名字 12#vi /etc/profiledefault_ccache_name = DIR:/tmp/kerberos 说明： 也可以设置环境变量达到相同的效果export KRB5CCNAME=DIR:/tmp/kerberos kerberos客户端在任意时刻只能有一个票证可用， 所以不管如何配置都不可能达到一次配置一次认证就可以了，而是认证之后可以进行多次切换不同的认证 多个kerberos的krb5.conf合并 通常情况每个kerberos服务器有一个krb5.conf文件， 如果有多个kerberos服务器需要认证那么通常会比较麻烦，需要拷贝配置文件，所以通常情况下我们可以讲这些文件合并一个文件。通常情况下只需要合并realms 就可以了，如下： 123456789[realms] ILIVOO.COM = &#123; admin_server = sandbox-hdp.hortonworks.com kdc = sandbox-hdp.hortonworks.com &#125; EXAMPLE.COM = &#123; admin_server = dev1 kdc = dev1 &#125; 多个keytab文件合并 1234#ktutilktutil: rkt /etc/security/keytabs/hdfs.keytabktutil: rkt /etc/security/keytabs/hbase.keytab ktutil: wkt /etc/krb5.keytab kerberos客户端工具使用 查看keytab文件中用户 klist -kt /etc/krb5.keytab 认证 12kinit -k admin/admin@ILIVOO.COMkinit -k admin/admin@EXAMPLE.COM 显示认证 显示当前认证 klist 显示所以认证 klist -A 切换认证 kswitch -p admin/admin@ILIVOO.COM 销毁认证 销毁当前认证 kdestroy 注意销毁当前认证， 并不会自动选择一个当前认证 销毁所有认证 kdestroy -A 更新认证 kinit -R kerberos服务端工具使用说明： headless keytab与没有headless的keytab的区别， 主要是headless keytab可以在任何机器上使用，而没有headless的keytab只能在特定的服务器使用， 也就是说它们是服务器绑定的， 如： headless keytab 名称是 `hdfs-ilivoo@ILIVOO.COM， 而没有headless keytab是nn/hdp1.ilivoo.com@ILIVOO.COM` kerberos管理员账户： `*/admin@ILIVOO.COM(可以通过/var/kerberos/krb5kdc/kadm5.acl `指定) 添加Principal 12kadmin.localaddprinc -randkey admin-ilivoo@ILIVOO.COM 导出keytab文件 123cd /etc/security/keytabkadmin.localktadd/xst -k admin.headless.keytab admin-ilivoo 注意 : 如果keytab是随机密码的, 那么直接使用上面就可以了, 但是如果上面的keytab不是使用randkey, 那么此时必须使用 -norandkey 参数导出来才不会有错. 例如: 1ktadd/xst -norandkey -k admin.admin.keytab admin/admin 合并keytab文件 12345cd /etc/security/keytabktutilrkt admin.headless.keytabrkt hdfs.headless.keytabwkt all.headless.keytab ambari、kerberos、service、host 权限 通常情况下来说为了方便， 我们会在kerberos中创建一个admin用户， 以后使用admin账户就可以访问所有的hadoop服务， 而在hadoop中也建立一个admin用户，并加入到hadoop组当中， 让其和kerberos中的admin用户对应， 但是实际上它们没有任何的关系。 kerberos权限仅仅用来判定AB问题， 也就是说如何证明自己是自己所申称的人， 所以它需要引入一个受信的第三方来作为验证。 但是对于hadoop中的各组件来说， 通常都是每个组件一个用户， 并且把所有的组件用户加入到一个hadoop用户组当中。所以对于kerberos的admin用户仅仅只是能够声明我是一个管理员用户，它可以访问所store the credentials有的启动kerberos的hadoop服务， 但是hadoop通常来说每个用户都有自己的用户访问权限， 也就是说kerberos只是作为第一个访问这个服务的权限， 而服务自身的权限就和kerberos没有任何关系了。如hdfs当中， 所有的用户权限是根据本地linux模型来管理的， 所以必须要有linux本地用户的权限才可以进行访问。 对于ambari来说它是一个集群的管理着， 它有自己权限管理， 通常分为集群管理者、集群使用者、服务管理者、服务操作者、服务开发者等等。 并且它的权限用户通常也是可以和host的权限进行对应上的。如：在主机上创建了 admin 用户， 并且在ambari中创建了admin用户， 并且把admin用户配置成为集群管理者。 kerberos主要是用来验证某个人就是自己所声称的那个人， 所以通常情况下也是和主机的用户对于的， 也就是说根据需要是否在kerberos中创建主机用户的principal。通常情况下， service使用principal验证能够访问自己的用户， 那么对应用户访问的时候就必须为自己获取票证。 通常来说service是为了验证的话才需要为自己开启权限验证， 并且指定只有某种用户才能访问自己， 如果其它的服务或者客户端需要访问自己， 那么就必须获得票证。并且通常来说service有自己权限验证，可以将其授权给某个用户， 也就是说想要访问service必须先证明你就是自己声称的那个人， 这样就表示通过了service的kerberos验证， 再就是service有自己的权限验证(acl)， service必须为这个用户授权之后这个用户才能访问这个服务。通常情况下服务与服务之间的kerberos验证是非常严格的， 如： DataNode想要连接到NameNode那么必须有 `nn/host@ILIVOO.COM, 而服务的使用者却比较宽泛点， 如hdfs用户想要访问集群， 那么通常只需要hdfs-ilivoo@ILIVOO.COM`， 甚至其它的用户也可以创建文件等等， 只是hdfs用户通常被指定为管理用户，其它用户需要hdfs用户为其授权。 host权限主要是用来控制对主机的访问权限， 但是通常情况下来说host权限也与其它的权限对应上， 方便服务的开发与管理。 通过上面的分析， 通常情况下我们使用这些复杂的权限的时候会非常小心， 并且规划好权限对于我们的开发至关重要。如下案例： 运维人员， 通常都是需要对集群进行管理， 并监控集群的状态， 通常情况可以简单分为两种运维人员， 集群管理员， 集群监测员，创建权限如下： 集群管理员： host中创建 cluster admin 用户(cadmin), ambari中创建用户(cadmin)用户，并为其设置为集群管理员角色。 集群监测源： host中创建 cluster view 用户(cview), ambari中创建(cview)用户， 并为其设置为集群操作员角色。 开发人员， 通常对服务进行启动停止， 配置修改， 对某些服务进行访问， 通常不同的开发人员有不用的权限。如下案例： spark开发人员： host中创建开发用户(feng), ambari中创建(feng)，并为其设置集群服务操作员角色， 在kerberos中添加票证， 并导出keytab文件给用户。 为用户配置hdfs权限， yarn权限， spark权限，hbase权限， kafka权限， hive权限等等。 注意： hdfs、yarn、spark、hbase、kafka、hive等为feng添加权限的时候， 通常需要先使用kinit登录到自己的权限， 再讲权限设置给feng用户。 如： hbase 为feng用户添加权限 123kinit -kt /var/kerberos/keytab/hbase.headless.keytab hbase-ilivoo@ILIVOO.COMhbase shellgrant &apos;feng&apos;, &apos;RWCA&apos; #授权feng用户有rwca权限 当然： 现在情况有所变化， hdfs、yarn、hbase、kafka、hive等都可以使用ranger为其配置权限， 所以如果这些组件使用了ranger插件， 那么就必须使用ranger为其配置。 建议能够使用hdfs自己的权限位， 就不要使用ranger来管理。 kafka安装kerberos 参考 https://www.cnblogs.com/dongxiao-yang/p/7131626.html 服务端修改基本配置 12listeners=SASL_PLAINTEXT://:6667 --修改listenerssasl.kerberos.service.name=kafka --添加kerberos.name 启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用, 建立 client.properties文件 1security.protocol=SASL_PLAINTEXT 新命令如下： 12345678所以新命令的使用方式为/usr/hdp/3.1.0.0-78/kafka/bin/kafka-topics.sh --create --zookeeper hdp1.ilivoo.com:2181 --replication-factor 3 --partitions 12 --topic testbin/kafka-consumer-groups.sh --bootstrap-server hdp1.ilivoo.com:6667 --list --command-config client.propertiesbin/kafka-console-producer.sh --broker-list hdp1.ilivoo.com:6667 --topic dxTT --producer.config client.properties过期，需要重新申请或者renew Ticketbin/kafka-console-consumer.sh --bootstrap-server hdp1.ilivoo.com:6667 --topic dxTT --consumer.config client.properties --from-beginning kerberos遇到的问题 kinit -R 无法续约，这是因为kerberos的每个principal都可以指定超过此时间则ticket就会过期，需要重新申请或者renew有效时间和刷新时间， 默认添加principal的时候并没有指定它的刷新时间和有效时间，所以就使用了服务器默认配置的时间 kerberos Server上的/var/kerberos/krb5kdbc/kdc.conf中的max_life kadmin.local 1234modprinc -maxlife &quot;1 week&quot; feng@ILIVOO.COMmodprinc -maxrenewlife &quot;1 week&quot; feng@ILIVOO.COMmodprinc -maxlife &quot;1 week&quot; krbtgt/ILIVOO.COM@ILIVOO.COMmodprinc -maxrenewlife &quot;1 week&quot; krbtgt/ILIVOO.COM@ILIVOO.COM 注意：krbtgt表示服务端的tgt必须设置， 设置完成需要重新kinit kerberos 的票据的生命周期由lifetime和renewlifetime决定， 当生命周期超过lifetime则ticket就会过期，需要重新申请或者renew，如果重新申请非常容易理解， 直接重新申请就可以了。如果需要renew则首先要开启renew功能， 由上一个问题解决。通常情况下来说，renewlifetime会大于lifetime，如：lifetime=2d，renewlifetime=7d。而这两个时间由五个基本的时间决定： 最基本由kerberos Server上的/var/kerberos/krb5kdbc/kdc.conf中的max_life 和 max_renewable_life决定 123456[realms] ILIVOO.COM = &#123; max_life = 2d max_renewable_life = 7d ... &#125; 内置principal krbtgt的maxmum ticket life 和 renew life,可在kadmin命令下执行getprinc命令查看 使用的principal的maximum tiket life 和 renew life，可在kadmin命令下用getprinc命令查看 再就是kerberos client上/etc/krb5.conf的ticket_lifetime 和 renew_lifetime 1234[libdefaults] ticket_lifetime = 2d renew_lifetime = 7d ... kinit 参数后面指定的时间, 如： kinit -l 2d -r 7d my_principal 注意：上面五个生命周期依次往上逐渐取最小 kerberos spark集群连接hbase、kafka， 由于spark以及处理了kerberos认证问题，在spark的Driver启动的时候spark获取kerberos认证， 并将认证打包成代理Token存放在HDFS上， 当启动Executor的时候会带上这个Token用于Executor上的验证。当Token快要失效的时候Spark Driver会再次去刷新认证， 并讲认证信息告诉Executor，从而保证整个过程中长时间运行。 注意：hbase的classpath是一个麻烦的事情， 因为通常情况下如果简单指定hbase-client会导致hbase 在Executor中没有认证的现象，所以必须讲hbase-server依赖带上。]]></content>
      <categories>
        <category>hadoop</category>
        <category>kerberos</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 存储程序]]></title>
    <url>%2F2018%2F01%2F04%2Fmysql-procedure%2F</url>
    <content type="text"><![CDATA[存储程序 MySQL允许通过触发器、存储过程、函数的形式来存储代码， 并且可以在定时任务中存放代码，这个定时任务也被成为”事件”。 一般来说，存储代码是一种很高的共享和复用代码的方法。 存储代码的优点： a. 在服务器内部运行，离数据最近，另外在服务器上执行还可以节省带宽和网络延迟。 b. 代码重用。 c. 可以简化代码的维护和版本更新。 d. 可以帮助提升安全，例如提供更细粒度的权限控制。如银行资金转移。 f. 服务器可以换成存储过程的执行计划。 g. 因为是在服务器端部署的，所以备份、维护都可以在服务器端完成。 h. 它可以在应用程序和数据库开发人员之间更好的分工。 存储代码的缺点： a. MySQL本身没有提供好用的开发和调试工具。 b. 较之应用程序的代码，存储代码效率要稍微差些。 例如：函数有限，无法编写复杂字符串维护功能。 c. 存储代码可能会给应用程序代码的部署带来额外的复杂性。 d. 可能有安全隐患，最好加密。 e. 存储过程会给数据库服务器增加额外的压力，而数据库服务器的扩展性相比应用服务器要差很多。 f. MySQL不能控制存储过程的资源消耗，所以在存储过程中的一个小错误，可能直接把服务器拖死。 g. 调试MySQL的存储过程是一件很困难的事情。 存储过程和函数 我们通常会希望程序越小、越简单越好。希望将更复杂的处理逻辑交给上层的应用实现，通常这样会使代码更易读、易维护，也会更灵活。这样做也会让你拥有更多的计算资源，潜在的还会让你拥有更多的缓存资源。 不过，对于某些操作，存储过程比其他的实现要快的多–特别是一个存储过程调用可以代替很多小查询的时候。如果查询很小，相比这个查询执行的成本，解析和网络开销就变得非常明显。 存储过程优点 存储代码，节省很多网络开销，如循环insert 封装核心业务 存储过程可以回传值，并可以接受参数。 存储过程缺点 存储过程无法使用 SELECT 指令来运行，因为它是子程序，与查看表，数据表或用户定义函数不同。 存储过程，往往定制化于特定的数据库上，因为支持的编程语言不同。当切换到其他厂商的数据库系统时，需要重写原有的存储过程。 存储过程的调试比较麻烦， 并且出错后对系统影响较大 基于语句的复制通常情况下来说需要在复制库中执行 存储过程与函数的区别 本质上没区别，执行的本质都一样。 函数只能返回一个变量的限制，而存储过程可以返回多个。 函数是可以嵌入在sql中使用的,可以在select中调用，而存储过程要让sql的query 可以执行， 需要把 mysql_real_connect 的最后一个参数设置为CLIENT_MULTI_STATEMENTS。 函数限制比较多，比如不能用临时表，只能用表变量．还有一些函数都不可用等等．而存储过程的限制相对就比较少。 一般来说，存储过程实现的功能要复杂一点，而函数的实现的功能针对性比较强。存储过程，功能强大，可以执行包括修改表等一系列数据库操作；用户定义函数不能用于执行一组修改全局数据库状态的操作。 对于存储过程来说可以返回参数，如记录集，而函数只能返回值或者表对象。函数只能返回一个变量；而存储过程可以返回多个。存储过程的参数可以有IN,OUT,INOUT三种类型，而函数只能有IN类，存储过程声明时不需要返回类型，而函数声明时需要描述返回类型，且函数体中必须包含一个有效的RETURN语句。 存储过程，可以使用非确定函数，不允许在用户定义函数主体中内置非确定函数。 存储过程一般是作为一个独立的部分来执行（ EXECUTE 语句执行），而函数可以作为查询语句的一个部分来调用（SELECT调用），由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字的后面。 SQL语句中不可用存储过程，而可以使用函数。 当存储过程和函数被执行的时候，SQL Manager会到procedure cache中去取相应的查询语句，如果在procedure cache里没有相应的查询语句，SQL Manager就会对存储过程和函数进行编译。 Procedure cache中保存的是执行计划 (execution plan) ，当编译好之后就执行procedure cache中的execution plan，之后SQL SERVER会根据每个execution plan的实际情况来考虑是否要在cache中保存这个plan，评判的标准一个是这个execution plan可能被使用的频率；其次是生成这个plan的代价，也就是编译的耗时。保存在cache中的plan在下次执行时就不用再编译了。 触发器 触发器可以让你在执行INSERT、UPDATE或者DELETE的时候，执行一些特定的操作。可以在MySQL中指定是在SQL语句执行前触发还是在执行后触发。触发器本身没有返回值，不过它们可以读取或者改变触发SQL语句所影响的数据。所以，可以使用触发器实现一些强制限制，或者而某些业务逻辑，否则，就需要在应用程序中实现这些逻辑。 因为使用触发器可以减少客户端和服务器之间的通信，所以触发器可以简化应用逻辑，还可以提高性能。另外，还可以用于自动更新反范式化数据或者汇总表数据。 MySQL触发器实现非常简单，所以功能也有限。特别需要注意一下几点： a. 对应每一个表的每一个事件，最多只能定义一个触发器(换句话说，不能在AFTER INSERT上定义两个触发器) b. MySQL只支持”基于行的触发” – 也就是说，触发器始终是针对一条记录的，而不是针对整个SQL语句的。如果变更的数据集非常大的话，效率会很低。 触发器本身的限制： a. 触发器可以掩盖服务器背后的工作。例如SQL影响的记录数翻一倍。 b. 触发器的问题很难排除 c. 触发器可能导致死锁或所等待。 在InnoDB表上的触发器是在同一个事务中完成的，所以它们执行的操作是原子的，原子操作和触发器操作会同时失败或者成功。不过，如果在InnoDB表上的触发器去检查数据的一致性，需要特别小心MVCC，稍不小心，可能会获得错误的结果。 可以使用触发器记录数据变更日志。 定时任务 定时任务类似于Linux的定时任务，不过是完全在MySQL内部实现的。你可以创建事件，执行MySQL在某个时候执行一段SQL代码，或者每隔一段时间执行一段SQL代码。通常，我们会把复杂的SQL都封装到一个存储过程中，这样事件在执行的时候只需要做一个简单的CALL调用。 如果一个定时事件执行需要很长的时间，那么有可能会出现这样的情况，即前面一个事件还未执行完成，下一个时间点的事件又开始了。MySQL本身不会防止这种并发，所以需要用户在自己编写这种情况下防并发代码。你可以使用函数GET_LOCK()来确保当前总是有一个事件在被执行。 虽然事件的执行是和连接无关的，但是它仍然是线程级别的。MySQL中有一个事件调度线程，必须在MySQL配置文件中设置，或者使用下面的命令来设置。SET GLOBAL event_scheduler :=1 在存储过程中保留注释：/! xxxxxxxxxxxxxxxxxxxx / MySQL在服务器端提供只读的、单向的游标，而且只能在存储过程或者更底层的客户端API中使用。因为MySQL游标中指向的对象都是存储在临时表中而不是实际查询到的数据，所以MySQL游标总是只读的。它可以逐行指向查询结果，然后让程序做进一步处理。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql exists与in的区别]]></title>
    <url>%2F2018%2F01%2F03%2Fmysql-exist-in%2F</url>
    <content type="text"><![CDATA[exists exists对外表用loop逐条查询，每次查询都会查看exists的条件语句，当 exists里的条件语句能够返回记录行时(无论记录行是的多少，只要能返回)，条件就为真，返回当前loop到的这条记录，反之如果exists里的条 件语句不能返回记录行，则当前loop到的这条记录被丢弃，exists的条件就像一个bool条件，当能返回结果集则为true，不能返回结果集则为 false，如下： 1select * from user where exists (select 1); 对user表的记录逐条取出，由于子条件中的select 1永远能返回记录行，那么user表的所有记录都将被加入结果集，所以与 select * from user;是一样的，又如下 1select * from user where exists (select * from user where userId = 0); 注意： 可以知道对user表进行loop时，检查条件语句(select * from user where userId = 0),由于userId永远不为0，所以条件语句永远返回空集，条件永远为false，那么user表的所有记录都将被丢弃 not exists与exists相反，也就是当exists条件有结果集返回时，loop到的记录将被丢弃，否则将loop到的记录加入结果集 总的来说，如果A表有n条记录，那么exists查询就是将这n条记录逐条取出，然后判断n遍exists条件 in in查询相当于多个or条件的叠加，这个比较好理解，比如下面的查询 select * from user where userId in (1, 2, 3); 等效于 select * from user where userId = 1 or userId = 2 or userId = 3; not in与in相反，如下 select * from user where userId not in (1, 2, 3); 等效于 select * from user where userId != 1 and userId != 2 and userId != 3; 总的来说，in查询就是先将子查询条件的记录全都查出来，假设结果集为B，共有m条记录，然后在将子查询条件的结果集分解成m个，再进行m次查询 值得一提的是，in查询的子条件返回结果必须只有一个字段，例如 select * from user where userId in (select id from B); 而不能是 select * from user where userId in (select id, age from B); 而exists就没有这个限制 exists和in的性能 考虑如下SQL语句 1: select from A where exists (select from B where B.id = A.id); 2: select * from A where A.id in (select id from B); 查询1.可以转化以下伪代码，便于理解 123456for ($i = 0; $i &lt; count(A); $i++) &#123; $a = get_record(A, $i); #从A表逐条获取记录 if (B.id = $a[id]) #如果子条件成立 $result[] = $a;&#125;return $result; 大概就是这么个意思，其实可以看到,查询1主要是用到了B表的索引，A表如何对查询的效率影响应该不大 假设B表的所有id为1,2,3,查询2可以转换为 select * from A where A.id = 1 or A.id = 2 or A.id = 3; 这个好理解了，这里主要是用到了A的索引，B表如何对查询影响不大 再看not exists 和 not in 1: select from A where not exists (select from B where B.id = A.id); 2: select * from A where A.id not in (select id from B); 看查询1，还是和上面一样，用了B的索引, 而对于查询2，可以转化成如下语句 select * from A where A.id != 1 and A.id != 2 and A.id != 3; 可以知道not in是个范围查询，这种!=的范围查询无法使用任何索引,等于说A表的每条记录，都要在B表里遍历一次，查看B表里是否存在这条记录故not exists比not in效率高 mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in： 例如：表A（小表），表B（大表） 1: select * from A where cc in (select cc from B) 效率低，用到了A表上cc列的索引； select * from A where exists(select cc from B where cc=A.cc) 效率高，用到了B表上cc列的索引。 相反的 2: select * from B where cc in (select cc from A) 效率高，用到了B表上cc列的索引； ​ select * from B where exists(select cc from A where cc=B.cc) 效率低，用到了A表上cc列的索引。 not in 和not exists 如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 in 与 =的区别 select name from student where name in (‘zhang’,’wang’,’li’,’zhao’); 与 select name from student where name=’zhang’ or name=’li’ or name=’wang’ or name=’zhao’]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 查询顺序]]></title>
    <url>%2F2018%2F01%2F03%2Fmysql-select%2F</url>
    <content type="text"><![CDATA[笛卡尔乘积 使用join执行笛卡尔乘积， 并且使用on来对笛卡尔乘积之前进行过滤操作, 这也称作显示的连接操作 使用from连接多个表执行笛卡尔乘积， 并且使用where进行过滤， 但是由于先执行笛卡尔乘积再过滤， 所以有效率问题， 这也称作隐式的连接操作 定义变化的表 可以使用(select @rank := 0) as ranker 来定义一张只有一个字段@rank的表ranker， 并且@rank字段也会作为用户自定义字段而存在， 所以相当于对@rank进行初始化。 笛卡尔乘积给一个表添加一个字段， 并且该字段自增1select p.id, p.name, @rank := @rank + 1 as rank from person as p join (select @rank := 0) as ranker; 查询步骤 查询操作是关系数据库中使用最为频繁的操作，也是构成其他SQL语句（如DELETE、UPDATE）的基础。查询处理的顺序如下: 123456789(7) SELECT (8) DISTINCT (1) FROM (3) JOIN (2) ON (4) WHERE (5) GROUP BY (6) HAVING (9) ORDER BY (10) LIMIT 1）FROM：对FROM子句中的所有的表一次左右执行笛卡儿积（Cartesian product），产生虚拟表VT1。 2，3）JOIN和ON应该是一个整体作为一个连接操作： 如果存在ON就先对VT1和JOIN后面的表进行过滤， 并且执行连接操作, 如果还有JOIN和ON继续接着进行连接， 如果此时又出现表则表示FROM后还有表进行笛卡尔乘积， 定义所有的这一连串操作产生VT3mysql不支持全连接， 并且它的连接与外连接是等价的， 跨连接与内连接是等价的 4）WHERE：对虚拟表VT3应用WHERE过滤条件，只有符合的记录才被插入虚拟表VT4中。此时数据还没有分组，所以不能在where中出现对统计的过滤 where: &lt;，&lt;=，=，!=或者&lt;&gt;，&gt; ，&gt;=， like (% 通配任意字符, _通配一个字符),exist(判空), beteeen(在某范围内)，any/som(集合中任意元素), all(集合中所有的元素), in(在某集合内), not ! 逻辑非, or || 逻辑或, and &amp;&amp; 逻辑与 5）GROUP BY：根据GROUP BY子句中的列，对VT4中的记录进行分组操作，产生VT5。在GROUP BY阶段，数据库认为两个NULL值是相等的，因此会将NULL值分到同一个分组中。 6）CUBE|ROLLUP：对表VT5进行CUBE或ROLLUP操作，产生表VT6。 7）HAVING：对虚拟表VT6应用HAVING过滤器，只有符合的记录才被插入虚拟表VT7中。count(expr) 会返回expr不为NULL的行数，count(1)、count(*)会返回包括NULL值在内的所有数量 8）SELECT：执行SELECT操作，选择指定的列，插入到虚拟表VT8中。 9）DISTINCT：去除重复数据，产生虚拟表VT9。 10）ORDER BY：将虚拟表VT9中的记录按照进行排序操作，产生虚拟表VT10。如果不指定排序，数据并非总是按照主键顺序进行排序的。NULL被视为最小值 11）LIMIT：取出指定行的记录，产生虚拟表VT11，并返回给查询用户。LIMIT n, m的效率是十分低的,一般可以通过在where条件中指定范围来优化 where id&gt; ? limit 10 12) UNION [ALL] (联合两个查询的结果), INTERSECT(交集)， EXCEPT(差集) 注意： 从上面的执行流程可以看出， 上面有很多过滤操作， 如： on where having distinct limit, 而每一个过滤操作都依赖前面的执行结果， 所以我们在前面的结果中如果能够尽量过滤掉不需要的结果那么对于后面的结果操作会简单很多。 所有的这些地方使用表的地方都可以使用子查询来建立新表， 如果数据量比较大尽量不适用子查询， 使用连接来优化。 对于要查询的结果来说， 从上面可以看到， 到第八步才开始进行真正的投影的， 所以不用担心查询出来结果信息。 可以看到limit是在最后执行的， 所以如果前期查询出大量结果， 再进行limit n,m可能会造成性能问题， 因为在limit之前以及进行了投影动作， 也就是所有的结果都以及查询出来了，如果此时我们只需要使用索引先搜索出满足limit的结果的主键id， 再通过主键id去查询结果， 那么此时会优化速度， 因为完全不用先查询出结果， 只需要最终去拿结果就可了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk使用教程]]></title>
    <url>%2F2018%2F01%2F03%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[awk的基本语法 awk -F&#39; &#39; &#39;BEGAIN {} {Pattern {Action} Pattern {Action} ...} END {}&#39; Pattern为一个判断条件， 如 $2&gt;3 &amp;&amp; $3 ~ /^Hel.*/， 基本比较符号和 ~/!~ 正则匹配 Action为一个动作， 如： print $1, $3 BEGAIN {} 只会在程序执行前执行一次 END {} 只会在程序执行后执行一次 {Pattern {Action} Pattern {Action} …} 会在每一行都会去循环执行 注意： Pattern也可以加上if作为一个判断， 其实awk只是简化了而已， 并且不用写中间的对每一行进行循环 awk存在内置常量， 不需要$来引用， NF字段的数量（列）， NR当前行数（awk读到第几行）， FILENAME正在处理的文件名字 字符串处理 ~/!~ 使用正则表达式匹配 substr( $2,1,length($2)-3) substr($2,length($2)-2) match(buffer,/[0-9]+.c/) split( “hello:world”, array, /[Pp]/) 分组统计​ 分组统计分组个数， 分组总数， 分组平均， 分组最小， 分组最大 按照协议进行分组统计 如： 游戏协议处理时间或者sql语句查询时间， 格式： 时间 协议/查询 时间 123456789102012 1 12012 2 22013 4 42014 3 02014 3 52015 2 22016 3 32017 2 12018 1 42018 4 1 1awk -F' ' '&#123;num[$2]+=1; sum[$2]+=$3; if(min[$2]=="")&#123;min[$2]=$3;max[$2]=$3;&#125; if(min[$2]&gt;$3)&#123;min[$2]=$3;&#125; if($3&gt;max[$2])&#123;max[$2]=$3;&#125;&#125; END &#123;for(i in num) print i, sum[i], num[i], sum[i]/num[i], min[i], max[i]&#125;' log.log 按照时间进行分组统计，格式如上 1awk -F' ' '&#123;num[$1]+=1; sum[$1]+=$3; if(min[$1]=="")&#123;min[$1]=$3;max[$1]=$3;&#125; if(min[$1]&gt;$3)&#123;min[$1]=$3;&#125; if($3&gt;max[$1])&#123;max[$1]=$3;&#125;&#125; END &#123;for(i in num) print i, sum[i], num[i], sum[i]/num[i], min[i], max[i]&#125;' log.log 注意： 从这里可以看到awk中对于未定义的对象使用 UNDEFINE == “” awk允许使用字符串最为数组下标， 所以这里协议可以是字符串协议， 并且使用数组前不需要进行声明 可以看到数组遍历非常简单， for(i in num) print num[i] awk 添加参数， 如， 在每一行前面添加文件名字： 12file_name=hello.logawk -F'|' '&#123;print file_name, $1&#125;' file_name=“$file_name” 添加换行1echo "a,b" | awk -F "," '&#123;for(i=1;i&lt;=NF;i++) &#123;print $i&#125;&#125;']]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell详解]]></title>
    <url>%2F2018%2F01%2F03%2Flinux-shell%2F</url>
    <content type="text"><![CDATA[shell属于弱类型语言, 其默认的变量类型为字符串类型shell 中符号使用12345678910$(),`` 命令替换用，变成一个完整的命令并执行$[],$(()) 简单的数学计算，支持+ - * / %$&#123;&#125;,$ 参数替换$n 位置参数$*,$@ 所有的参数，只有在引号时候才有差别$# 参数个数$? 上一个进程的退出状态$$ shell本身进程PID$! shell最后运行的后台Process的PID$- 使用Set命令设定的Flag一览 set命令12345678set -o 显示所有的set命令Flag表，显示开启或者关闭set +o 显示所有的set命令Flag表，相当与set命令Flag的备份模式显示set -e or -o errexit 命令执行失败时，shell会立即退出set -n or -o noexec 不执行命令，相当与语法检查set -u or -o unset 引用为定义的变量的时候，会推出程序并打印错误set -x or -o xtrace 跟踪代码执行traceset -o pipefail 默认情况下，一个管道的返回值是最后一个命令的返回值，比如cmda | cmdb | cmdc这个管道，返回值是由cmdc命令的返回值决定的。如果指定了pipefail选项，那么管道的返回值就会由最后一个失败的命令决定，意思就是有命令失败就会返回非0值。如果所有命令都成功，则返回成功。set -a or -o allexport 所有的变量都会export到环境变量中 说明： set命令主要使用来开关shell的扩展的，在set命令中，选项前面跟着-号表示开启这个选项，+表示关闭这个选项。shopt命令也是扩展shell的功能。 shell内建命令 可以使用 man bash-builtins 查看搜有的内建命令及其使用 shell命令类型 type命令判断一个命令是否是shell的内置命令 type -a command 打印出command的执行优先级, 因为shell命令可能被 alise 重新命名, function添加名称, 或者是外部命令取相同的名称, 所以必须要一个执行顺序, 顺序如： alias –&gt; function –&gt; builtin –&gt; program 后 which查看命令所在的位置, 如果内置命令不显示位置, whereis也可以用来显示位置, 默认过滤内部命令直接外部去找, command 命令可以查看命令的具体位置，如： command -v java shell中有一些命令可能与外部命令有相同的名称, 默认shell会调用内置的命令, 因为这可以避免fork/exec shell执行某个命令后, 如果是内部命令(其实就像是内置的方法一样), shell程序直接调用自己的一段方法就可以了, 如果是外部命令, 那么shell首先fork出一个进程, 再使用exec家族中的方法来执行外部命令, exec家族命令中可以选择是否将父进程中的环境变量传递给子进程. shell中不同的执行方式 sh/bash: 表示另起一个shell， 也就是首先fork一个子进程, 再在子进程中exec子进程代码后(父进程停止)， 再运行父进程的代码 exec: 不会fork出子进程， 而是使用新的命令代替当前进程中所有的代码开始执行， 也就是exec后面的代码都是不会执行的, 注意exec后面是完整的执行命令， 而不仅仅只是一个执行文件(exec bash file.sh) source/.: 不会fork出子进程，直接停止当前进程，运行source后面的命令，当source后面的命令执行完成之后，再启动父进程，继续执行 注意： 用export会把父进程中的变量向子进程中继承，但是反过来却不行，在子进程中，不管环境如果改变，均不会影响父进程，所以如果想要环境变量可以传递不能使用sh/bash 同一个进程中变量的传递是可以直接传递的，不需要使用export成环境变量， 也就是说使用 source/. 和 exec 可以直接从上下文中获取，也就是source/.和exec中执行的代码可以获取前面的变量， 而source/.和exec后面的代码也可以获取到source/.和exec中定义的变量 命令返回值 所有的命令输出或者echo命令的输出, 都可以赋值给一个具体的变量, 这样非常有用 a=[ 3 -ge 2 ] &amp;&amp; echo &quot;large&quot; || echo &quot;small&quot; 或者使用$(command) 测试命令 test/[ 命令的与或非 与 bash命令中的 &amp;&amp; || !的区别 test 1 &gt; 2 -a 2 &lt; 3 test 1 &gt; 2 -o 2 &lt; 3 test !1 &gt; 2 test 1 &gt; 2 &amp;&amp; test 2 &lt; 3 test 1 &gt; 2 || test 2 &lt; 3 ! test 1 &gt; 2 从上面命令可以看到test命令提供的与或非是在test命令中实现的, 也就是test遇到这样的命令它会默认将其解析成两个命令执行, 而 &amp;&amp; || !则是将其当两个命令来执行的, 先执行前面的命令, 再看是否需要执行后面的命令, 所以这样就可以拼凑出, 三元表达式如:[ 3 -ge 2 ] &amp;&amp; echo “large” || echo “small shell中不存在布尔值的概论, 即使执行test或false命令, 也仅仅只是返回给程序执行结果一个标示, 标示命令是否执行成功, $?, 所以对于shell而言, 所谓的bool值就是命令执行的结果是否成功, $?=0表示程序执行成功, 而非零则表示执行失败 shell的条件判断 [ -n “$1” ] &amp;&amp; echo “exist” || echo “none” shell 语言 shell中使用#表示注释， 首行 #!/bin/bash 或者 #!/usr/bin/env bash shell中使用\表示转义， 表示后面字符使用其真实含义， 如：用在行为表示另外启动一行， 也就是\n本身的含义保留， 所以换行才可以使用\ [ 实际上是一个命令， [ -d /etc ] 实际上-d /etc ]都是这个测试命令的参数， 并且它要求最后一个参数必须是 ], 如果测试结果为真，则该命令的Exit Status为0，如果测试结果为假，则命令的Exit Status为1（注意与C语言的逻辑表示正好相反）， 并且可以使用man [ 开查看帮助信息, 它与test命令没有任何区别 通过$?获取退出码 算数计算 $(()), $(())中只能用+-*/和()运算符，并且只能做整数运算。 for循环可以直接在命令行输入： for i in $(jps | grep -v ‘Jps’| awk ‘{print $1}’); do echo $i; done for i in $(ls); do echo $i; done for i in `ls`; do echo $i; done for i in hello world welcome; do echo $i; done for i in *; do echo $i; done 表示将当前文件夹下所有的文件都打印出来 注意： in后面的参数是以空格和分行符来确定的， 并且如果参数为一个路径， 或者路径的正则表达式形式， 那么模式是使用 ls 参数 的形式来进行操作的。 in后面的参数都是以字符串的形式表示的， 所以对于bash而言没有数据类型的说法， 一切皆为字符串 每次遍历完成之后， 其实变量 i 并没有销毁，此时变量i保存着最后循环值， 这也表明shell script并非使用传统的局部变量的编译形式。可以手动进行取消 unset i while/until 循环 while [[ -f file.txt ]]; do … done until [[ ! -f file.txt ]]; do … done while表示条件满足就执行， until表示条件满足就退出 case语句 在shell script中可以使用tip return 返回当前脚本或者代码块 exit 退出 printf 格式化打印 shell中的变量 方法局部变量： 使用local 定义 局部变量： 只能在当前脚本中使用 环境变量： 所有的脚本中都可以使用 shell变量： shell中特定的变量 字符串， 字符串的定义比较宽松， 单引号、双引号、不用引号 单引号： 单引号中的数据会原封输出， 单引号中变量无效，且不能再存在单引号，不存在转义 双引号： 双引号中可以存在变量， 且可以转义 常用命令 命令判断： command command -v java 选取： cut cut /etc/passwd -d “:” -f 3,5 grep ps -ef | grep -Ei “mysql|syslog” | grep -v ‘grep’ | grep -B 1 “root” awk less /etc/passwd | awk -F’:’ ‘BEGIN {print “begin”}{print $3} END {print “after”}’ 排序统计： sort less /etc/passwd | cut -d “:” -f 3,7| sort -t : -k 1 -n -r wc less /etc/passwd | wc -l -w -c uniq less /etc/passwd | cut -d : -f 7 | sort | uniq -c -u/d 多重定向： tee less /etc/passwd | tee hello world 将输出重定向到 中端、hello和world的文件当中 字符转换命令： tr 删除、替换、压缩 sed paste paste -d : /etc/passwd /etc/group 案行合并文件 join 按文件相关性合并文件， 类似excel中的lookup函数 字符显示： printf 文件查看： cat(tac, rev)、 more、 less、 head、 tail、 nl、grep、hexdump、 od、 xxd 文件路径： dirname、basename、rename 文件定位： locate 基于数据库， 需要手动维护数据库， 快速 find 时实查找， 相对慢, 可以查找并删除 文件内容搜索 grep 强大的正则表达式 ag 比grep更加快速 文件创建 mkdir mktemp -p 指定创建父目录，-d 创建文件夹，-u不创建文件只是生成随机字符串 数学操作 seq 生成序列 RANDOM 环境变量, 产生从 0 到 32767 的随机数 多个命令同时执行 所有的命令必须执行成功, 第一个命令执行完成后才会执行下一个命令， 如果中间命令执行出错则会打断后续命令执行， &amp;&amp; ​ 如： less … &gt; a.txt &amp;&amp; less … &gt; b.txt &amp;&amp; less a.txt b.txt | sort | uniq -u 所有的命令至少有一个执行成功, 前面的命令执行错误不会打断命令继续执行， || 如： hello || cat a.txt, 此时第一条命令打印错误信息， 第二条命令执行完成 好用的的命令工具 httpie ag]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基本信息查看]]></title>
    <url>%2F2018%2F01%2F03%2Flinux-info%2F</url>
    <content type="text"><![CDATA[系统​ uname -a 或者 cat /proc/version # 查看内核/操作系统/CPU信息 ​ head -n 1 /etc/issue # 查看操作系统版本 ​ cat /etc/redhat-release # 查看操作系统版本 ​ cat /etc/os-release #查看操作系统发行版本 ​ cat /proc/cpuinfo # 查看CPU信息 ​ cat /proc/cpuinfo | grep ‘physical id’ #查看cpu个数 ​ cat /proc/cpuinfo | grep ‘core id’ #查看cpu核心个数 ​ cat /proc/cpuinfo | grep ‘processor’ #查看cpu线程个数 ​ hostname # 查看计算机名 ​ lspci -tv # 列出所有PCI设备 ​ lsusb -tv # 列出所有USB设备 ​ lsmod # 列出加载的内核模块 ​ env # 查看环境变量 资源​ lscpu # 查看cpu信息 ​ free -m # 查看内存使用量和交换区使用量 ​ df -h/TH # 查看各分区使用情况 ​ du -sh &lt;目录名&gt; # 查看指定目录的大小 ​ grep MemTotal /proc/meminfo # 查看内存总量 ​ grep MemFree /proc/meminfo # 查看空闲内存量 ​ uptime # 查看系统运行时间、用户数、负载 ​ cat /proc/loadavg # 查看系统负载 磁盘​ mount | column -t # 查看挂接的分区状态 ​ fdisk -l # 查看所有分区 ​ swapon -s # 查看所有交换分区 ​ hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) ​ dmesg | grep IDE # 查看启动时IDE设备检测状况 ​ lsblk ​ blkid #查看磁盘uuid 网络​ ifconfig # 查看所有网络接口的属性 ​ iptables -L # 查看防火墙设置 ​ route -n # 查看路由表 ​ netstat -lntp # 查看所有监听端口 ​ netstat -antp # 查看所有已经建立的连接 ​ netstat -s # 查看网络统计信息 ​ rfkill list # 查看无线网卡和蓝牙情况 ​ nethogs # 查看网络使用情况 ​ Iftop 测试网络带宽​ iperf -s #开启服务端 ​ iperf -c 192.168.0.11 #客户端连接服务端，并进行网络测试 进程​ ps -ef # 查看所有进程 ​ top # 实时显示进程状态 ​ jobs、 fg、 bg、 ctrl+z/c、 启动 &amp; 用户​ w # 查看活动用户 ​ id &lt;用户名&gt; # 查看指定用户信息 ​ last # 查看用户登录日志 ​ tty # 查看所在终端 ​ cut -d: -f1 /etc/passwd # 查看系统所有用户 ​ cut -d: -f1 /etc/group # 查看系统所有组 ​ crontab -l # 查看当前用户的计划任务 ​ whoami 或者 who am i # 查看登陆信息 清空访问痕迹​ history -c ​ echo &gt; /var/spool/mail/root​ echo &gt; /var/log/wtmp​ echo &gt; /var/log/secure​ echo &gt; /root/.bash_history]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 用户]]></title>
    <url>%2F2018%2F01%2F03%2Flinux-user%2F</url>
    <content type="text"><![CDATA[shell作用​ 用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。 linux中用户概念 linxu中存在用户的概念， 系统中存在三种用户： root用户： id=0 登陆shell为/bin/bash(表示可以登录) 系统用户： id=(1-1000) 没有登陆shell（/usr/sbin/nologin, 不需要登陆） 普通用户： id=(1000-65534) 登陆shell为/bin/bash(可以登录) 系统中的所有的文件都存在一个所属用户和所属组的概念， 并且存在所属用户、所属组、附属组的权限， 并且每个用户都存在一个用户名（用户id）， 一个所属组（组id）， 0-31个附属组（附属组id）， 并且每个进程都需要以一个用户的身份来运行， 表示当前用户是否存在访问系统资源的权限 查看用户信息 id显示当前用户信息 ​ uid=1000(feng) 用户id（用户名） ​ gid=1000(feng) 用户组id（用户组名） ​ groups=1000(feng) 用户附属组id（附属组名） 可以存在多个附属组 passwd修改用户密码 w/who/whoami who -r 查看运行级别 基本文件 /etc/passwd 用户相关信息 ​ 用户名:密码:uid:gid:描述信息:家目录:登陆shell 123456789root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologin.....mail:x:8:8:mail:/var/mail:/usr/sbin/nologinnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologinuuidd:x:108:112::/run/uuidd:/bin/falsefeng:x:1000:1000:feng,,,:/home/feng:/bin/bashsshd:x:110:65534::/var/run/sshd:/usr/sbin/nologin /etc/shadow 用户密码 /etc/group 用户组信息 创建用户 useradd 在/etc/passwd中添加用户信息 为用户创建一个家目录/home/uname 将/etc/skel中的文件复制到用户的家目录当中(所以如果希望每次创建新用户的相同动作) 建立一个与用户名相同的组， 新建用户默认属于这个组 如果使用passwd命令创建密码， 则将密码保存在/etc/shadow当中 ​ 可以使用以下参数： ​ -d 家目录 ​ -s 登陆shell ​ -u userid ​ -g 主组 ​ -G 附属组（多个， 用“，”分割） 用户其它操作 usermod 修改用户 userdel 删除用户 -r 家目录也一起删除 用户组相关操作（组和用户是独立的概念）​ groupadd 添加组 ​ groupmod 修改组 ​ groupdel 删除组 组的规划案例： 组 用户 training feng、xiang market alice、bob manage steve、john 创建组：​ groupadd training ​ groupadd market ​ groupadd manage 创建用户：​ useradd -G training feng -p 123456 ​ useradd -G training xiang -p 123456 ​ useradd -G market alice -p 123456 ​ useradd -G market bob -p 123456 ​ useradd -G manage steve -p 123456 ​ useradd -G manage john -p 123456 权限(文件夹必须用户可执行权限， 不然无法查看文件夹中的内容)​ 文件类型|文件所属组|其它 链接数 所属用户 所属组 大小 最后修改时间 文件名 ​ drwxr-xr-x 6 feng feng 4096 Aug 4 15:22 ./ ​ drwxr-xr-x 3 root root 4096 Jan 2 2017 ../ ​ -rw——- 1 feng feng 15478 Aug 4 14:34 .bash_history ​ 所有的权限命令都是-R来递归修改目录下的权限 更改文件所属用户 ​ chown feng Hello.java 更改文件所属组 ​ chgrp feng Hello.java 更改文件权限 ​ chmod 模式 文件 ​ 模式： ​ u、g、o、a 表示用户、组、其它、所有 ​ +、-表示添加或者减少 ​ r、w、x代表三种权限 ​ 777用三个数字来表示三组权限 ​ 数字权限： r = 2^2 = 4, w = 2^1 = 2 , x = 2^0 = 1 ​ 如：rw = 4+2 = 6 ​ 实例： 1234chmod u+rw Hello.javachmod g-x hellochmod go+r Hello.javachmod a-x Hello.class 登陆中断的umask值， 表示当前登录用户创建文件的权限​ 注意：对于普通用户的默认umask值为0002， 管理员用户的默认umask值为0002 ​ 前面1位：特殊权限， 有三个（suid、sgid、sticky） ​ 后面3位： ugo模型的基本权限 ​ 特殊权限： 权限 对文件的影响 对目录的影响 suid 以文件的所属用户身份执行，而非执行文件的用户 无 sgid 以文件所属组身份执行 在该目录中创建的文件的所属组与该目录的所属组相同 sticky 无 对目录拥有些权限的用户仅能删除拥有文件，无法删除其它 suid（通常都是设置给可执行文件的）： 当操作系统需要暴露出一个命令可以给普通用户使用， 但是这个命令需要root用户的权限， 此时将此命令的O权限设置r_x给普通用户， 并添加器suid权限， 此时就满足需求。 12feng@ubuntu:~$ ll /usr/bin/passwd -rwsr-xr-x 1 root root 54256 May 17 07:37 /usr/bin/passwd* 可以看出此时命令的U权限位的x权限设置成了s， 并且O权限位为r_x， 表示：其它用户可以执行当前命令， 但是此时我们知道密码保存在/etc/shadow文件当中， 但是此时西面可以看出shadow文件必须只有root用户才可以访问的， 所以此时passwd命令必须让其以root用户的身份运行。 feng@ubuntu:~$ ll /etc/shadow -rw-r—– 1 root shadow 998 Aug 4 17:33 /etc/shadow mysql程序虽然是root用户启动， 但是此时实际运行的用户为mysql用户 sticky： drwxrwxrwt 8 root root 4096 Aug 4 18:20 tmp/ ​ 可以看到对于O权限位的x权限变成了t， 对于文件夹而言必须要x权限才能读取， 此时变成了 ​ t权限位， 表示目录下的文件只有当前用户才能删除用户的文件， 其它的文件无法删除 sgid： 一般使用在多人团队的项目当中（系统中使用很少）， 为文件夹设置一个sgid权限， 并设置 ​ 所属组都存在访问操作权限， 这样更加方便管理 文件夹有一个点（.），表示操作系统启用selinux，但是selinux关闭后原先的文件存在点好并不会取消​]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gopass安装]]></title>
    <url>%2F2018%2F01%2F03%2Flinux-gopass%2F</url>
    <content type="text"><![CDATA[gopass各组件说明 gopass 用于密码管理的工具， 是pass的go语言版本， 提供了多人的密码管理命令行接口 gnupg 用于真正的加密， gnupg用于生成非对称秘钥， 使用公钥进行机密， 私钥进行解密 git 用于将gopass需要管理的密码（被gnupg生产的公钥加密过的密码）上传到git中便于管理 http://www.voidcn.com/article/p-gngvxvlm-nx.html 安装gopass 安装基本工具 apt-get install gnupg git rng-tools 说明： gnupg通过非对称加密算法对密码进行加密 git用于将加密后的密码上传到git远程仓库中 初始化GPG key pair gpg --gen-key 安装gopass 12wget -q -O- https://api.bintray.com/orgs/gopasspw/keys/gpg/public.key | sudo apt-key add -echo "deb https://dl.bintray.com/gopasspw/gopass trusty main" | sudo tee /etc/apt/sources.list.d/gopass.list 12sudo apt-get updatesudo apt-get install gopass 添加自动完成脚本到.zshrc中 12source &lt;(gopass completion zsh | head -n -1 | tail -n +2)compdef _gopass gopass 初始化gopass gopass init 安装passmenu 安装dmemu sudo apt install dmenu 安装passmenu脚本到path中 12345678910111213141516171819202122232425#!/usr/bin/env bashshopt -s nullglob globstartypeit=0if [[ $1 == "--type" ]]; then typeit=1 shiftfiprefix=$&#123;PASSWORD_STORE_DIR-~/.password-store&#125;password_files=( "$prefix"/**/*.gpg )password_files=( "$&#123;password_files[@]#"$prefix"/&#125;" )password_files=( "$&#123;password_files[@]%.gpg&#125;" )password=$(printf '%s\n' "$&#123;password_files[@]&#125;" | dmenu "$@")[[ -n $password ]] || exitif [[ $typeit -eq 0 ]]; then pass show -c "$password" 2&gt;/dev/nullelse pass show "$password" | &#123; IFS= read -r pass; printf %s "$pass"; &#125; | xdotool type --clearmodifiers --file -fi 上面还存在另外一种实现方式 1234# Simply copy the selected password to the clipboardgopass ls --flat | dmenu | xargs --no-run-if-empty gopass show -c# First pipe the selected name to gopass, encrypt it and type the password with xdotool.gopass ls --flat | dmenu | xargs --no-run-if-empty gopass show -f | head -n 1 | xdotool type --clearmodifiers --file - 添加系统快捷键 路径： System Settings --&gt; keyboard --&gt; Shortcuts --&gt; Custom Shortcuts 添加： 上面脚本的路径 配置git远程仓库 在github中创建个人私有仓库 讲gopass的git仓库添加到远程仓库当中 git remote add origin https://github.com/xxxx/gopass.git gopass sync 配置gpg 创建回收key， 用于在私钥发送泄露或者忘记私钥密码时回收公钥 gpg --armor --output revoke-key.txt --gen-revoke keyname 导出公钥和私钥 gpg --armor --output public-key.txt --export keyname gpg --armor --output private-key.txt --export-secret-keys keyname 备份自己的gpg秘钥对到google云盘当中 说明： gpg的秘钥和gopass保护的密码都以及保存起来了， 用于以后的密码找回 直接查看gopass中密码 由于gopass中的密码以及被上面生成的密钥对的公钥加密过， 所以只需要通过上面导出的私钥就可以解密出正真的密码明文 gpg -o hello.txt -d ~/.password-store/storm.gpg 注意：此时需要输入我们生成密钥时候的密钥密码， 所以从这里可以看到即使别人拿到我们的密钥如果没有私钥密码那么他们照样是无法进行解密的 恢复gopass的环境 git clone 密码库到本地 git clone https://github.com/xxxx/gopass.git ~/.password-store gpg 导入公钥和私钥 12gpg --import ~/public-key.txt gpg --allow-secret-key-import --import ~/private-key.txt gopass配置使用公钥用户进行加密 gopass init 总结 从上面的说明可以看出github中的密码和google的密码也是非常重要的， 但是通常情况下绑定了手机可以直接找回， 所以不用担心， 直接使用gopass自动生成的密码即可 私钥是一定不能忘记的， 所以如果这三个密码如果没有全部丢失那么通常情况下自己的gopass密码管理就是没有问题的。 但是存在一个问题， 那就是机器丢失的问题 团队协作 创建gopass git仓库，使用github创建 1gopass --yes setup --remote git@github.com:example/pass.git --alias example --create --name &quot;John Doe&quot; --email &quot;john.doe@example.com&quot; 将团队其它成员的公钥导入到gopass仓库当中 123456#团队成员导出公钥gpg -a --export logan@mail.com &gt; logan.pub.asc#导入公钥到创建gopass仓库的gpg当中gpg --import &lt; logan.pub.asc#将用户添加到gopass仓库当中gopass recipients add logan@mail.me/name 团队其它成员clone仓库 1gopass --yes setup --remote github.com/example/pass.git --alias example --name &quot;logan&quot; --email &quot;logan@mail.com &quot; 手机端也可以使用pass工具]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 服务器监控]]></title>
    <url>%2F2018%2F01%2F02%2Flinux-server-monitor%2F</url>
    <content type="text"><![CDATA[top top -H -p free free -m vmstat vmstat 1 10 pmstat pmstat -P -ALL 2 3 pidstat pidstat -p pid 1 5 -l/d/r/w/t iostat iostat -x 1 3 ifstat dstat crontab 定时任务， 注意： 添加PATH，或者直接执行source /etc/profile， 添加文件权限，写定时任务时输出错误日志，脚本中所有的命令和文件必须在path中找到，不然就需要全路径，如果有问题查看日志/var/log/cron netstat 显示服务器监听状态， 和正在被其它机器连接以及连接到其它的机器的连接 -a -t -u： 表示查看正在被其他机器连接或者连接到其它机器， -t tcp, -u udp, -a tcp/udp -l: 表示服务器监听状态 -n 拒绝显示别名，能显示数字的全部转化成数字。 -p 显示建立相关链接的程序名和端口号 -e 显示扩展信息 常用案例： 查看某个端口的连接数： netstat -an | grep 9991 | wc -l 查看某个程序监听的端口 netstat -lp | grep mysql 查看某个程序对外连接数在ip上的分布： netstat -an | grep &quot;192.168.1.15:22&quot; |awk &#39;{print $5}&#39;|awk -F: &#39;{print $1}&#39;|sort|uniq -c|sort -nr|head -20 192.168.1.15:22 服务端监听的程序 netstat -c实时监控连接的建立 ps -ef | grep name 可以先通过netstat查看到程序进程id， 再通过ps来查看相关信息 lsof 列出打开的文件(list open file), 这是个非常重要的命令， 非常方便我们查看文件相关，我们知道在linux当中所有的硬件设备都是以文件的形式存在， 所以我们可以通过查看文件的信息查看我们的硬件信息， 如： 网络设备， 磁盘等等 常用参数： lsof -p pid： 列出指定进程号所打开的文件 ​ lsof -p 2 列出程序打开的文件 lsof -i 条件： 列出符合条件的进程。 （4， 6， 协议， :端口， @ip） ​ sudo lsof -i 4 列出ipv4的进行 ​ sudo lsof -i 6:2181 列出ipv4并且端口为2181的进程 lsof +d/D 目录： 列出目录下被打开的文件/递归 ​ lsof -d . 当前目录 ​ lsof -D . 递归显示 ​ 可以指定多个条件，但默认是OR关系的，如果需要AND关系，必须传入-a参数，比如查看22端口并且使用Ipv6连接的进程： ​ sudo lsof -c sshd -i 6 -a -i :22]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark注意点]]></title>
    <url>%2F2018%2F01%2F02%2Fhadoop-spark%2F</url>
    <content type="text"><![CDATA[spark transform 和 action 动作中的闭包序列化，由于在闭包中，spark只是负责把闭包进行序列化成对象，也就是说从Driver到Executor中，spark拿到的都是一个个序列化对象，并通过rdd的组合对这些序列化对象进行调用，那么对于闭包来说，spark没有任何的特例，它就是完全使用scala语言闭包的语义，也就是说在创建闭包的时候，对于闭包的可见性对象， 它会将其打包成类的属性， 并对闭包进行序列化，那么通常来说它仅仅只是会打包闭包可见性并且需要的对象，对于闭包中的可见性对象如果它的方法依赖与另外一个对象（并且这个对象，不是这个可见对象的属性，那么spark是不会序列化这个对象的，如： 123456789101112131415//业务调用import CacheHelper.cachedataset.filter&#123;log =&gt; cache(log .....)&#125;// rdd闭包中使用这个单例中的方法， object CacheHelper&#123;cache(log ....) &#123;Sington.name&#125;&#125;// 程序启动的时候， 初始化这个单例对象name=&quot;init name&quot;class Sington(name: String)object Sington &#123;private var instance = new Sington(&quot;default name&quot;)val getInstance = instancedef init(name: String) instance.name = name&#125; 注意： 从上面的分析可以看到， 闭包中的调用的方法依赖一个Sington的全局对象, 对于普通的本地运行master = local[n]此时的Sington的name为”init name”, 对于集群中运行的时候master = yarn此时Sington的那么为”default name” 分析： 现在通过上面的规则可以非常明显的看出此时Executor中的Sington对象没有被序列化， 为默认值， 所以对于spark而言要非常注意这种序列化问题， 其实规则非常简单， 但是通常会由于对象图谱而造成混乱， 所以通常必须要非常清晰的全局对象。 处理： 更改这个错误也非常简单， 只需要在dataset.filter中直接传入这个Sington对象， 或者只传入它需要的参数就可以， 也就是说将这个单例对象暴露到闭包的可见范围之内， 这样闭包就会被序列化。 缺点： 当然这里非常明显的代码结构发生了变化， 但这也是无赖之举， 也就是说我们一定要缩小闭包的可见范围。 1234567891011121314import CacheHelper.cachedataset.filter&#123;log =&gt; cache(log ...., Singtong.getInstance.name)&#125;//将这个Sington暴露到闭包可见范围之内， rdd闭包中使用这个单例中的方法， object CacheHelper&#123;cache(log ...., name) &#123;Sington.name&#125;&#125;// 程序启动的时候， 初始化这个单例对象name=&quot;init name&quot;class Sington(name: String)object Sington &#123;private var instance = new Sington(&quot;default name&quot;)val getInstance = instancedef init(name: String) instance.name = name&#125; pyspark的原理 使用pyspark启动时，首先会进入python 12export PYTHONSTARTUP=&quot;$&#123;SPARK_HOME&#125;/python/pyspark/shell.py&quot;exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit pyspark-shell-main --name &quot;PySparkShell&quot; &quot;$@&quot; 说明： PYTHONSTARTUP 是 python 的一个环境变量，当启动python时候，如果这个环境变量存在，那么python启动的时候会调用这个python脚本。而这里的 shell.py 中再次调用 spark-submit 进行启动spark的 PythonGatewayServer ，这样python使用py4j就可以和这个和spark的jvm进行通讯了。接着 shell.py 中创建SparkSession等对象。 pyspark启动后，通常会使用SparkSession对象创建RDD，并最终对RDD进行转换和操作，那么对于RDD的各种python代码的操作是如何进行转换到spark当中的呢？ 首先对于pyspark中创建的对象与jvm中的对象是一一对应的 其次当使用python闭包时候，python会使用pickle对闭包进行序列化，再当做参数传递给spark spark在Executor中运行RDD中的compute方法时候，当然是不能运行python代码的，而是在Executor启动一个python虚拟机将RDD的数据传到python虚拟机中，再将结果传回到spark中，spark再将最终计算的结果驱动python虚拟机当中 spark 日志查看 程序还在运行的时候，到想要查看容器对应的服务器中，运行下面命令查看日志存放的位置 1ps -ef |grep spark.yarn.app.container.log 程序已经停止运行，此时日志已经被yarn进行聚合到HDFS当中，通常已经压缩，所以一般无法直接查看，只能借助 yarn longs 查看 1yarn logs -applicationId application_1585056926959_0002 -containerId container_e14_1585056926959_0002_01_000006 -log_files stdout -size -102400 &gt; hello.log 说明： size后面使用负数表示查看最后的102400个字节]]></content>
      <categories>
        <category>hadoop</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hortonwork的使用以及常见问题]]></title>
    <url>%2F2018%2F01%2F02%2Fhadoop-hortonwork%2F</url>
    <content type="text"><![CDATA[配置本地源出错时 方法一：需要选择Use RedHat Satellite/Spacewalk， 并且配置源的名字，注意修改每一个repo的name字段 方法二：如果实在不行就直接在管理员中(version)，配置本地源即可 Hostname 必须不能配置出错，hostname 和 hostname -f 必须有相同的输出，/etc/hosts 配置ip必须相同 123172.19.36.14 iot0.dhls.com iot0172.19.36.11 iot1.dhls.com iot1172.19.36.18 iot2.dhls.com iot2 注意：完整域名必须在前面 浏览器配置kerberos访问 firefox中输入 about:config，搜索并设置 1network.negotiate-auth.trusted-uris = .domain.com chrome 直接添加命令行参数 1google-chrome --auth-server-whitelist=&quot;*.domain.com&quot; --auth-negotiate-delegate-whitelist=&quot;*.domain.com&quot; yarn 队列有自己的提交权限策略， 需要配置才能提交任务 yarn.scheduler.capacity.root.acl_administer_queue=yarn,spark,hive,admin,zeppelin spark的历史日志查看也需要进行权限设置， 通常关闭 spark.history.ui.acls.enable 因为日志通常记录在hdfs中， 所以还必须拥有hdfs的文件访问权限 ranger 页面无法登录， 如果显示无法访问DB这是因为使用Mysql的时候必须明确指定是否使用SSL， 所以必须在配置连接的时候使用jdbc:mysql://hdp1.ilivoo.com:3306?useSSL=false， 其它的组建都可以加上，包括ambari 注意： 任何的报错都应该去查看日志才能精确定位错误，不用假想和凭经验断定。 ambari-server 开启security，删除或添加服务的时候需要 更改admin/admin@TEPIA.COM的密码 12kadmin.localchange_password admin/admin 开启ambari-server的security 12345ambari-server setup-security选择[2] Encrypt passwords stored in ambari.properties file.选择[Y]keytool -list -keystore /var/lib/ambari-server/keys/credentials.jceks -storetype JCEKSambari-server restart 添加credential 123456添加curl -H &quot;X-Requested-By:ambari&quot; -u admin:ambari_passwd -X POST -d &apos;&#123; &quot;Credential&quot; : &#123; &quot;principal&quot; : &quot;admin/admin@TEPIA.COM&quot;, &quot;key&quot; : &quot;admin_princ_passwd&quot;, &quot;type&quot; : &quot;persisted&quot; &#125; &#125;&apos; http://hdp1:8080/api/v1/clusters/tepia/credentials/kdc.admin.credential查看curl -H &quot;X-Requested-By:ambari&quot; -u admin:ambari_passwd -X GET http://hdp1:8080/api/v1/clusters/tepia/credentials/kdc.admin.credential删除curl -H &quot;X-Requested-By:ambari&quot; -u admin:ambari_passwd -X DELETE http://hdp1:8080/api/v1/clusters/tepia/credentials/kdc.admin.credential ambari 可以运行在非root用户下， 通常来说这样会更安全些，但是需要一些配置，参考，但是建议还是运行在root用户下，这样更加简单，所有的安全问题也可以参考上面。 更改权限 chown -R ambari /var/run/ambari-server NameNode UI不能访问, 这是因为服务器存在多个网卡, 默认NameNode默认只是绑定到集群指定的网卡, 并没有绑定到所有的网卡, 所以如果使用了openvpn后, 实际上50070并没有绑定到openvpn指定的网卡, 所以就会导致无法openvpn的网卡无法访问了, 通过指定 hdfs-site.xml 绑定所有网卡 1234dfs.namenode.http-bind-host=0.0.0.0dfs.namenode.https-bind-host=0.0.0.0dfs.namenode.rpc-bind-host=0.0.0.0dfs.namenode.servicerpc-bind-host=0.0.0.0 kafka日志目录必须为空, 对于单独挂载的目录来说通常下来存在 lost+found , 所以必须再加一层目录. kafka的 zookeeper.connect 应该添加根目录, 这样更加便于管理, 如: 1zookeeper.connect=cmsw2.tepia.com:2181,cmsw1.tepia.com:2181/kafka kafka多网卡绑定, 如果仅仅只是绑定某个ip而没有绑定openvpn网卡, 那么同样无法访问, 所以需要配置为 listeners=PLAINTEXT://:6667 kerberos kafka需要更改配置为 listeners = SASL_PLAINTEXT://:6667 hbase 双网卡绑定问题, 在hbase-site.xml中配置 12hbase.master.ipc.address 0.0.0.0hbase.regionserver.ipc.address 0.0.0.0 更改hadoop运行用户, 通常情况下线上用户和线下用户不是同一个用户,如果 线下调试线上spark程序,需要指定本地为线上用户, 则通过指定环境变量来指定运行用户 1export HADOOP_USER_NAME=tepia yarn timeline 两种配置后端存储模式，第一种嵌入式的hbase运行，第二种集群的hbase（推荐），配置如下 配置并重启yarn 123456use_external_hbase = true hbase.zookeeper.quorum = hdp1.tepia.com,hdp2.tepia.com,hdp3.tepia.com hbase.zookeeper.property.clientPort = 2181 zookeeper.znode.parent = /hbase-secure yarn.timeline-service.hbase.configuration.file= file:///etc/hbase/conf/hbase-site.xml 说明：最后的一个配置 yarn.timeline-service.hbase.configuration.file ，hdp的官网并没有说明，如果没有指定配置hbase配置文件，TimelineReader无法连接到hbase。但是yarn官网有提及，从这里也充分的可以感觉到很多答案还得到项目的官网中寻求答案。 使用hbase用户创建yarn需要的schema 12export HBASE_CLASSPATH_PREFIX=/usr/hdp/3.1.0.0-78/hadoop-yarn/timelineservice/*/usr/hdp/3.1.0.0-78/hbase/bin/hbase org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator -Dhbase.client.retries.number=35 -create -s 为新添加的表(prod.*)增加权限, 使用基本方式或者ranger 12grant &apos;yarn&apos;, &apos;RWXCA&apos;grant &apos;yarn-ats&apos;, &apos;RWXCA&apos; 关闭yarn-ats本身开启的hbase 123su - yarn-atsjpskill master regionserver 尝试使用 yarn 的service服务，可以参考 ats-hbase 这个服务。 ams collector 后端写入模式，如果改为分布式的，只是将hbase的存储放到hdfs当中，而本地还是开启了hbase应用。参考 12345https://docs.cloudera.com/HDPDocuments/Ambari-2.7.4.0/using-ambari-core-services/content/amb_customize_the_ams_collector_mode.htmltimeline.metrics.service.operation.mode = distributedhbase.rootdir = /apps/ams/metricshdfs dfs -mkdir -p /apps/ams/metricshdfs dfs -chown -R ams:hadoop /apps/ams/metrics 注意：设置为distributed，默认会有很多配置发送变化 ams 开启hbase tables metrics 收集 进入目录 1cd var/lib/ambari-server/resources/stacks/HDP/#version/services/HBASE/package/templates 编辑下列文件 12hadoop-metrics2-hbase.properties-GANGLIA-MASTER.j2hadoop-metrics2-hbase.properties-GANGLIA-RS.j2 删除或注释下列类容 12*.source.filter.class=org.apache.hadoop.metrics2.filter.RegexFilterhbase.*.source.filter.exclude=.*(Regions|Users|Tables).* 重启 amber-server，再重启hbase 设置grafana https 连接，参考 phoenix 包冲突问题， 由于项目中现在基本上使用logback，但是phoenix中使用log4j并且使用slf4j进行进行桥接，所以解决phoenix包冲突前先应该去掉slf4j和log4j，再更具具体情形进行对比。 hdp3.1 中 spark stream 内存占用太高bug修复，通常情况下修复spark bug需要非常仔细。 找准spark 对应的源代码（通过maven源代码） 找准scala版本（查看spark中的pom文件获取） 创建项目拷贝对应源代码， 修改源代码。 获取spark集群中的jar包， 并且备份jar包 解压jar包（jar -xf ），并拷贝编译的新的class文件，并重新打包(jar cvfM) 上传新的jar包， 并重启history server yarn 本地缓存导致jar包冲突 查看yarn本地缓存位置 yarn.nodemanager.local-dirs 删除 rm -rf filecache/ usercache/ 重启yarn 更改spark history日志自动删除，在 Custom spark-defaults 中添加 123spark.history.fs.cleaner.enabled=truespark.history.fs.cleaner.interval=1dspark.history.fs.cleaner.maxAge=7d 手动最近30天的脚本 123456789101112131415161718192021222324252627282930#!/bin/bash# delete 30 day's before spark history logs ########################start ########################days=30day_01=`date -d "-$&#123;days&#125; day" +%Y-%m-%d`echo $day_01#running_array=(`yarn application -list | grep application_1 | awk '&#123;print $1&#125;' &gt; running.log `)#running_array=(`more running.log`)running_array=(`yarn application -list | grep application_1 | awk '&#123;print $1&#125;' `)len=$&#123;#running_array[*]&#125;len=$(($len-1))running_string=""for ((i=0; i&lt; $&#123;#running_array[*]&#125;; i++))doif [ $i -lt $len ];then running_string+=$&#123;running_array[$i]&#125;"\|"else running_string+=$&#123;running_array[$i]&#125;fidoneecho $running_string #history_logs_to_delete=(`hdfs dfs -ls /spark2-history/ | grep application_ | grep -v $running_string | awk '&#123;if( $6 &lt; $day_01) print $8&#125;'`)history_logs_to_delete=(`hdfs dfs -ls /spark2-history/ | grep application_ | grep -v $running_string | awk '&#123;if( $6 lt $day_01) print $8 &#125; ' `)for ((j=0; j&lt; $&#123;#history_logs_to_delete[*]&#125;; j++))dohistory_logs_str=$&#123;history_logs_to_delete[$j]&#125;echo "`date +%F\ %T` to delete $j ==$history_logs_str"#hdfs dfs -rm -r -f -skipTrash $history_logs_strdone 小集群配置hdfs客户端，防止写入报错，在 hdfs-site 中配置 12dfs.client.block.write.replace-datanode-on-failure.enable=truedfs.client.block.write.replace-datanode-on-failure.policy=NEVER yarn 配置shuffle service， 在 yarn-site 和 spark2-defaults 中配置 1spark.shuffle.service.port=7337 spark持久化表，写入到spark的catalog当中，由于spark中snappy版本不对导致冲突，可以在启动程序的使用自定义的snappy jar包 1spark-shell --jars snappy-java-1.1.4.jar --conf spark.driver.extraClassPath=snappy-java-1.1.4.jar --conf spark.executor.extraClassPath=snappy-java-1.1.4.jar 后续的配置， HDFS、YARN高可用、开启ambari-server的keystore，这样在开启kerberos的时候可以选择记住key，检查hadoop native lib hadoop checknative 错误 Loading ISA-L failed: Failed to load libisal.so.2 1234567yum install gcc make autoconf automake libtool yasm gitgit clone https://github.com/01org/isa-l/cd isa-l/./autogen.sh./configure --prefix=/usr --libdir=/usr/lib64makemake install 错误 zstd: false ? 在配置ambari的时候， 如果使用ambari用户作为ambari的服务启动者， 那么在使用ambari配置的过程中可能经常会出现ambari用户权限不足的情况，所以可能需要如下操作，可以参考hdp官网： 更改权限 1chown -R ambari /var/run/ambari-server 添加sudo操作权限，使用 visudo 添加一行，建议使用完成之后收回权限 1ambari ALL=(ALL) ALL phoenix jar包冲突是一个比较棘手的问题， 使用spark的时候不能直接拷贝phoenix jar包到spark的jars目录中， 而是通过指定 -jars 参数来解决， hive的connector是同样的道理。 kerberos在登录的时候可以直接指定 krb5.conf 文件的位置 kerberos 命令行调试 1env KRB5_TRACE=/dev/stdout java 程序直接指定环境变量 1-Djava.security.krb5.conf=/etc/krb5.conf.prod linux 下 kinit直接添加环境变量 1env KRB5_CONFIG=/etc/krb5.conf.prod kinit -kt /etc/security/keytabs/feng.admin.keytab feng@TEPIA.COM 调用 UserGroupInformation.getCurrentUser 的时候，首先在core-site.xml 中查找是否开启kerberos，如果开启默认情况下会去查找系统是否已经kerberos登录， 主要是通过 krb5.conf 中配置文件的credentials存放位置并进行判断，如： /tmp/krb5cc_1000 hadoop可以使用 UserGroupInformation 实现代理机制，真正的用户进行kerberos登录，而代理用户进行真是的操作，如oozie进行提交任务，但是使用普通用户进行HDFS等各种权限，通过设置代理用户HADOOP_PROXY_USER MapReduce在访问kerberos HDFS和HBASE的时候，需要从NameNode和Hbase集群中获分别取代理的Token，并将其写入到Credentials当中，当MapReduce访问这两个组件的时候，就会带着这两个认证信息，从而达到访问的目的，参见 TableMapReduceUtil.initCredentials 方法。 修改 core-site.xml 设置 hadoop.proxyuser.yarn.hosts=* 保证kerberos集群中，spark长时间运行的应用。 hdp重新安装 使用ambari移除每一个应用 运行脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#!/bin/bashhostDomain=tepia.commaster=hdp1.tepia.comhostList=$(cat /etc/hosts| grep $hostDomain| awk -F' ' '&#123;print $2&#125;')services="pgsql postgres HDP ambari hadoop hdfs yarn mapred zookeeper tez hive hcatalog webhcat atlas druid zeppelin smartsense storm hbase phoenix spark kafka sqoop oozie infra solr accumulo flume hst knox livy pig ranger activity_analyzer infra-solr yarn-ats logsearch ambari-qa kms"for host in $hostListdo #检测主机的连通性 ping -W 1 -c 1 $host|grep "1 received" &gt; /dev/null 2&gt;&amp;1 if [ $? -ne 0 ];then echo -e "$======&gt;$host is Unreachable,please check '/etc/hosts' file" continue fi #停止agent ssh $host "ambari-agent stop" ssh $host "ambari-agent reset" if [ $host = $master ]; then ssh $host "ambari-server stop" ssh $host "ambari-server reset" fi echo "$host start deleting... \n" #repo #ssh $host "rm -rf /etc/yum.repos.d/hdp.repo" #ssh $host "rm -rf /etc/yum.repos.d/HDP*" #ssh $host "rm -rf /etc/yum.repos.d/ambari.repo" # 删除相关用户 ssh $host "python /usr/lib/ambari-agent/lib/ambari_agent/HostCleanup.py --silent" #postgres mysql admin # 删除服务相关信息 for ser in $services do #删除安装软件 packageList=$(ssh $host "yum list installed | grep $ser | cut -d ' ' -f 1") for package in $packageList do echo "uninstalling $package" ssh $host "yum remove -y $package" done #删除文件 ssh $host "rm -rf /etc/$&#123;ser&#125;*" ssh $host "rm -rf /etc/alternatives/$&#123;ser&#125;*" ssh $host "rm -rf /usr/bin/$&#123;ser&#125;*" ssh $host "rm -rf /usr/sbin/$&#123;ser&#125;*" ssh $host "rm -rf /var/$&#123;ser&#125;*" ssh $host "rm -rf /var/log/$&#123;ser&#125;*" ssh $host "rm -rf /var/run/$&#123;ser&#125;*" ssh $host "rm -rf /var/lib/$&#123;ser&#125;*" #删除用户 ssh $host "userdel -r $&#123;ser&#125;" ssh $host "groupdel $&#123;ser&#125;" #根据需要删除的文件, 加上查找的目录，通常已经不用。也可以使用locate命令更快(先安装mlocate,再更新locate数据库updatedb) #ssh $host "find / -name $&#123;ser&#125; &gt;&gt; /root/del_files.log" done # 删除hadoop文件夹，包括HDFS数据 #ssh $host "rm -rf /hadoop" ssh $host "rm -rf /home/data" ssh $host "rm -rf /usr/hdp" # 删除kerberos ssh $host "rm -rf /etc/security/keytabs/*" # 删除临时文件 ssh $host "rm -rf /tmp/*" ssh $host "yum clean all" echo "$host delete is done! \n"done 删除mysql中的数据库，并重新创建。注意其中有些用户是需要设置为特定主机的。 kerberos的数据库也要删除 12kdb5_util destroy TEPIA.COMkdb5_util create -s -r TEPIA.COM nifi 开启SSL，nifi 自带 NiFi Certificate Authority 授权程序，不推荐使用，直接选择独立安装即可 首先按照正常方式添加nifi服务 1nifi.allow.explicit.keytab = false 进入ranger plugin页面关闭nifi，并重启nifi（先关闭ranger认证，后期按需添加） 进入nifi-toolkit中，生成nifi认证文件 1bin/tls-toolkit.sh standalone -n &quot;hdp1.tepia.com,hdp2.tepia.com,hdp3.tepia.com&quot; --nifiDnPrefix &quot;CN=&quot; --nifiDnSuffix &quot;, OU=TEPIA.COM&quot; -d 10950 -C &quot;CN=feng, OU=TEPIA.COM&quot; -f ../nifi/conf/nifi.properties -o nifi -K admin123 -P admin123 -S admin123 -B admin123 复制生成的 keystore.jks 和 truststore.jks 到nifi中的conf目录中 修改nifi配置文件 nifi-ambari-ssl-config 中如下配置 12345678910111213141516Initial Admin Identity = feng@TEPIA.COMEnable SSL? = trueKey password = passKeystore password = passTruststore password = passKeystore type = jksTruststore type = jksNiFi CA DN suffix = , OU=TEPIA.COMNode Identities = &lt;property name=&quot;Node Identity 1&quot;&gt;CN=hdp1.tepia.com, OU=TEPIA.COM&lt;/property&gt; &lt;property name=&quot;Node Identity 2&quot;&gt;CN=hdp2.tepia.com, OU=TEPIA.COM&lt;/property&gt; &lt;property name=&quot;Node Identity 3&quot;&gt;CN=hdp3.tepia.com, OU=TEPIA.COM&lt;/property&gt; nifi.security.identity.mapping.pattern.dn = ^CN=(.*?), OU=(.*?)$nifi.security.identity.mapping.pattern.kerb = ^(.*?)@(.*?)$nifi.security.identity.mapping.value.dn = $1@$2nifi.security.identity.mapping.value.kerb = $1@$2 注意：配置 Node Identities 时候，注意 , OU 中间的空格 重启nifi，注意authorizations.xml必须没有策略，可以直接删掉 /var/lib/nifi/conf 下的authorizations.xml和users.xml文件。 将p12证书导入到浏览器当中，此时就可以进入nifi中，如果此时报如下错误 1Cannot replicate request to Node hdp2.tepia.com:9090 because the node is not connected 注意：这不是认证的错误，而是集群从不安全切换到安全集群的时候，集群状态的文件，此时需要删除 /var/lib/nifi/conf 下的authorizations.xml和users.xml文件，并重新启动nifi，如果实在不行就直接删除 /var/lib/nifi/* 再次重启，每次重启必须保证 /var/lib/nifi/conf 下没有authorizations.xml和users.xml 这两个文件 nifi开启SSL并使用ranger进行权限管理 首先说明，如果在配置更改的过程中出现无法启动或无法访问等问题，可以直接删除nifi相关文件，不影响配置 1rm -rf /var/lib/nifi/* /var/log/nifi/* 首先按照正常方式添加nifi服务，不需要添加NiFi Certificate Authority 授权程序 1nifi.allow.explicit.keytab = false 进入nifi-toolkit中，生成nifi认证文件 1bin/tls-toolkit.sh standalone -n &quot;hdp1.tepia.com,hdp2.tepia.com,hdp3.tepia.com&quot; --nifiDnPrefix &quot;CN=&quot; --nifiDnSuffix &quot;, OU=TEPIA.COM&quot; -d 10950 -C &quot;CN=feng, OU=TEPIA.COM&quot; -f ../nifi/conf/nifi.properties -o nifi -K admin123 -P admin123 -S admin123 -B admin123 复制生成的 keystore.jks 和 truststore.jks 到nifi中的conf目录中 修改nifi配置文件 nifi-ambari-ssl-config 中如下配置 1234567Enable SSL? = trueKey password = passKeystore password = passTruststore password = passKeystore type = jksTruststore type = jksNiFi CA DN suffix = , OU=TEPIA.COM 修改 Advanced nifi-authorizers-env 中如下配置 1&lt;property name=&quot;Ranger Admin Identity&quot;&gt;CN=feng, OU=TEPIA.COM&lt;/property&gt; 修改 Advanced nifi-properties 中的配置如下 1nifi.remote.input.socket.port = 1026 修改 Advanced ranger-nifi-plugin-properties 中的配置如下 123456Authentication = SSLKeystore for Ranger Service Accessing NiFi = /usr/hdf/current/nifi-toolkit/nifi/keystore.jksKeystore Type = jksTruststore for Ranger Service Accessing NiFi = /usr/hdf/current/nifi-toolkit/nifi/truststore.jksTruststore Type = jksOwner for Certificate = CN=feng, OU=TEPIA.COM 修改mapping信息 123456789nifi.security.identity.mapping.pattern.dn=^CN=(.*?), OU=(.*?), O=(.*?), L=(.*?), ST=(.*?), C=(.*?)$nifi.security.identity.mapping.value.dn=$1@$2nifi.security.identity.mapping.transform.dn=NONEnifi.security.identity.mapping.pattern.kerb=^(.*?)/(.*?)@(.*?)$nifi.security.identity.mapping.value.kerb=$1@$2nifi.security.identity.mapping.transform.kerb=NONEnifi.security.group.mapping.pattern.anygroup=^(.*)$nifi.security.group.mapping.value.anygroup=$1nifi.security.group.mapping.transform.anygroup=LOWER - 修改ranger web配置页面中的 `tepia_nifi` ，让其能够连接到nifi中，并对nifi进行权限控制，配置如下 1234567891011NiFi URL = https://hdp2.tepia.com:9091/nifi-api/resourcesAuthentication Type = SSLKeystore = /home/ranger/keystore.jksKeystore Type = jksKeystore Password = keypassTruststore = /home/ranger/truststore.jksTruststore Password = trustpasscommonNameForCertificate = CN=feng, OU=TEPIA.COMtag.download.auth.users = nifipolicy.download.auth.users = nifiambari.service.check.user = nifi **说明：** - 需要测试直至连接成功，连接的过程中可能需要先添加权限，并且此时可以看ranger的access访问 - 此处的keystore 和 truststore是通过 `tls-toolkit.sh` 生成的客户端秘钥和证书，但生成的是 `CN=feng_OU=TEPIA.COM.p12` 文件，需要转换成jks文件，并加入服务端证书到客户端truststore中。 - 转换p12证书为jks的keystone 1keytool -importkeystore -srckeystore CN\=feng_OU\=TEPIA.COM.p12 -srcstoretype pkcs12 -destkeystore keystore.jks -deststoretype jks - 导出 `hdp1.tepia.com, hdp2.tepia.com, hdp3.tepia.com` 的证书 123keytool -export -alias nifi-key -keystore hdp1.tepia.com/keystore.jks -rfc -file hdp1.cerkeytool -export -alias nifi-key -keystore hdp2.tepia.com/keystore.jks -rfc -file hdp2.cerkeytool -export -alias nifi-key -keystore hdp3.tepia.com/keystore.jks -rfc -file hdp3.cer - 将三个证书导入到truststore中 123keytool -import -alias hdp1 -file hdp1.cer -keystore truststore.jkskeytool -import -alias hdp2 -file hdp2.cer -keystore truststore.jkskeytool -import -alias hdp3 -file hdp3.cer -keystore truststore.jks - 注意我们这里ranger连接nifi直接使用了，自动生产的客户端的证书，所以我们并不需要将客户端证书导入到nifi中，这是因为 `tls-toolkit.sh` 已经自动导入过了，所以如果我们需要使用不同的用户名，需要自行证书导入即可。 - 修改ranger web配置页面，添加权限，不存在的用户需要在ranger中添加用户 - 添加all权限到 `CN=feng, OU=TEPIA.COM` 账号 1/flow /system /controller /counters /provenance /policies /tenants /proxy /resources /site-to-site /restricted-components / /* - 添加proxy权限到 `hdp1.tepia.com,hep2.tepia.com,hdp3.tepia.com` 当中 1/proxy /data/* /* Knox 安装配置 12开启Knox SSO单点登录https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/configuring-knox-sso/content/sso_set_up_knox_sso.html 12配置相关属性knoxsso.token.ttl 1234ranger 插件连接knoxecho | openssl s_client -connect cmsw1.tepia.com:8443 2&gt;&amp;1 | sed --quiet &apos;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p&apos; &gt; knox.crtkeytool -import -file knox.crt -keystore &lt;ranger config property ranger.truststore.file&gt; -alias knoxranger.truststore.password = trustsotre password 1knox存在默认密码，knox配置页面可以查看 1配置knox proxy，即 ranger插件配置，ranger插件配置是需要连接到对应的服务当中，这样在ranger配置权限的时候就有提示功能，如果没有在ranger页面配置连接到对应的服务，不影响ranger授权，只是页面没有提示功能而已。常见的配置错误如下： hbase、hive需要配置rangerlookup用户访问对应的表或者服务的权限（现在对于权限中配置，再配置连接） hbase需要配置Kerberos，并配置master的principal，如：`hbase/cmsw2.tepia.com@TEPIA.COM` Knox需要配置admin用户的用户名和密码 安装opentsdb 开启lzo压缩，教程 123yum install hadooplzo* -yio.compression.codecs = com.hadoop.compression.lzo.LzoCodecio.compression.codec.lzo.class = com.hadoop.compression.lzo.LzoCodec 安装opentsdb，直接从 github 上下载 OpenTSDB 的 release 版本的 RPM 包 1yum localinstall opentsdb-2.4.0.noarch.rpm 说明：如果需要更改opentsdb执行用户，可以更改为hbase，需要修改如下配置 1234chown -R hbase:hbase /tmp/opentsdb/chown -R hbase:hbase /var/cache/opentsdb/chown -R hbase:hbase /var/log/opentsdb/ln -s /usr/share/opentsdb/bin/tsdb /usr/bin/tsdb 可以通过命令查看有哪些opentsdb文件，rpm -ql opentsdb 和 find / -name *opentsdb* 创建opentsdb需要的表 12export HBASE_HOME=/usr/hdp/3.0.1.0-187/hbasetools/create_table.sh 修改logback.xml的两个日志的绝对地址，如果通过systemctl启动 opentsdb@.service ，那么可以不用更改 配置opentsdb.conf 123456tsd.storage.hbase.zk_basedir = /hbase-securetsd.storage.hbase.zk_quorum = cmsw1.tepia.com,cmsw2.tepia.com,cmsw3.tepia.comhbase.security.auth.enable=truehbase.security.authentication=kerberoshbase.kerberos.regionserver.principal=hbase/_HOST@TEPIA.COMhbase.sasl.clientconfig=Hbase 添加 /etc/opentsdb/opentsdb-jass.conf 12345678910111213141516171819Hbase &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;/etc/security/keytabs/hbase.headless.keytab&quot; storeKey=true useTicketCache=false serviceName=&quot;opentsdb&quot; principal=&quot;hbase-cmsw@TEPIA.COM&quot;;&#125;;Zookeeper &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;/etc/security/keytabs/hbase.headless.keytab&quot; storeKey=true useTicketCache=false serviceName=&quot;zookeeper&quot; principal=&quot;hbase-cmsw@TEPIA.COM&quot;;&#125;; 修改tsdb启动文件，添加jvm启动参数 1JVMARGS=&quot;$JVMARGS -Djava.security.krb5.conf=/etc/krb5.conf -Dhbase.security.authentication=kerberos -Dhbase.kerberos.regionserver.principal=hbase/_HOST@TEPIA.COM -Dhbase.rpc.protection=authentication -Dhbase.sasl.clientconfig=Hbase -Dzookeeper.sasl.clientconfig=Zookeeper -Djava.security.auth.login.config=/etc/opentsdb/opentsdb-jaas.conf&quot; 修改自动启动脚本，不建议使用这种方式启动 1234567891011...[Service]Type=simpleUser=hbaseGroup=hbaseLimitNOFILE=65535Environment=JAVA_HOME=/usr/java/jdk1.8.0_261-amd64Environment=&apos;JVMARGS=-Xmx6000m -DLOG_FILE=/var/log/opentsdb/%p_%i.log -DQUERY_LOG=/var/log/opentsdb/%p_%i_queries.log -XX:+ExitOnOutOfMemoryError -enableassertions -enablesystemassertions&apos;ExecStart=/usr/bin/tsdb tsd --config /etc/opentsdb/opentsdb.conf --port %iRestart=alwaysStandardOutput=journal 直接启动方式 1nothup bin/tsdb tsd --config /etc/opentsdb/opentsdb.conf &amp; 安装freeipa 服务端安装（freeipa服务端不能作为amber agent）： 12yum -y install bind bind-utils bind bind-dyndb-ldap ipa-server ipa-server-dnsipa-server-install --setup-dns --allow-zone-overlap 注意：所有的选择yes，注意配置forward dns 配置DNS，/etc/resolv.conf 1234search dhls.comnameserver 127.0.0.1nameserver 100.100.2.136nameserver 100.100.2.138 添加每一台客户端的DNS 12kadmin adminipa dnsrecord-add dhls.com iot0 --a-rec 172.19.36.14 客户端安装 12yum -y install ipa-clientipa-client-install 配置DNS 123nameserver 172.19.36.14 #ipa 服务端地址nameserver 100.100.2.136nameserver 100.100.2.138 ambari 安装 kerberos 之前 配置 /etc/krb5.conf 1default_ccache_name = FILE:/tmp/krb5cc_%&#123;uid&#125; 设置ipa密码不过期 设置java jce 安装的过程中，不要使用ambari 管理 krb5.conf 文件 Amber 配置 ldap 使用ambari-server setup-ldap 启动配置，ldap是内外直接使用389端口，不需要使用ssl，外网需要使用636（需要设置证书），bind的用户可以指定具有查看ldap权限的用户即可，如果没有创建直接使用admin用户，可以查看用户信息 123kinit adminipa user-show admin --raw --allipa group-show admins --raw --all 导入指定groups的用户 1ambari-server sync-ldap --groups=ambari.txt ambari.txt 内容如下 1dev 编写脚本自动导入/etc/ambari-server/script/ambari_sync_ldap.sh 1234567#!/usr/bin/expectspawn /usr/sbin/ambari-server sync-ldap --groups=/etc/ambari-server/script/ambari.txtexpect &quot;Enter Ambari Admin login:&quot;send &quot;admin\r&quot;expect &quot;Enter Ambari Admin password:&quot;send &quot;password\r&quot;expect eo 添加cron 1*/5 * * * * /etc/ambari-server/script/ambari_sync_ldap.sh &gt; /var/log/ambari-server/ambari_sync_ldap.log 2&gt;&amp;1 Knox 配置 ldap 修改 admin、knox、advanced topology 12345678&lt;param&gt; &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt; &lt;value&gt;uid=&#123;0&#125;,cn=users,cn=accounts,dc=dhls,dc=com&lt;/value&gt;&lt;/param&gt;&lt;param&gt; &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt; &lt;value&gt;ldap://iot0.dhls.com:389&lt;/value&gt;&lt;/param&gt; 配置knox proxy 1234&lt;service&gt; &lt;role&gt;SPARKHISTORYUI&lt;/role&gt; &lt;url&gt;http://bd0.dhls.com:18081&lt;/url&gt;&lt;/service&gt; 配置knox单点登录 https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/configuring-knox-sso/content/sso_set_up_knox_sso.html 1ambari-server setup-sso 注意：由于ambari的admin账户无法通过ldap导入进来，所以如果ambari也使用knox单点登录的话，admin账户是无法登录的，所以最好提前建立一个ambari账户，使其有admin管理功能。也可以使用命令关闭knox单点登录。 Ranger 配置ldap common configs 123LDAP/AD URL = ldap://iot0.dhls.com:389Bind User = uid=admin,cn=users,cn=accounts,dc=dhls,dc=comIncremental Sync = true （先不开启，后面再开启） user configs 123456Username Attribute = uidUser Object Class = posixaccountUser Search Base = cn=accounts,dc=dhls,dc=comUser Search Scope = subUser Group Name Attribute = memberofGroup User Map Sync = true group configs 123456Group Member Attribute = memberGroup Name Attribute = cnGroup Object Class = posixGroupGroup Search Base = cn=groups,cn=accounts,dc=dhls,dc=comGroup Search Filter = (cn=dev)Enable Group Search First = true advance config 12Ldap Base DN = dc=dhls,dc=comLdap User DN Pattern = uid=&#123;0&#125;,cn=users,cn=accounts,dc=dhls,dc=com]]></content>
      <categories>
        <category>hadoop</category>
        <category>hortonwork</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hortonwork</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 安装]]></title>
    <url>%2F2018%2F01%2F02%2Flinux-centos-install%2F</url>
    <content type="text"><![CDATA[journalctl 启动过程中遇到任何问题都可通过这里解决 安装完成后配置网卡自动启动 修改配置文件 vi /etc/sysconfig/network-scripts/ifcfg-ent33 1ONBOOT=YES 重启网卡 service network restart 安装网络工具 yum install net-tools 更换阿里云的源 安装wget yum install wget 获取阿里云的源 wget http://mirrors.aliyun.com/repo/Centos-7.repomv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS-7.repo CentOS-Base.repo 更新缓存存并更新 yum clean allyum makecacheyum -y update 设置ssh， 参考ssh使用教程 设置固定ip 编辑配置文件，添加或修改如下类容 vim /etc/sysconfig/network-scripts/ifcfg-eth0 123456BOOTPROTO=&quot;static&quot; #dhcp改为static ONBOOT=&quot;yes&quot; #开机启用本配置IPADDR=192.168.7.106 #静态IPGATEWAY=192.168.7.2 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.7.2 #DNS 配置 重启网络 service network restart 注意： 通常情况下，安装虚拟机后使用NAT网络， 那么主机的地址为192.168.7.1, 虚拟机网关地址 192.168.7.2, 剩下的地址才是虚拟机地址 注意, 不同的网络模式上面的配置方式不是一样的, 上面是NAT模式, 如果是桥接模式 1234567BOOTPROTO=&quot;static&quot;ONBOOT=&quot;yes&quot;IPADDR=192.168.0.137GATEWAY=192.168.0.1NETMASK=255.255.255.0DNS1=8.8.8.8DNS2=114.114.114.114 永久更改host name，会反映到 /etc/hostname 1hostnamectl set-hostname Linuxidc 关闭防火墙 systemctl stop firewalld.service systemctl disable firewalld.service 开放端口 firewall-cmd --zone=public --add-port=80/tcp --permanent 重新加载防火墙 firewall-cmd --reload 关闭SELINUX vim /etc/selinux/config` 1SELINUX=disabled 安装本地源 挂载U盘 mount -t vfat /dev/sdc4 /mnt/Centos7 步骤参考 [u盘挂载]: https://blog.csdn.net/leshami/article/details/78133716 安装无线网卡参考 设置开机启动 /etc/init.d/中添加可执行脚本, 开头可以参考这个目录下其它的脚本, 如： 添加 sslocal 1234567#!/bin/bash## sshd Start up the OpenSSH server daemon## chkconfig: 2345 55 25# description: sslcal shadowsocks/usr/bin/sslocal -c /etc/shadowsocks.json 添加到启动服务当中 chkconfig sslocal on 添加交换分区 添加swap分区dd if=/dev/zero of=/var/swapfile bs=1024 count=4096k 执行完毕，对交换文件格式化并转换为swap分区：mkswap /var/swapfile 挂载并激活分区：swapon /var/swapfile 赋权限chmod -R 0600 /var/swapfile 设置开机自动挂载该分区：在文件/etc/fstab文件末尾追加如下内容后/var/swapfile swap swap defaults 0 0 安装vmware 1234sudo yum updatesudo yum upgradesudo yum install &quot;kernel-devel-uname-r == $(uname -r)&quot; gccsudo sh ./VMware-Workstation-Full-14.1.1-7528167.x86_64.bundle 注意: 如何kernel headers 或者 kernel devel 找不到, 通常情况下是因为kernel的版本与 headers或者devel不同, 所以需要将版本安装成统一的. 挂载ntfs磁盘 123yum -y install epel-releaseyum -y install ntfs-3g fuseyum -y install ntfsprogs 防止敏感信息泄漏 12ln -s /dev/null .bash_historyln -s /dev/null .mysql_history 时间同步 12345678# 安装ntpyum install ntp -y &amp;&amp; \systemctl enable ntpd.service &amp;&amp; \systemctl start ntpd.service#设置时区timedatectl set-timezone Asia/Shanghai &amp;&amp; \timedatectl set-ntp yes &amp;&amp; \ yum自动更新 12345678910111213yum -y update # 首先更新一次yum -y install yum-cron# /etc/cron.daily/0yum-daily.cron # anacron 每天执行0yum-daily.cron一次# 它根据 /etc/yum/yum-cron.conf 来更新软件vim /etc/yum/yum-cron.confupdate_messages = yes,download_updates = yes,apply_updates = yes # 自动安装# 最后重启 crond yum-cronsystemctl start crondsystemctl start yum-cron]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux wifi配置]]></title>
    <url>%2F2018%2F01%2F02%2Flinux-wifi%2F</url>
    <content type="text"><![CDATA[首先下载iw工具。 yum -y install iw 获取无线网卡的名称 执行iw dev，假设获得名称为 wlp3s0（示例） 激活无线网络接口 执行ip link set wlp3s0 up 扫描当前环境中的无线网络 执行iw wlp3s0 scan|grep SSID，假设你能够连接的网络名称是TP-LINK-1（示例） 登录指定网络 执行wpa_supplicant -B -i wlp3s0 -c &lt;(wpa_passphrase “TP-LINK-1” “此网络的密码”) 主动请求动态地址 dhclient wlp3s0 查看获取的网络地址 执行ip addr show wlp3s0 设置自动启动 设置NetworkManager自动启动 chkconfig NetworkManager on 安装NetworkManager-wifi yum -y install NetworkManager-wifi 开启WiFi nmcli r wifi on 测试（扫描信号） nmcli dev wifi 连接 nmcli dev wifi connect wifi_name password wifi_password]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摄影]]></title>
    <url>%2F2018%2F01%2F02%2Flive-photo%2F</url>
    <content type="text"><![CDATA[摄影的三要素 曝光： 光线通过光圈到照片中接受的多少。 审美： 照片通过各种元素给人的视觉不同的冲击力。 其它： 照片体现的各种主题、人文等等。 曝光的三要素： 光圈： 小孔成像中小孔的描述， 光圈也存在大小。 快门： 光线通过小孔传递到照片， 通过一个开关控制光圈打开的时间。 ISO：由于场景中光线的强度有限， 如果通过大光圈慢快门还是无法达到我们想要的效果，那么通常需要调大ISO， 从而使得照片增加曝光。ISO也叫感光度， 如果ISO增大那么照片的曝光也会增大， 但是同样带来了另外一个问题， 噪音也会变大，所以通常情况下光线良好的都是低ISO。 景深的三要素： 景深定义：对于定焦物体后面的景色清晰程度， 也就是焦点后面物体多远的保持清晰 光圈：通过小孔成像的原理我们知道， 小孔越大照片越模糊， 小孔越小照片越清晰， 所以通常情况下拍摄人物会选择大光圈， 而拍摄景色使用小光圈， 这样也会使得人物定焦后， 人物清晰而人物后面的景色比较朦胧， 而景色整个的都比较清晰。 焦距：当拍摄时的光圈大小不变，被摄体的位置也不改变时，使用的镜头焦距越短，景深就越大;镜头的焦距越长，景深就越小。也就是在光圈不变的条件下，使用广角镜头时，景深的清晰范围就要相对大一些，使用中、长焦距镜头时，景深的清晰范围就相对要小得多。 焦点与相机的距离：而当拍摄时的光圈大小不变，所使用的镜头焦距也不改变时，被摄体越远，画面中的前后清晰范围就越大；反之，被摄体越近，前后的清晰范围也就相对越小。这就提醒我们，在拍摄一些特定和近景的画面时，调焦应该特别仔细，稍有疏忽，使主体景物越出景深范围，整个画面就都虚了。]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql性能解析]]></title>
    <url>%2F2018%2F01%2F02%2Fmysql-profile%2F</url>
    <content type="text"><![CDATA[profiling使用： 性能剖析一般分为两步， 测量任务所花费的时间， 然后对结果进行统计和排序。 New Relic, xhprof 可以通过设置long_query_time为0来捕获所有的查询 1234slow_query_log = onslow_query_log_file = /var/log/mysql/mysql-slow.loglong_query_time = 0#set global slow_query_log=on;通常情况下，服务器可以通过随时打开来进行开启慢查询 pt-query-digest, mysqldumpslow等等来分析满查询 性能剖析步骤： 使用慢查询， 直接开启慢查询或者使用pt-query-digest获取(show full processlist实现)或者直接使用tcpdump将网络包数据保存到磁盘， 然后使用pt-query-digest –type=tcpdump来进行解析并分析查询 使用pt-query-digest来分析慢查询日志， 并分析出结果 使用explain分析执行的语句 使用profiling来分析单条语句 1234set profiling=1select * show profilesshow profile for query n 通常使用show status、show processlist和show innodb status开销是非常低的 通过show profile for query n来获取到查询主要的耗时在哪个地方， 如： 123Copying to temp table 0.090623Sorting result 0.011555Sending result 0.045931 注意：可以看到此处最大的消耗是在临时表中， 所有此时我们考虑的是如何去掉临时表， 但经常可能会误导我们的是结果排序， 它占比非常低， 而我们如果简单通过查询语句很容易掉入到这个陷阱当中， 所以一般原则是”不建议优化排序缓冲区”。 show status通常是一个很有用的工具， 但是它不是一款剖析工具， show status大多都是一个计数器。 mysql在5.6.6中正是加入了performance_schema数据库（它的存储引擎也是performace_schema, 内存式数据引擎）， 用来记录系统中需要记录的性能相关的数据， 并且是默认开启的]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql自动分区]]></title>
    <url>%2F2018%2F01%2F02%2Fmysql-partitions%2F</url>
    <content type="text"><![CDATA[修改已经存在的表为其添加分区 1234567891011121314151617ALTER TABLE `device_model_report_copy1` PARTITION BY RANGE (to_days(time)) (PARTITION p201811 VALUES LESS THAN (to_days('2018-12-01')), PARTITION p201812 VALUES LESS THAN (to_days('2019-01-01')), PARTITION p201901 VALUES LESS THAN (to_days('2019-02-01')), PARTITION p201902 VALUES LESS THAN (to_days('2019-03-01')), PARTITION p201903 VALUES LESS THAN (to_days('2019-04-01')), PARTITION p201904 VALUES LESS THAN (to_days('2019-05-01')), PARTITION p201905 VALUES LESS THAN (to_days('2019-06-01')), PARTITION p201906 VALUES LESS THAN (to_days('2019-07-01')), PARTITION p201907 VALUES LESS THAN (to_days('2019-08-01')), PARTITION p201908 VALUES LESS THAN (to_days('2019-09-01')), PARTITION p201909 VALUES LESS THAN (to_days('2019-10-01')), PARTITION p201910 VALUES LESS THAN (to_days('2019-11-01')), PARTITION p201911 VALUES LESS THAN (to_days('2019-12-01')), PARTITION p201912 VALUES LESS THAN (to_days('2020-01-01')), PARTITION pmax VALUES LESS THAN (MAXVALUE)) 添加分区和差分分区 12ALTER TABLE `device_model_report_copy1` DROP PARTITION pmax;ALTER TABLE `device_model_report_copy1` ADD PARTITION (PARTITION p202001 VALUES LESS THAN (to_days('2020-02-01'))); 注意 : 由于分区表通常会带有一个最大的分区用来控制忘记添加分区， 所以我们添加分区的时候一定要知道最大分区中是否存在数据， 如果存在那么必须先修改最大分区， 所以通常情况下我们不会使用上面的语句进行添加分区， 而是使用Reorganize关键字， 并且Reorganize关键字不会导致数据丢失 1234ALTER TABLE `device_model_report_copy1` reorganize partition pmax into(PARTITION p202001 VALUES LESS THAN (to_days('2020-02-01')), PARTITION p202002 VALUES LESS THAN (to_days('2020-03-01')), PARTITION pmax VALUES LESS THAN (MAXVALUE)); 修改多个分区，在into关键字之前或之后都指定多个分区 1ALTER TABLE `device_model_report_copy1` reorganize partition p201901, p201902 ... p201912 into(Partition p2019 values less than(to_days('2020-01-01'))); 修改整个表的分区 1234ALTER TABLE `device_model_report_copy1` reorganize partition into(Partition p2019 values less than(to_days('2020-01-01'))，Partition p2020 values less than(to_days('2021-01-01'))，Partition pmax values less than(MAXVALUE)); 子分区 123456789101112create table orders_range(id int auto_increment primary key,customer_surname varchar(30),store_id int,salesperson_id int,order_Date date,note varchar(500)) engine=myisam partition by range(id) subpartition by hash(store_id) subpartitions 2(partition p0 values less than(5),partition p1 values less than(10),partition p3 values less than(15)); 注意 : 只允许对range和list类型的分区再进行分区，子分区的类型只允许是hash或key. 查看分区信息 123456789select partition_name part, partition_expression expr, partition_description descr, table_rows, data_length/1024/1024from information_schema.partitions where table_schema = schema() and table_name='device_model_report'; 创建分区存储过程 12345678910111213141516171819202122232425262728293031323334353637drop procedure if exists p_time_range_partition_add;DELIMITER // CREATE DEFINER=`root`@`%` PROCEDURE `p_time_range_partition_add`(IN `sch_name` varchar(50),IN `tab_name` varchar(50),IN `patition_date` date)b_lable:BEGINdeclare pmax_name VARCHAR(50) default "pmax"; -- max patition name must be pmaxdeclare p_name varchar(50) default concat('p', date_format(patition_date,'%Y%m'));declare p_time varchar(50) default concat(date_format(date_add(patition_date,interval 1 month),'%Y-%m'), '-01');declare pmax_table_rows integer default 1;-- check paramif (SELECT table_name FROM information_schema.TABLES WHERE table_schema = sch_name and table_name = tab_name) is null then leave b_lable;end if;-- test pmaxselect table_rows into pmax_table_rows from information_schema.partitions where table_schema = sch_name and table_name = tab_name and partition_name = pmax_name;if pmax_table_rows &gt; 0 then leave b_lable; -- need to manual alterend if;-- test exist pnameif (select partition_name from information_schema.partitions where table_schema = sch_name and table_name = tab_name and partition_name = p_name) = p_name then leave b_lable; -- has patitionend if;-- add patitionset @p_ps=concat("ALTER TABLE ", tab_name, " reorganize partition pmax into(PARTITION ", p_name, " VALUES LESS THAN (to_days('", p_time, "')), PARTITION pmax VALUES LESS THAN (MAXVALUE));");prepare stm from @p_ps;execute stm;END//DELIMITER ; 设置事件 1234-- 设置开启(数据库默认是关闭的，这个需要在my.cnf中设置下，否则数据库重启后会自动关闭)set global event_scheduler = 1;-- 查看任务调度启动状态show global VARIABLES like 'event_scheduler'; 1234567-- 创建任务调度create event event_p_time_range_partition_addon schedule every 1 month starts sysdate()on completion preserveenabledocall p_time_range_partition_add('test', 'device_model_report',date_add(curdate(),interval 1 month));]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端使用技巧]]></title>
    <url>%2F2017%2F09%2F26%2Flinux-terminal%2F</url>
    <content type="text"><![CDATA[操作系统启动顺序12345引导程序init进程(初始化操作系统 fork exec)login进程(用于校验用户信息, 并执行exec)bash进程(解释用户输入,并执行fork和exec)用户进程, 整个启动顺序 进程基本操作1234567nohup 用来接管程序运行，不会挂起程序并接管控制台输出到文件中command &amp; //将进程放在后台执行 ctrl-z //暂停当前进程 并放入后台, 放入后台后可以直接使用bg转换成后台执行 jobs //查看当前后台任务bg //将任务转为后台执行 fg //将任务调回前台 kill //杀掉任 进程输入输出重定向12345678重定向（在终端下如果，没有指明标准输入、标准输出、标准错误， 那么默认都是屏幕）标准输入 0， 标准输出 1， 标准错误 2， &gt; 替换， &gt;&gt; 追加&lt; 将标准输入更改为某个文件&gt; 默认将标准输入重定向， 和 1&gt; 完全等价2&gt; 将标准错误重定向&amp; 表示获取到某种类型管道（如果没有数字表示获取所有管道）， 如 &amp;1表示获取到标准输出tee 多重输出如：command &gt; log.out 2&gt;&amp;1 &lt; /dev/null &amp;， 将标准输出重定向到我文件（此时标准输出1就是文件log.out）, 把标准错误重定向到标准输出中(最终也是文件中), 标准输入使用 /dev/null 文件， 并后台启动 ！使用技巧1234567!! 上一个命令， 在缺少sudo的时候是非常有用的， sudo !!!n 运行历史的第n条命令!-n 运行历史的倒数第n条命令!* 使用上一个命令的所有参数： touch hello world, rm -rf !* 删除刚才创建的两个文件!^ 使用上一个命令的地一个参数: touch hello world, rm -rf !^ 删除hello文件!$ 使用上一个命令的最后一个参数 : touch hello world, rm -rf !$ 删除world文件!:- 使用上一个命令的除了最后一个参数所有的参数，包括命令: touch hello world]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux安装配置]]></title>
    <url>%2F2017%2F04%2F28%2Flinux-ubuntu-setup%2F</url>
    <content type="text"><![CDATA[基本安装 安装的时候一定要使用网络安装， 用iphone提供网络， 安装网卡驱动 显卡安装 完全卸载nvida显卡 1sudo apt-get remove --purge nvidia* 禁止加载 /etc/modprobe.d/blacklist-nouveau.conf 12blacklist nouveauoptions nouveau modeset=0 官网选择合适的cuda版本，安装完成后会默认安装最适合的nvida显卡驱动 查看 /lib/modprobe.d/blacklist-nvidia.conf是否存在，如果存在就注释其中的内容 重新编译启动文件 sudo update-initramfs -u 添加驱动盘 12345678910111213141516171819202122232425262728293031#!/bin/bash/sbin/modprobe nvidiaif [ &quot;$?&quot; -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo &quot;$NVDEVS&quot; | grep &quot;3D controller&quot; | wc -l` NVGA=`echo &quot;$NVDEVS&quot; | grep &quot;VGA compatible controller&quot; | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255else exit 1fi/sbin/modprobe nvidia-uvmif [ &quot;$?&quot; -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk &apos;&#123;print $1&#125;&apos;` mknod -m 666 /dev/nvidia-uvm c $D 0else exit 1fi 安装fusuma控制触摸板 安装 1234sudo gpasswd -a $USER input #需要管理员权限执行，配置开机启动也可以不用sudo apt-get install libinput-toolssudo apt-get install xdotoolgem install fusuma #使用ruby安装 配置 12345#确保开启多点触控gsettings set org.gnome.desktop.peripherals.touchpad send-events enabled#创建配置文件mkdir -p ~/config/fusum &amp;&amp; touch ~/.config/fusuma/config.yml#vi ~/.config/fusuma/config.yml 1234567891011121314151617181920212223242526272829303132swipe: 3: left: command: &apos;xdotool key alt+Left&apos; right: command: &apos;xdotool key alt+Right&apos; up: command: &apos;xdotool key ctrl+t&apos; down: command: &apos;xdotool key ctrl+w&apos; 4: left: command: &apos;xdotool key super+Left&apos; right: command: &apos;xdotool key super+Right&apos; up: command: &apos;xdotool key super+a&apos; down: command: &apos;xdotool key super+s&apos;pinch: in: command: &apos;xdotool key ctrl+plus&apos; out: command: &apos;xdotool key ctrl+minus&apos;threshold: swipe: 1 pinch: 1interval: swipe: 1 pinch: 1 配置开机启动， 参考下面的shadowsocks配置 wifi驱动安装后无法启动 方式一 查看网络信息 12345678910111213$ rfkill list 0:ideapad_wlan: Wireless LAN Soft blocked: no Hard blocked:yes 表示硬件阻塞没有开启 1:ideapad_bluetooth: Bluetooth Soft blocked: no Hard blocked: yes 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes 表示软件阻塞没有安装驱动 Hard blocked: no 移除ideapad模块， 后面的没有阻塞的模块可以启动 12345678$ modprobe -r ideapad_laptop$ rfkill list 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes Hard blocke 配置开机启动禁止ideapad模块 12创建/etc/modprobe.d/my_blacklist.conf文件block掉ideapad模块，这样没有重启就不用手动blockblacklist ideapad_laptop 方法二 直接添加ideap_wlan的模块到内核当中 1234sudo apt install git dkms build-essentialgit clone https://github.com/jeremyb31/ideapad-laptop.gitsudo dkms add ./ideapad-laptopsudo dkms install ideapad-laptop/1.0 安装iwlwifi驱动 1234567891011121314sudo apt updatesudo apt install build-essential gitgit clone https://git.kernel.org/pub/scm/linux/kernel/git/iwlwifi/backport-iwlwifi.gitcd backport-iwlwifisudo makesudo make installsudo -iecho &quot;options iwlwifi disable_msix=1&quot; &gt;&gt; /etc/modprobe.d/iwlwifi.confexit如果需要升级kernel需要重新安装cd backport-iwlwifisudo make cleansudo makesudo make install 开机自动启动 方式一 以systemctl创建shadowsockets开机启动为例子 sudo vim /etc/systemd/system/shadowsocks.service 1234567891011[Unit]Description=Shadowsocks Client ServiceAfter=network.target[Service]Type=simpleUser=rootRestart=on-failureRestartSec=5sExecStart=/usr/bin/sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json[Install]WantedBy=multi-user.target 把/home/xx/Software/ShadowsocksConfig/shadowsocks.json修改为你的shadowsocks.json路径，如：/etc/shadowsocks.json 生效配置 1sudo systemctl enable /etc/systemd/system/shadowsocks.service 方式二 直接启动Startup Applications程序， 填写需要执行的命令和参数 系统问题修复 查看dmesg日志， 并且google错误信息 1sudo dmesg 查看系统日志 系统卡死现象 12Ctrl + Alt + F1 ~ F6 进入tty1 ~ tty6模式Ctrl + Alt + F7 恢复到桌面模式 快捷键冲突问题 设置fcitx 网易云音乐 更改系统快捷键， 不常用的建议都去掉，保留下列即可 12345678910Ctrl + Alt + t 打开终端Ctrl + Super + up 放大应用Ctrl + Super + down 缩小应用Super + d 关闭/显示桌面Super + l 锁住桌面Super + w 显示打开的应用Super 显示左边栏并打开搜索Super + Tab 显示左边栏并上下切换打开应用Alt [+ SHift] + [向前]向后切换应用Alt + F4 关闭应用 安装compizconfig-settings-manager继续修改快捷键(可选) 关闭关闭tty防止ctrl + alt + F1-12的影响 1234# sudo vi /usr/share/X11/xorg.conf.d/50-novtswitch.confSection &quot;ServerFlags&quot; Option &quot;DontVTSwitch&quot; &quot;true&quot;EndSection 配置idea 博客 使用gsetting设置快捷键, 如更改全局快捷键 ctrl alt s键 找到快捷键的使用key值 gsettings list-recursively | grep -i \&gt;s\&#39; 设置key绑定的快捷键为空 12gsettings set org.gnome.shell.extensions.screenshot-window-sizer cycle-screenshot-sizes [&apos;&apos;]gsettings set org.gnome.shell.extensions.screenshot-window-sizer cycle-screenshot-sizes-backward [&apos;&apos;] 其它可选选项 安装限制程序 12sudo apt updatesudo apt-get install ubuntu-restricted-extras ubuntu-restricted-addons 安装字体 1sudo apt install ttf-mscorefonts-installer 12wget http://ftp.de.debian.org/debian/pool/contrib/m/msttcorefonts/ttf-mscorefonts-installer_3.7_all.deb -P ~/Downloadssudo gdebi ~/Downloads/ttf-mscorefonts-installer_3.7_all.deb 1234wget https://github.com/fangwentong/dotfiles/raw/master/ubuntu-gui/fonts/Monaco.ttfsudo mkdir -p /usr/share/fonts/customsudo mv Monaco.ttf /usr/share/fonts/customsudo chmod 744 /usr/share/fonts/custom/Monaco.ttf 123mkdir /usr/share/fonts/WindowsFontscp /Windowsdrive/Windows/Fonts/* /usr/share/fonts/WindowsFonts 复制windows中的字体chmod 755 /usr/share/fonts/WindowsFonts/* 123sudo mkfontscale #生成核心字体信息sudo mkfontdirsudo fc-cache -f -v 修改 sudoer 的环境变量 PATH， 在 .bashrc 中添加方法 使用其它命令代替sudo命令 1psudo() &#123; sudo env PATH=&quot;$PATH&quot; &quot;$@&quot;; &#125; 完全覆盖sudo命令 1sudo() &#123; command sudo env PATH=&quot;$PATH&quot; &quot;$@&quot;; &#125; 说明： command 命令防止命令自身递归调用 温度监控 sudo apt-get install lm-sensors hddtemp sudo sensors-detect sensors（查看硬件实时温度） sudo apt-get install psensor 安装成功后，打开psensor，在Sensor Preferences中选择需要的硬件勾选Display sensor in the label 搜狗输入法 太阳和月亮切换 shift + 空格 符号切换 ctrl + 点号 与windows交互软件 鼠标交互synergy 文件共享samba 远程连接到windows，xfreerdp 有道词典命令行工具ydcv zsh历史命令共享工具 history-sync，https://github.com/wulfgarpro/history-sync]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce使用]]></title>
    <url>%2F2017%2F04%2F26%2Fhadoop-map-reduce%2F</url>
    <content type="text"><![CDATA[概述 MapReduce是一个简单的计算处理框架, 它能够在一个廉价的大型集群上以一种容错的方式并行的处理TB级的数据。 一个MapRecude任务通常讲输入数据集进行分块, 并且以完全并行的方式处理map任务, 框架对map任务输出进行。排序, 并作为reduce任务的输入, 通常任务的输入和输出都存储在文件系统, 框架负责处理协调、监控并重运行失败的任务。 通常情况下计算节点和存储节点都是在同一个HDFS(如果MapReduce以HDFS作为文件系统)的datenode中， 这种配置允许框架能够有效的协调任务在已经存在数据的节点中运行，这通常在集群中对网络带宽非常友好。 MapReduce也可以通过yarn（分布式资源协调工具）来运行， 并且为每一个MapReduce任务创建一个MRAppMaster负责从yarn的ResourceManager申请协调资源和对MapReduce任务进行监控和管理。 最小的一个MapReduce任务由一个输入输出、实现特定接口的map和reduce方法、以及一个job配置， job客户端提交任务， ResouceManager对任务进行协调监控并提供诊断信息给客户端。 虽然Hadoop框架是使用java运行， 但是MapReduce应用却不要求一定使用java编写， 如Hadoop Streaming允许用户使用任何可执行的shell（能够在shell命令中执行） 创建并运行mapper或者reducer， Hadoop Pipes以一个SWIG兼容的C++ API来实现MapReduce任务（而不是基于JNI） 输入和输出 MapReduce框架操作唯一的&lt;key, value&gt;对，框架输入&lt;key, value&gt;对， 并产生&lt;key, value&gt;对作为输出，key和value需要在框架中通过RPC或者文件的方式进行传输， 所以必须序列化， 框架提供Writable接口作为序列化的基础， 另外框架需要通过key来对&lt;key, value&gt;对进行排序， 所以必须实现WritableComparable接口， 虽然key没有要求必须实现hashcode方法， 但是map任务后的reduce任务通常需要对&lt;key, value&gt;行partition， 而通过hash来进行partition却是默认的方式。 简单的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; //创建单实例的对象， 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; //输入的key其实是一个相对于文件的偏移量， value为每一行的数据 //所以读取文件必须有一个格式， 并且讲文件split给每个map任务，这里由InputFormat提供 StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one);//将split的单词和数量输出&lt;workd, count&gt; &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, "word count"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); //指定Combiner进行本地Reducer任务执行， 主要是这里的reducer任务具有操作无序性质， 所以 //本地的reducer一般用来减小mapper到reducer的带宽， 对于速度上可能并不明显 job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //等待任务完成， 返回job客户端， 也可以执行返回 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 运行应用1$ bin/hadoop jar wc.jar WordCount /user/joe/wordcount/input /user/joe/wordcount/output 为MapReduce任务提供ClassPath和work dir提供资源1$ bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -files dir1/dict.txt#dict1,dir2/dict.txt#dict2 -archives mytar.tgz#tgzdir input output 说明： -files 提供文件到任务work dir中， 通过逗号分割， 使用#号取别名, -archives 提供文件并且解压缩到work dir， 通过都好分割， 使用#号取别名， 两者指定的文件可以在MapReduce任务中获取 Mapper Hadoop MapReduce通过InputFormat将输入的files进行split并创建一任意个InputSplit，并为每个InputSplit创建一个map任务用来执行这个InputSplit表示的逻辑文件， map任务通过RecordReader来读取InputSplit这个逻辑文件， 并生成map的输入对&lt;key1, value1&gt;, InputSplit和RecordReader都是由InputFormat创建。 Mapper类是执行map任务的核心， 它循环的执行RecordReader产生的&lt;key1, value1&gt;对， 并且在执行任何的&lt;key1, value1&gt;对之前可以对Mapper进行初始化操作，如下： 1234Mapper.setup(Context)Mapper.run(Context) # 用于提供map操作的模板方法， Mapper调用的方法Mapper.map(key, value, Context)Mapper.cleanup(Context) MapReduce 输出对不需要和输入对有任何的联系， 输入对可以map出零个或者多个输出对， 通过context.write()输出。 用户可以指定combiner对mapper产生的输出进行聚合， 这样可能会加快速度和减少网络带宽， 其实combiner就是一个reducer， 只是它单纯的对当前map任务的所有输出进行reduce任务操作， 操作完成之后再传给总的reduce来进行处理， 所以可以看到此处的reduce任务必须具有无序性， 其次reducer也可以接受并非只是mapper的中间结果&lt;key2, value2&gt;, 也可以接受&lt;key2, [value2, vlaue3…]&gt; map任务的个数通常是通过输入文件的总大小，也就是总的输入文件的块， 正确的并发map任务通常每个节点10-100个， 通常对于轻量级cpu任务开启300个map任务， 因为一般情况下map需要从磁盘获取数据， 并进行简单操作， 所以通常都是io密集型而对cpu不敏感， map任务的创建需要时间， 所以最好map任务执行超过一分钟。 Partitioner Partitioner是用map产生的中间结果&lt;key1, value1&gt;的key空间来对每个map任务的的中间结果进行分区的， 通常情况下是通过hash来进行分区， 总的分区数量和reduce任务的分区数量是一样的。 ReducerReducer 用来reduce Mapper任务产生的中间结果， 当Mapper产生结果后， 通过Partitioner将具体&lt;key2, value2&gt;或者是&lt;key2, [vlaue2, value3…]&gt;(经过combiner之后的结果)传输到具体的Reducer当中， 具体的某个Reducer收到所有Mapper需要传输到自己的&lt;key2, value2/[value2, value3…]&gt;之后， 对其进行shuffle、sort和最终的reduce Shuffle 讲所有从Mapper中获取的结果进行shuffle成&lt;value2, [value2, value3…]&gt; Sort 对shuffle结果进行排序，shuffle和sort是同时进行的， 排序方式可以通过Job.setSortComparatorClass(Class)来提供， 此处是对key进行排序 Secondary Sort 如果需要对相同key的values进行排序可以指定Job.setGroupingComparatorClass(Class)来对value进行排序 Reduce 对当前Reducer收到并处理的&lt;key2, [value2, value3…]&gt;进行reduce操作， 用来产生&lt;key3, value3&gt;输出到文件系统中，reduce输出没有排序。reduce在操作之前和之后可以和map进行相同的初始化或者结束回调。 reduce的数量通常在[0.95~1.75] 之间， 当maps任务执行完后， 指定倍数为0.95时所有的reducers可以立即启动并且执行传输map的输出， 当为1.75时最快的节点将会完成它们第一轮并且执行第二轮， 有助于负载均衡。 增加的reduces的数量虽然增加了负载， 但是增加了负载均衡的能力， 并且减小了失败造成的消耗。 CounterCounter 是一个用来报告统计的基础设施， Mapper和Reducer实现可以使用Counter来报告统计信息。 Job Configuration Job代表一个MapReduce的任务配置， 通常用来配置InputFormat、Mapper、combiner（如果有）、Partitioner、Reducer、OutputFormat的实现 FileInputFormat和FileOutpuFormat配置输入和输出格式 12FileInputFormat.setInputPaths(Job, Path…) FileInputFormat.addInputPath(Job, Path)FileInputFormat.setInputPaths(Job, String…) FileInputFormat.addInputPaths(Job, String) Job通常需要指定先进的参数， 如Comparator、文件put到DistributedCache、是否使用compressed、是否speculative执行以及map和reduce最大的attempts 当然Job还可以使用Configuration的get/set来获取和设置更多的参数1Configuration.set(String, String) 任务执行和环境变量 MRAppMaster使用隔离jvm的一个子进程去执行Mapper/Reducer任务， 子进程继承父进程的环境变量， 用户可以指定额外的子进程虚拟机启动参数，如下：12345678910111213141516&lt;!-- @taskid@ 表示MapReduce任务id--&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; 任务提交和监控 Job是一个原始的接口， 用于用户job与ResourceManager交互， Job提供的基本功能是提交任务、跟踪任务进程、访问任务的报告和日志和获取MapRecude集群状态等信息， job提交任务涉及到下面几个方面： 检查任务的输入输出 计算InputFormat的InputSplit 为任务分布式缓存设置必要的账号信息 复制任务jar和配置信息到MapReduce系统目录 提交任务到ResourceManager并选择是否监控它的状态 任务历史日志记录在两个指定的目录当中， 如下： 12mapreduce.jobhistory.intermediate-done-dirmapreduce.jobhistory.done-dir 用户可以查看任务运行历史日志总览 1$ mapred job -history [all] output.jhist Job 输入 InputFormat 描述MapReduce任务的输入，主要功能如下： 验证任务输入 split 所有的输入文件为逻辑的InputSplit实例， 每一个InputSplit相当于一个Mapper任务 提供RecordReader实现， 用于Mapper任务启动时从InputSplit中获取单个&lt;key1, value1&gt;记录 默认基于文件的输入是FileInputFortmat， 它通过总文件大小来split出逻辑的InputSplit， 显然通过总大小来分割有时候是无效的， 因为RecordReader无法获取记录的边界 Job输出 OutputFormat 描述MapReduce任务的输出， 主要功能如下： 验证任务输出， 如：检查输出文件是否存在 提供RecorderWriter实现， 用于写入任务输出， Output文件存储到文件系统 OutputCommiter 描述MapReduce任务输出提交， OutputCommiter的主要功能如下： 初始化设置， 如：为任务创建临时输出目录， 任务的初始化设置是在单独的任务中完成的， 初始化之前任务处在PREP状态， 初始化完成后处在RUNNING状态 任务完成后的清理动作， 如：删除零时输出目录， 任务的清理工作也是在单独的任务中， 清理工作完成后任务可能出现SUCCEDED/FAILED/KILLED状态 检查任务是否需要提交 一旦任务完成就提交任务的输出 如果任务失败， 任务输出将会被清理， 并且丢弃任务提交， 如果任务不能清理， 一个相同attempt-id的任务将会单独启动并执行清理动作 RecordWriter 负责讲Reducer的输出&lt;key3, value3&gt;输出到文件系统， 并且负责讲job的outputs输出到文件系统 提交Job到队列 用户提交任务到队列当中， 队列作为一个任务集合允许系统提供指定的功能， 如队列使用ACLs控制那个用户可以提交任务到队列， 在Hadoop中默认就是使用队列作为调度器来调度任务。 Hadoop中存在一个叫做default的队列用于用户默认的任务提交队列， 用户也可以通过Configuration.set(MRJobConfig.QUEUE_NAME, String)指定将任务提交到特定名字的队列当中， 并且Hadoop还提供了一个可插拔的Capacity Scheduler用来支持多租户， 提供资源的隔离， 所以它存在多个队列。 计数器 Counters代表全局的计数器， 可以被MapReduce框架或者应用定义， 每一个Counter可以是一个Enum， 特定的Enum的Counter被集中到特定的Counters.Group当中， 应用可以定义任意的Enum并且Counter.incrCounter(Enum, long)或者Counter.incrCounter(String, String, long)到map或者reduce方法当中， 这些counters被框架自动聚合。 分布式缓存 分布式的缓存用于有效的存储特定应用大型只读的文件， 它是被MapReduce框架提供的基础设施用于缓存应用需要的文本、归档和jar文件等等 Job中的应用通过hdfs://指定被缓的文件， 分布式的缓存假设被指定hdfs://的文件已经存在于文件系统当中 框架将会在任何任务被执行之前，复制必须的文件到worker节点， 事实上它是相当有效的， 文件只会被复制一次， 并且un-archive到work节点 分布式的缓存跟踪缓存文件的修改时间戳， 显然分布式的缓存文件是不能被任何的修改 文件、归档、jar/native lib都可以被缓存， 可以通过property设置， 或者通过Job添加， 如：12Job.addCacheFile(URI) / Job.addCacheArchive(URI)Job.AddArchiveToClassPath(Path) / JOb.addFileToClassPath(Path) URI: hdfs://host:port/absolute-path#link-name 分布式的缓存文件可以是私有或公用， 这决定了它们是怎样被worker节点所共享的， 主要是通过分布式文件的用户和文件的可见性来决定 #### Debugging MapReduce框架提供当任务失败时执行用户指定脚本的功能， 脚本可以访问任务的标准输出、标准错误、系统日志和任务配置， 脚本处理访问的信息并输出错误信息到控制台或者任务UI Debugging脚本的使用步骤： 提交脚本到分布式缓存中 提交脚本到map或者reduce当中， 使用property或者Configuration 脚本访问 $script $stdout $stderr $syslog $jobconf 数据压缩 MapReduce提供map和reduce应用输出压缩功能， 为了性能它使用本地实现 跳过错误的记录 Hadoop可以通过ShipBadRecords类来控制map输入的错误记录， 对于某些应用来说如果少量的错误记录是可以接受的那么可以开启此功能， 使用SkipBadRecords.set[Mapper|Reducer]MaxSkipGroups(Configuration, long)来设置最大的错误记录数量。 复杂的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Counter;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.StringUtils;public class WordCount2 &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; // 提供counter功能 static enum CountersEnum &#123; INPUT_WORDS &#125; // 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); // 提供单词大小写敏感和模式忽略功能 private boolean caseSensitive; private Set&lt;String&gt; patternsToSkip = new HashSet&lt;String&gt;(); private Configuration conf; private BufferedReader fis; // 在setup中处理Mapper的初始化动作 @Override public void setup(Context context) throws IOException, InterruptedException &#123; // 获取配置 conf = context.getConfiguration(); // 获取Job中设置的环境变量 caseSensitive = conf.getBoolean("wordcount.case.sensitive", true); // 解析配置 if (conf.getBoolean("wordcount.skip.patterns", false)) &#123; //获取并解析缓存 URI[] patternsURIs = Job.getInstance(conf).getCacheFiles(); for (URI patternsURI : patternsURIs) &#123; Path patternsPath = new Path(patternsURI.getPath()); String patternsFileName = patternsPath.getName().toString(); parseSkipFile(patternsFileName); &#125; &#125; &#125; private void parseSkipFile(String fileName) &#123; try &#123; fis = new BufferedReader(new FileReader(fileName)); String pattern = null; while ((pattern = fis.readLine()) != null) &#123; patternsToSkip.add(pattern); &#125; &#125; catch (IOException ioe) &#123; System.err.println("Caught exception while parsing the cached file '" + StringUtils.stringifyException(ioe)); &#125; &#125; @Override public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; // 处理单词大小写敏感和模式忽略功能 String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase(); for (String pattern : patternsToSkip) &#123; line = line.replaceAll(pattern, ""); &#125; StringTokenizer itr = new StringTokenizer(line); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); // counter计数， 会在日志中打印 Counter counter = context.getCounter(CountersEnum.class.getName(), CountersEnum.INPUT_WORDS.toString()); counter.increment(1); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 配置解析 Configuration conf = new Configuration(); GenericOptionsParser optionParser = new GenericOptionsParser(conf, args); String[] remainingArgs = optionParser.getRemainingArgs(); if ((remainingArgs.length != 2) &amp;&amp; (remainingArgs.length != 4)) &#123; System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt; [-skip skipPatternFile]"); System.exit(2); &#125; Job job = Job.getInstance(conf, "word count"); // 基本设置 job.setJarByClass(WordCount2.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); List&lt;String&gt; otherArgs = new ArrayList&lt;String&gt;(); for (int i=0; i &lt; remainingArgs.length; ++i) &#123; if ("-skip".equals(remainingArgs[i])) &#123; // 设置缓存 job.addCacheFile(new Path(remainingArgs[++i]).toUri()); // 设置环境变量 job.getConfiguration().setBoolean("wordcount.skip.patterns", true); &#125; else &#123; otherArgs.add(remainingArgs[i]); &#125; &#125; // 设置输入与输出 FileInputFormat.addInputPath(job, new Path(otherArgs.get(0))); FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1))); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 命令行执行123456789101112131415161718192021222324$ bin/hadoop fs -ls /user/joe/wordcount/input//user/joe/wordcount/input/file01/user/joe/wordcount/input/file02$ bin/hadoop fs -cat /user/joe/wordcount/input/file01Hello World, Bye World!$ bin/hadoop fs -cat /user/joe/wordcount/input/file02Hello Hadoop, Goodbye to hadoop.$ bin/hadoop fs -cat /user/joe/wordcount/patterns.txt\.\,\!to$ bin/hadoop jar wc.jar WordCount2 -Dwordcount.case.sensitive=false /user/joe/wordcount/input /user/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt$ bin/hadoop fs -cat /user/joe/wordcount/output/part-r-00000bye 1goodbye 1hadoop 2hello 2horld 2]]></content>
      <categories>
        <category>hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pssh自动化运维]]></title>
    <url>%2F2017%2F04%2F26%2Ftools-dsh%2F</url>
    <content type="text"><![CDATA[pass使用教程12345678910111213-h 执行命令的远程主机列表文件 -H user@ip:port 文件内容格式[user@]host[:port]-l 远程机器的用户名-p 一次最大允许多少连接-o 输出内容重定向到一个文件-e 执行错误重定向到一个文件-t 设置命令执行的超时时间-A 提示输入密码并且把密码传递给ssh（注意这个参数添加后只是提示作用，随便输入或者不输入直接回车都可以）-O 设置ssh参数的具体配置，参照ssh_config配置文件-x 传递多个SSH 命令，多个命令用空格分开，用引号括起来-X 同-x 但是一次只能传递一个命令-i 显示标准输出和标准错误在每台host执行完毕后-I 读取每个输入命令，并传递给ssh进程 允许命令脚本传送到标准输入 Host 文件 1234192.168.1.101:22192.168.1.109:22192.168.1.118:25791192.168.1.105:25791 使用ssh参数，禁止ssh提示 1pssh -h hosts.txt -l root -p 6 -t 0 -i -A -x &apos;-o StrictHostKeyChecking=no&apos; date i：命令行输出结果 A：提供密码输入 x：添加ssh参数 批量执行命令 1pssh -h hosts.txt -l root -i &apos;uptime&apos; 批量上传本地文件到服务器 12pscp.pssh -l root -h hosts.txt /mnt/file /tmp/pscp.pssh -l root -h hosts.txt -r /mnt/dir /tmp/ 批量上传目录 批量下载文件到本地 12pslurp -l root -h hosts.txt /etc/hosts .pslurp -l root -h hosts.txt -r /home/ ./ 批量下载目录 批量同步本地文件到服务器 1prsync -l root -h hosts.txt -r /mnt/test/ /mnt/test/ 注意：同步文件的时候，其实就是完全覆盖，远程机器对应文件内的文件会被全部替换！ 批量杀死远程服务器上的进程 1pnuke -h hosts.txt -l root nginx]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile 详解]]></title>
    <url>%2F2017%2F04%2F26%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[原理 docker 每一条命令都会创建一层， 并且会使用上一层的镜像运行一个docker容器， 再执行本层的命令，执行完成之后就提交停止容器， 并提交当前容器的修改为一个镜像 当容器开始构建的时候， 第一步会将指定的context目录下的所有的文件都传递给docker守护进程（除了.dockerignore文件标识的文件除外), 第二步启动FROM image指定的镜像的容器， 并执行下面一条命令，关闭当前容器并提交一层镜像， 以后各层都是如此，以上一层的镜像运行一个容器并执行命令后提交镜像所以我们可以看到命令越多则层数越多， 建议尽量使用最少的层， 并且将相关的动作描述清晰使其容易维护 注意 妥善使用下面点： 基础镜像选择， 使用两种环境变量， 组合命令 设置基础镜像1FROM ubuntu 设置LABELLABEL用来标示当前镜像， 作为一种元数据存在与镜像当中， 作为一种描述123LABEL com.ilivoo.game=&quot;fengxiang&quot; \ version=&quot;1.0&quot; \ description=&quot;this is my first game!&quot; 设置环境变量设置docker容器运行时的环境变量， 并且可以在构建的过程中使用这些命令， 其实道理很简单， 以后的构建过程中都是使用上一个镜像层作为基础， 而运行的时候启动容器这个环境变量就已经生成了， 所以后面的RUN 等命令当然也是可以获取到这个环境变量的， 所以脚本引用方式与正查的脚本完全一样, 可以看到巧妙的设计环境变量可以使得构建过程非常灵活， 并且运行时容器也非常灵活12ENV VERSION=1.0 \ DEBUG=on 设置ARG参数ARG命令也是用来设置环境变量的， 但是ARG设置的环境变量不会保留到最终容器运行的层当中, 其实实现也非常简单当运行容器的时候设置环境变量， 并且在容器退出的时候就将环境变量删除就可以达到此目的了, 并且此参数可以通过–build-arg参数在构建的时候进行覆盖，这样就可以达到非常灵活的效果了虽然ARG环境变量不会保存到运行是环境变量， 但是也不能使用敏感数据， 因为使用history会查看到其值1ARG REDIS_VERSION=3.2.10 环境变量与ARG参数结合使用 通过在构建的时候覆盖当前–build-arg NAME=FENGXIANG此时后面设置的环境变量FENG就变成了FENGXIANG, 可以看到虽然环境变量不能通过参数的形式进行改变但是这里确提供了灵活的方式来达到环境变量的不同， 所以通过脚本来构建就可以达到非常灵活的效果， 所以在我们的代码当中需要进行灵活变化的变量应该使用环境变量来进行设置， 这样就可以达到任何代码都不需要更改确有一个灵活的生产 测试 线上环境， 这个非常重要 12ARG NAME=XIANGENV FENG=$&#123;NAME&#125; 建议将那些需要进行覆盖的ARG使用这种方式给出， 并且在使用的时候通过字符串获取的方式来获得， 此处标示如果COMPANY不存在那么默认值就是ilivoo通过这种方式能够非常明显标示当前ARG用来进行占位的， 真正使用的时候可以进行指定， 并且最好是通过注释的方式标示可以指定的值 12ARG COMPANYENV COMPANY $&#123;COMPANY:-ilivoo&#125; 指定工作目录指定docker构建的以后的各层当中都使用指定的这个目录作为工作目录， 也就是以后命令执行的当前目录都是这个目录，如：CMD [“./start.sh”], 这个命令默认就去查找这个目录下对应的脚本， 如果不存在则报错， 并且每次设置都会更改以后的所有层， 可以覆盖。 所以建议再构建镜像的时候只有在最终执行命令的时候再去指定这个目录， 构建最终层镜像的时候显示的指定每个工作的目录是一个非常不错的选择1WORKDIR /app 指定执行用户USER和WORKDIR命令类似， 都是改变环境状态并影响以后的层， 而USER则是更改RUN CMD ENTRYPOINT这类动作命的身份， 用户必须事先建立好。如果以root执行脚本， 在执行容器的时候希望使用特定的身份执行容器， 不建议使用sudo命令， 这样会非常麻烦，而且容易出错，建议使用gosu来控制， 借鉴redis的做法。1USER root ADD命令当使用ADD添加本地文件时， 如果是单个文件才会进行解压缩(如果多个文件不会解压缩)如： ADD hello.tar.gz /app如果原路径是一个URL那么docker会试图区下载这个文件， 下载后权限自动设置为600，并且不会解压文件，如果需要更改还需要添加可以看到ADD命令其实限制非常多， 并且语义也并不明确， 如果需要从网络下载还不如使用RUN命令，再使用curl gzip chmod等一次更改当前修改， 这样只需要创建一层就可以完成了。所以在项目中尽量不要使用这个命令来添加文件， 仅当需要解压时才使用， 如将所有的项目文件压缩为一个zip包， 再通过一条ADD命令将其添加到项目目录当中12ADD http://download.redis.io/releases/redis-$&#123;REDIS_VERSION&#125;.tar.gz /appADD hello.tar.gz /app COPY命令注意docker构建的context， 也就是docker构建的上下文再docker构建的过程中是没有当前目录的概念的，比如：不能指定../clear.sh， 因为docker引擎不知道这个目录是哪里，所以获取不到这个文件当使用docker build进行构建的时候， 会设置Dockerfile和Context上下文对于的路径， 如果没有设置Dockerfile的位置， 那么默认会在context中查找Dockerfile, 如果没有那么构建失败而当指定Context的时候，docker客户端会将context指定的目录下所有的文件都传送给docker引擎， docker引擎再根据dockerfile中指定的需要的文件从context中去查找， 如果找不到构建失败由此可见Dockerfile中指定的原路径都是指定当前docker引擎的context路径docker引擎每向下构建一层需要的依赖文件就从docker引擎的context路径传送到启动的docker容器当中， 再提交容器生成一层镜像1COPY . /app RUN命令在构建的过程中运行当前命令， 存在两种形式shell格式和exec形式, shell格式就像直接运行shell命令一样执行 而exec格式则是只按照当前给出的命令和参数的形式进行执行此处必须写成./start.sh， 而不能直接写成start.sh， 因为在shell下直接写成start.sh也是不能运行的RUN ./start.sh而此处exec格式，必须添加上执行的shell1RUN [&quot;/bin/bash&quot;, &quot;start.sh&quot;] CMD命令指定容器启动的主进程启动命令， 也就是操作系统挂载文件系统之后启动的init进程， 而此处的主进程就是类似init进程, 当且仅当主进程推出那么此时容器内所有的进程都退出， 并且我们知道主进程init是所有的进程的父进程，当产生僵尸进程的时候当父进程死后，而子进程没有死那么为了防止僵尸进程，此时就需要init进程来寄养子进程。这里完全和linux操作系统是一样的。在使用shell格式的时候， 系统会默认为其添加sh -c的参数形式执行容器在其启动的时候可以重新覆盖CMD命令，从而启动其他的命令1CMD echo $HOME ENTRYPOINT命令ENTRYPOINT, 当使用了ENTRYPOINT之前的CMD和ENTRYPOINT就被覆盖， 而定义在ENTRYPOINT后面的CMD则作为ENTRYPOINT的参数, 而且在容器运行的时候还可以覆盖CMD参数， 这样就可以达到一种默认命令， 并且可以更改参数的用途， 这样某些场景会非常有用， 比如redis的启动， ENTRYPOINT指定启动脚本， 并且在真正的主进程启动之间进行一些环境的修改， 如执行用户等等，并且可以设置参数。 第二可以把容器当作命令来执行， 并且可以为其指定参数。容器启动的时候也可以使用 –entrypoint 来覆盖这里的entrypoint命令12ENTRYPOINT [&quot;/bin/bash&quot;, &quot;start.sh&quot;]CMD [&quot;3&quot;] 存储卷VOLUME定义匿名卷， 容器运行时应该尽量（必须）保证容器存储层不发生写操作， 任何的数据要么保存到卷当中， 要么通过数据库服务保存在其它的地方为了防止用户忘记运行时将需要使用的目录挂载为卷， 我们可以在Dockerfile中定义匿名的卷， 如果用户不挂载那么运行是容器自动挂载当前卷， 使用docker守护进程在本地创建一个目录作为此匿名卷的挂载目录， 这样就可以保证容器的存储层不会被写入了. 所以任何时候我们都应该为一个需要写入数据的容器声明卷， 而在docker中， 最终这个卷被挂载到具体那个位置是未知的，docker提供了一个灵活的处理方式， docker的卷可以使用任何第三方实现的插件， 将其保存在分布式文件系统或者云或者本地系统等等， 这样卷就可以作为一种资源来供容器使用了， 这也swarm kubernetes等工具的实现方式。运行时挂载 -v mydata:/data12VOLUME /dataVOLUME [&quot;/pass1&quot;, &quot;/pass2&quot;] 端口映射EXPOSE 声明运行时容器提供服务的端口， 仅仅只是一个声明而已， 运行是并不会因为这个声明就会开启这个端口的服务， 其实容器也是完全做不到这一点的， 主要有两个意图， 第一帮助镜像使用者理解这个镜像的服务的守护端口， 方便配置映射， 另一个用户则是运行时使用随机端口映射， 也就是docker run -P时， 会自动随机映射EXPOSE的端口， 不管当前端口是否有服务在监听1EXPOSE 9092]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible 教程]]></title>
    <url>%2F2017%2F04%2F26%2Fansible%2F</url>
    <content type="text"><![CDATA[Sensible-playbook 通过密码登录 1ansible-playbook ssh.yml -e &quot;host=dhls user=root&quot; -k 传入参数 1ansible-playbook user.yml -e &quot;host=dhls user=feng passwd=Feng@129.com&quot; 使用tag运行部分task 1ansible-playbook user.yml -e &quot;host=dhls user=feng passwd=123456&quot; -t &quot;passwd&quot; 常用模块1234567ansible tests -m pingansible tests -m setupansible tests -m command -a &apos;uptime&apos;ansible tests -m shell -a &apos;date&apos;ansible tests -m raw -a &apos;yum install python -y&apos; #ansible没有python环境无法执行，raw用户执行不需要模块的低级任务ansible tests -m script -a &apos;&apos;/some/local/script.sh --some-argument 1234&apos; lineinfile​ 此模块确保文件中包含特定行，或使用正则表达式替换现有行。 ​ 如果要更改多行，请参见replace模块； ​ 如果要在文件中插入/更新/删除行块，请查看[blockinfile]。 ​ 对于其他情况，请参见[copy]或[template]模块 community.crypto.openssh_keypair1ansible-galaxy collection install community.crypto ​]]></content>
      <categories>
        <category>ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 网络]]></title>
    <url>%2F2017%2F01%2F12%2Fdocker-network%2F</url>
    <content type="text"><![CDATA[docker四种网络类型 host模式，容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口，Docker Container可以和宿主机一样，使用宿主机的eth0，实现和外界的通信。换言之，Docker Container的IP地址即为宿主机eth0的IP地址， 不需要NET转换，使用–net =bridge指定。 bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。Brdige桥接模式为Docker Container创建独立的网络栈，保证容器内的进程组使用独立的网络环境，实现容器间、容器与宿主机之间的网络栈隔离。另外，Docker通过宿主机上的网桥(docker0)来连通容器内部的网络栈与宿主机的网络栈，实现容器与宿主机乃至外界的网络通信，使用–net =host指定。 在none模式下，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等，使用–net =none模式启动容器 这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信，使用–net =container模式启动容器： 更换默认网桥 安装网桥管理工具 sudo apt install bridge-utils 添加网桥 sudo brctl addbr br0 修改网桥IP地址 sudo ifconfig br0 172.37.0.1 netmask 255.255.0.0 更改docker守护进程配置 12#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --b dr0 -H unix:// 还可以直接更改docker0的IP地址， 如果没有文件直接创建 1234sudo cat /etc/docker/daemon.json &#123; "bip": "172.37.0.1/16"&#125; 也可以直接在docker运行的时候直接加上参数指定网络设备 docker run -itd --name test --network br0 --ip 172.37.0.2 centos:latest`/bin/bash` 跨主机访问 环境，两台独立主机 12host1: 192.168.0.100 netmask 255.255.255.0host2: 192.168.0.101 netmask 255.255.255.0 网桥方式 原理，将主机的物理网卡当做一个网桥工具（更像是一个路由器），对外暴露物理网卡分配IP地址，对内建立局域网络环境， 物理网卡通过NET进行网络转换， 这样也就达到了只有两台物理主机可以进行访问，那么整个网络就是连通的。但是存在一个缺陷， 那就是必须有效的控制网络中的IP地址段。 创建网桥 sudo brctl addbr br0 由于使用网桥管理工具添加网桥后，配置IP地址会在下次启动的时候自动失效， 所以我们需要将网络配置信息写入到文件当中。 12345678910#vi /etc/network/interfacesauto loiface lo inet loopbackauto br0iface br0 inet static #也可以直接使用动态地址 address 192.168.0.100netmask 255.255.255.0gateway 192.168.0.1bridge_ports eth0 #将主机网卡绑定到网桥上面 设置docker启动参数, 限定docker网络可以分配的ip地址段 12#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --b dr0 --fixed-cidr 192.168.0.128/26 -H unix:// 两台机器使用相同配置， 但是控制不同的IP段 weave方式 参考：极客学院课程]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
