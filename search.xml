<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux-httpie]]></title>
    <url>%2F2019%2F03%2F17%2Flinux-httpie%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[linux-openvpn]]></title>
    <url>%2F2019%2F03%2F15%2Flinux-openvpn%2F</url>
    <content type="text"><![CDATA[openvpn服务端安装 直接使用搬瓦工的openvpn安装 安装obfsproxy， 服务端和客户端完全一样 修改openvpn服务端配置 123456789101112131415161718port 3737proto tcpdev tunserver 10.10.10.0 255.255.255.0ifconfig-pool-persist ipp.txtca ca.crtcert server.crtkey server.keydh dh2048.pempush &quot;route 10.10.10.0 255.255.255.0&quot;push &quot;redirect-gateway&quot;comp-lzokeepalive 20 120ping-timer-rempersist-tunpersist-keygroup nobodydaemon 启动openvpn和obfsproxy 1obfsproxy obfs3 --dest=127.0.0.1:3737 server 0.0.0.0:8088 &amp; openvpn 说明 openvpn主要是用来解决虚拟局域网的功能和高度加密功能 obfsproxy混淆tcp，用于讲上层协议的包封装成obfsproxy的包 整个流程 openvpn client –&gt; obfsproxy client –&gt; wall –&gt; obfsproxy server –&gt; openvpn server –&gt; others openvpn的安装建议直接使用搬瓦工的自动安装就可以了， 并下载它的 Download key files， 再创建客户端密钥。 openvpn安装完成之后需要每个客户端需要添加不同的密钥， 这样每个客户端才分配不同的IP地址，以便搭建虚拟局域网, 路径 /etc/openvpn 更改变量 vi vars 123456export KEY_COUNTRY=&quot;CN&quot;export KEY_PROVINCE=&quot;HUBEI&quot;export KEY_CITY=&quot;wuhan&quot;export KEY_ORG=&quot;tepia&quot;export KEY_EMAIL=&quot;316488140@qq.com&quot;export KEY_OU=&quot;tepia&quot; 生成客户端配置 123cd easy-rsasource ./vars./build-key newclient 复制客户端密钥文件到客户端 1scp root@地址:/etc/openvpn/easy-rsa/keys/newclient.* /etc/openvpn/client 下载最原始的客户端配置文件，从搬瓦工官网下载 openvpn-keys.zip 1234mv openvpn-keys.zip /etc/openvpn/clientunzip openvpn-keys.zipls #查看所有文件ca.crt ca.key client1.crt client1.csr client1.key newclient.crt newclient.csr newclient.key host.localdomain.ovpn openvpn-keys.zip 更改 host.localdomain.ovpn文件 12cert newclient.crtkey newclient.key 安装openvpn后需要更改openvpn走obfsproxy通道, 配置说明 12345678910111213clientdev tunproto tcpremote 127.0.0.1 9999 #本地obfsproxy的地址resolv-retry infinitecomp-lzoca ca.crtcert newclient.crtkey newclient.keyroute-delay 2route-method exeredirect-gateway def1verb 3 obfsproxy 说明 安装obfsproxy，最好是先将python升级到2.7最新版本 12345git clone https://github.com/dounine/obfsproxy.gitcd obfsproxytar -xzf obfsproxy-0.2.13.tar.gzcd obfsproxy-0.2.13python setup.py install 说明：可能需要解决python相关的问题 安装 1234wget https://twistedmatrix.com/Releases/Twisted/18.9/Twisted-18.9.0.tar.bz2tar -jxvf Twisted-18.9.0.tar.bz2cd Twisted-18.9.0python setup.py install 启动 1sudo obfsproxy obfs3 --dest=服务端地址:服务端obfsproxy端口 client 127.0.0.1:9999 启动openvpn和obfsproxy12345obfsproxy obfs3 --dest=服务端地址:服务端obfsproxy端口 client 127.0.0.1:9999 &amp;cd /etc/openvpn/client/openvpn --config host.localdomain.ovpn &amp;route del -net 128.0.0.0/1route del -net 0.0.0.0/1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 集成hive]]></title>
    <url>%2F2019%2F01%2F23%2Fhadoop-spark-hive%2F</url>
    <content type="text"></content>
      <categories>
        <category>hadoop</category>
        <category>spark</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn 动态资源分配]]></title>
    <url>%2F2019%2F01%2F23%2Fhadoop-yarn%2F</url>
    <content type="text"><![CDATA[spark动态资源分配 开启External shuffle service Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle数据，给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。通过该服务来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他Executor的任务运行。 NodeManager中启动External shuffle Service 在yarn-site.xml中添加如下配置 12345678910&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 可选配置--&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 拷贝${SPARK_HOME}/lib/spark-${SPARK_VERSION}-yarn-shuffle.jar 到${HADOOP_HOME}/share/hadoop/yarn/lib/目录下, 并重启NodeManager进程，也就启动了External shuffle Service。 Spark应用中使用External shuffle Service， 在spark-defaults.conf中添加配置或者直接使用–conf配置 12spark.shuffle.service.enabled truespark.shuffle.service.port 7337 注意： 如果 yarn.nodemanager.aux-services配置项已存在，则在 value 中添加park_shuffle，且用逗号和其他值分开。 spark.shuffle.service.port的值需要和上面yarn-site.xml中的值一样， 可以都直接使用默认值。 spark程序开启动态资源分配 1234567spark.dynamicAllocation.enabled truespark.dynamicAllocation.minExecutors 1spark.dynamicAllocation.initialExecutors 1spark.dynamicAllocation.maxExecutors 9spark.dynamicAllocation.executorIdleTimeout 60spark.dynamicAllocation.cachedExecutorIdleTimeout 60spark.executor.cores 2 注意： cores尽量设置为 &lt;= 3, 也就是说通常都是增大maxExecutors, 减小cores yarn 资源分配建议 设置Executor最大可用cores 和 memery， 这样能防止单个机器压力过大， 对于总的物理cores和memery来说需要减去操作系统以及hbase等其它的存储需要消耗的资源才为yarn最大可用资源， 通常需要预留生效资源的10%左右]]></content>
      <categories>
        <category>hadoop</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-kerberos]]></title>
    <url>%2F2019%2F01%2F13%2Fhadoop-kerberos%2F</url>
    <content type="text"><![CDATA[kerberos客户端配置 krb5.conf配置选项参考： https://linux.die.net/man/5/krb5.conf 配置kerberos多用户ticket缓存名字 12#vi /etc/profiledefault_ccache_name = DIR:/tmp/kerberos 说明： 也可以设置环境变量达到相同的效果export KRB5CCNAME=DIR:/tmp/kerberos kerberos客户端在任意时刻只能有一个票证可用， 所以不管如何配置都不可能达到一次配置一次认证就可以了，而是认证之后可以进行多次切换不同的认证 多个kerberos的krb5.conf合并 通常情况每个kerberos服务器有一个krb5.conf文件， 如果有多个kerberos服务器需要认证那么通常会比较麻烦，需要拷贝配置文件，所以通常情况下我们可以讲这些文件合并一个文件。通常情况下只需要合并realms 就可以了，如下： 123456789[realms] ILIVOO.COM = &#123; admin_server = sandbox-hdp.hortonworks.com kdc = sandbox-hdp.hortonworks.com &#125; EXAMPLE.COM = &#123; admin_server = dev1 kdc = dev1 &#125; 多个keytab文件合并 1234#ktutilktutil: rkt /etc/security/keytabs/hdfs.keytabktutil: rkt /etc/security/keytabs/hbase.keytab ktutil: wkt /etc/krb5.keytab kerberos客户端工具使用 查看keytab文件中用户 klist -kt /etc/krb5.keytab 认证 12kinit -k admin/admin@ILIVOO.COMkinit -k admin/admin@EXAMPLE.COM 显示认证 显示当前认证 klist 显示所以认证 klist -A 切换认证 kswitch -p admin/admin@ILIVOO.COM 销毁认证 销毁当前认证 kdestroy 注意销毁当前认证， 并不会自动选择一个当前认证 销毁所有认证 kdestroy -A 更新认证 kinit -R kerberos服务端工具使用说明： headless keytab与没有headless的keytab的区别， 主要是headless keytab可以在任何机器上使用，而没有headless的keytab只能在特定的服务器使用， 也就是说它们是服务器绑定的， 如： headless keytab 名称是 `hdfs-ilivoo@ILIVOO.COM， 而没有headless keytab是nn/hdp1.ilivoo.com@ILIVOO.COM` kerberos管理员账户： `*/admin@ILIVOO.COM(可以通过/var/kerberos/krb5kdc/kadm5.acl `指定) 添加Principal 12kadmin.localaddprinc -randkey admin-ilivoo@ILIVOO.COM 导出keytab文件 123cd /etc/security/keytabkadmin.localktadd/xst -k admin.headless.keytab admin-ilivoo 合并keytab文件 12345cd /etc/security/keytabktutilrkt admin.headless.keytabrkt hdfs.headless.keytabwkt all.headless.keytab ambari、kerberos、service、host 权限 对于ambari来说它是一个集群的管理着， 它有自己权限管理， 通常分为集群管理者、集群使用者、服务管理者、服务操作者、服务开发者等等。 并且它的权限用户通常也是可以和host的权限进行对应上的。如：在主机上创建了 admin 用户， 并且在ambari中创建了admin用户， 并且把admin用户配置成为集群管理者。 kerberos主要是用来验证某个人就是自己所声称的那个人， 所以通常情况下也是和主机的用户对于的， 也就是说根据需要是否在kerberos中创建主机用户的principal。通常情况下， service使用principal验证能够访问自己的用户， 那么对应用户访问的时候就必须为自己获取票证。 通常来说service是为了验证的话才需要为自己开启权限验证， 并且指定只有某种用户才能访问自己， 如果其它的服务或者客户端需要访问自己， 那么就必须获得票证。并且通常来说service有自己权限验证，可以将其授权给某个用户， 也就是说想要访问service必须先证明你就是自己声称的那个人， 这样就表示通过了service的kerberos验证， 再就是service有自己的权限验证(acl)， service必须为这个用户授权之后这个用户才能访问这个服务。通常情况下服务与服务之间的kerberos验证是非常严格的， 如： DataNode想要连接到NameNode那么必须有 `nn/host@ILIVOO.COM, 而服务的使用者却比较宽泛点， 如hdfs用户想要访问集群， 那么通常只需要hdfs-ilivoo@ILIVOO.COM`， 甚至其它的用户也可以创建文件等等， 只是hdfs用户通常被指定为管理用户，其它用户需要hdfs用户为其授权。 host权限主要是用来控制对主机的访问权限， 但是通常情况下来说host权限也与其它的权限对应上， 方便服务的开发与管理。 通过上面的分析， 通常情况下我们使用这些复杂的权限的时候会非常小心， 并且规划好权限对于我们的开发至关重要。如下案例： 运维人员， 通常都是需要对集群进行管理， 并监控集群的状态， 通常情况可以简单分为两种运维人员， 集群管理员， 集群监测员，创建权限如下： 集群管理员： host中创建 cluster admin 用户(cadmin), ambari中创建用户(cadmin)用户，并为其设置为集群管理员角色。 集群监测源： host中创建 cluster view 用户(cview), ambari中创建(cview)用户， 并为其设置为集群操作员角色。 开发人员， 通常对服务进行启动停止， 配置修改， 对某些服务进行访问， 通常不同的开发人员有不用的权限。如下案例： spark开发人员： host中创建开发用户(feng), ambari中创建(feng)，并为其设置集群服务操作员角色， 在kerberos中添加票证， 并导出keytab文件给用户。 为用户配置hdfs权限， yarn权限， spark权限，hbase权限， kafka权限， hive权限等等。 注意： hdfs、yarn、spark、hbase、kafka、hive等为feng添加权限的时候， 通常需要先使用kinit登录到自己的权限， 再讲权限设置给feng用户。 如： hbase 为feng用户添加权限 123kinit -kt /var/kerberos/keytab/hbase.headless.keytab hbase-ilivoo@ILIVOO.COMhbase shellgrant &apos;feng&apos;, &apos;RWCA&apos; #授权feng用户有rwca权限 当然： 现在情况有所变化， hdfs、yarn、hbase、kafka、hive等都可以使用ranger为其配置权限， 所以如果这些组件使用了ranger插件， 那么就必须使用ranger为其配置。 建议能够使用hdfs自己的权限位， 就不要使用ranger来管理。 kafka安装kerberos 参考 https://www.cnblogs.com/dongxiao-yang/p/7131626.html 服务端修改基本配置 12listeners=SASL_PLAINTEXT://:9092 --修改listenerssasl.kerberos.service.name=kafka --添加kerberos.name 启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用, 建立 client.properties文件 123security.protocol=SASL_PLAINTEXTsasl.kerberos.service.name=kafkasasl.mechanism=GSSAPI 新命令如下： 1234567所以新命令的使用方式为bin/kafka-consumer-groups.sh --bootstrap-server hdp1.ilivoo.com:6667 --list --command-config client.propertiesbin/kafka-console-producer.sh --broker-list hdp1.ilivoo.com:6667 --topic dxTT --producer.config client.properties过期，需要重新申请或者renew。Ticketbin/kafka-console-consumer.sh --bootstrap-server hdp1.ilivoo.com:6667 --topic dxTT --consumer.config client.properties --from-beginning kerberos遇到的问题 kinit -R 无法续约，这是因为kerberos的每个principal都可以指定超过此时间则ticket就会过期，需要重新申请或者renew有效时间和刷新时间， 默认添加principal的时候并没有指定它的刷新时间和有效时间，所以就使用了服务器默认配置的时间 kerberos Server上的/var/kerberos/krb5kdbc/kdc.conf中的max_life kadmin.local 1234modprinc -maxlife &quot;1 week&quot; feng@ILIVOO.COMmodprinc -maxrenewlife &quot;1 week&quot; feng@ILIVOO.COMmodprinc -maxlife &quot;1 week&quot; krbtgt/ILIVOO.COM@ILIVOO.COMmodprinc -maxrenewlife &quot;1 week&quot; krbtgt/ILIVOO.COM@ILIVOO.COM 注意：krbtgt表示服务端的tgt必须设置， 设置完成需要重新kinit kerberos 的票据的生命周期由lifetime和renewlifetime决定， 当生命周期超过lifetime则ticket就会过期，需要重新申请或者renew，如果重新申请非常容易理解， 直接重新申请就可以了。如果需要renew则首先要开启renew功能， 由上一个问题解决。通常情况下来说，renewlifetime会大于lifetime，如：lifetime=2d，renewlifetime=7d。而这两个时间由五个基本的时间决定： 最基本由kerberos Server上的/var/kerberos/krb5kdbc/kdc.conf中的max_life 和 max_renewable_life决定 123456[realms] ILIVOO.COM = &#123; max_life = 2d max_renewable_life = 7d ... &#125; 内置principal krbtgt的maxmum ticket life 和 renew life,可在kadmin命令下执行getprinc命令查看 使用的rincipal的maximum tiket life 和 renew life，可在kadmin命令下用getprinc命令查看 再就是kerberos client上/etc/krb5.conf的ticket_lifetime 和 renew_lifetime 1234[libdefaults] ticket_lifetime = 2d renew_lifetime = 7d ... kinit 参数后面指定的时间, 如： kinit -l 2d -r 7d my_principal 注意：上面五个生命周期依次往上逐渐取最小 kerberos spark集群连接hbase、kafka， 由于spark以及处理了kerberos认证问题，在spark的Driver启动的时候spark获取kerberos认证， 并将认证打包成代理Token存放在HDFS上， 当启动Executor的时候会带上这个Token用于Executor上的验证。当Token快要失效的时候Spark Driver会再次去刷新认证， 并讲认证信息告诉Executor，从而保证整个过程中长时间运行。 注意：hbase的classpath是一个麻烦的事情， 因为通常情况下如果简单指定hbase-client会导致hbase 在Executor中没有认证的现象，所以必须讲hbase-server依赖带上。]]></content>
      <categories>
        <category>hadoop</category>
        <category>kerberos</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-network]]></title>
    <url>%2F2019%2F01%2F12%2Fdocker-network%2F</url>
    <content type="text"><![CDATA[docker四种网络类型 host模式，容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口，Docker Container可以和宿主机一样，使用宿主机的eth0，实现和外界的通信。换言之，Docker Container的IP地址即为宿主机eth0的IP地址， 不需要NET转换，使用–net =bridge指定。 bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。Brdige桥接模式为Docker Container创建独立的网络栈，保证容器内的进程组使用独立的网络环境，实现容器间、容器与宿主机之间的网络栈隔离。另外，Docker通过宿主机上的网桥(docker0)来连通容器内部的网络栈与宿主机的网络栈，实现容器与宿主机乃至外界的网络通信，使用–net =host指定。 在none模式下，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等，使用–net =none模式启动容器 这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信，使用–net =container模式启动容器： 更换默认网桥 安装网桥管理工具 sudo apt install bridge-utils 添加网桥 sudo brctl addbr br0 修改网桥IP地址 sudo ifconfig br0 172.37.0.1 netmask 255.255.0.0 更改docker守护进程配置 12#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --b dr0 -H unix:// 还可以直接更改docker0的IP地址， 如果没有文件直接创建 1234sudo cat /etc/docker/daemon.json &#123; "bip": "172.37.0.1/16"&#125; 也可以直接在docker运行的时候直接加上参数指定网络设备 docker run -itd --name test --network br0 --ip 172.37.0.2 centos:latest`/bin/bash` 跨主机访问 环境，两台独立主机 12host1: 192.168.0.100 netmask 255.255.255.0host2: 192.168.0.101 netmask 255.255.255.0 网桥方式 原理，将主机的物理网卡当做一个网桥工具（更像是一个路由器），对外暴露物理网卡分配IP地址，对内建立局域网络环境， 物理网卡通过NET进行网络转换， 这样也就达到了只有两台物理主机可以进行访问，那么整个网络就是连通的。但是存在一个缺陷， 那就是必须有效的控制网络中的IP地址段。 创建网桥 sudo brctl addbr br0 由于使用网桥管理工具添加网桥后，配置IP地址会在下次启动的时候自动失效， 所以我们需要将网络配置信息写入到文件当中。 12345678910#vi /etc/network/interfacesauto loiface lo inet loopbackauto br0iface br0 inet static #也可以直接使用动态地址 address 192.168.0.100netmask 255.255.255.0gateway 192.168.0.1bridge_ports eth0 #将主机网卡绑定到网桥上面 设置docker启动参数, 限定docker网络可以分配的ip地址段 12#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --b dr0 --fixed-cidr 192.168.0.128/26 -H unix:// 两台机器使用相同配置， 但是控制不同的IP段 weave方式 参考：极客学院课程]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 存储程序]]></title>
    <url>%2F2019%2F01%2F04%2Fmysql-procedure%2F</url>
    <content type="text"><![CDATA[存储程序” MySQL允许通过触发器、存储过程、函数的形式来存储代码， 并且可以在定时任务中存放代码，这个定时任务也被成为”事件”。 一般来说，存储代码是一种很高的共享和复用代码的方法。 存储代码的优点： a. 在服务器内部运行，离数据最近，另外在服务器上执行还可以节省带宽和网络延迟。 b. 代码重用。 c. 可以简化代码的维护和版本更新。 d. 可以帮助提升安全，例如提供更细粒度的权限控制。如银行资金转移。 f. 服务器可以换成存储过程的执行计划。 g. 因为是在服务器端部署的，所以备份、维护都可以在服务器端完成。 h. 它可以在应用程序和数据库开发人员之间更好的分工。 存储代码的缺点： a. MySQL本身没有提供好用的开发和调试工具。 b. 较之应用程序的代码，存储代码效率要稍微差些。 例如：函数有限，无法编写复杂字符串维护功能。 c. 存储代码可能会给应用程序代码的部署带来额外的复杂性。 d. 可能有安全隐患，最好加密。 e. 存储过程会给数据库服务器增加额外的压力，而数据库服务器的扩展性相比应用服务器要差很多。 f. MySQL不能控制存储过程的资源消耗，所以在存储过程中的一个小错误，可能直接把服务器拖死。 g. 调试MySQL的存储过程是一件很困难的事情。 存储过程和函数 我们通常会希望程序越小、越简单越好。希望将更复杂的处理逻辑交给上层的应用实现，通常这样会使代码更易读、易维护，也会更灵活。这样做也会让你拥有更多的计算资源，潜在的还会让你拥有更多的缓存资源。 不过，对于某些操作，存储过程比其他的实现要快的多–特别是一个存储过程调用可以代替很多小查询的时候。如果查询很小，相比这个查询执行的成本，解析和网络开销就变得非常明显。 存储过程优点 存储代码，节省很多网络开销，如循环insert 封装核心业务 存储过程可以回传值，并可以接受参数。 存储过程缺点 存储过程无法使用 SELECT 指令来运行，因为它是子程序，与查看表，数据表或用户定义函数不同。 存储过程，往往定制化于特定的数据库上，因为支持的编程语言不同。当切换到其他厂商的数据库系统时，需要重写原有的存储过程。 存储过程的调试比较麻烦， 并且出错后对系统影响较大 基于语句的复制通常情况下来说需要在复制库中执行 存储过程与函数的区别 本质上没区别，执行的本质都一样。 函数只能返回一个变量的限制，而存储过程可以返回多个。 函数是可以嵌入在sql中使用的,可以在select中调用，而存储过程要让sql的query 可以执行， 需要把 mysql_real_connect 的最后一个参数设置为CLIENT_MULTI_STATEMENTS。 函数限制比较多，比如不能用临时表，只能用表变量．还有一些函数都不可用等等．而存储过程的限制相对就比较少。 一般来说，存储过程实现的功能要复杂一点，而函数的实现的功能针对性比较强。存储过程，功能强大，可以执行包括修改表等一系列数据库操作；用户定义函数不能用于执行一组修改全局数据库状态的操作。 对于存储过程来说可以返回参数，如记录集，而函数只能返回值或者表对象。函数只能返回一个变量；而存储过程可以返回多个。存储过程的参数可以有IN,OUT,INOUT三种类型，而函数只能有IN类，存储过程声明时不需要返回类型，而函数声明时需要描述返回类型，且函数体中必须包含一个有效的RETURN语句。 存储过程，可以使用非确定函数，不允许在用户定义函数主体中内置非确定函数。 存储过程一般是作为一个独立的部分来执行（ EXECUTE 语句执行），而函数可以作为查询语句的一个部分来调用（SELECT调用），由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字的后面。 SQL语句中不可用存储过程，而可以使用函数。 当存储过程和函数被执行的时候，SQL Manager会到procedure cache中去取相应的查询语句，如果在procedure cache里没有相应的查询语句，SQL Manager就会对存储过程和函数进行编译。 Procedure cache中保存的是执行计划 (execution plan) ，当编译好之后就执行procedure cache中的execution plan，之后SQL SERVER会根据每个execution plan的实际情况来考虑是否要在cache中保存这个plan，评判的标准一个是这个execution plan可能被使用的频率；其次是生成这个plan的代价，也就是编译的耗时。保存在cache中的plan在下次执行时就不用再编译了。 触发器 触发器可以让你在执行INSERT、UPDATE或者DELETE的时候，执行一些特定的操作。可以在MySQL中指定是在SQL语句执行前触发还是在执行后触发。触发器本身没有返回值，不过它们可以读取或者改变触发SQL语句所影响的数据。所以，可以使用触发器实现一些强制限制，或者而某些业务逻辑，否则，就需要在应用程序中实现这些逻辑。 因为使用触发器可以减少客户端和服务器之间的通信，所以触发器可以简化应用逻辑，还可以提高性能。另外，还可以用于自动更新反范式化数据或者汇总表数据。 MySQL触发器实现非常简单，所以功能也有限。特别需要注意一下几点： a. 对应每一个表的每一个事件，最多只能定义一个触发器(换句话说，不能在AFTER INSERT上定义两个触发器) b. MySQL只支持”基于行的触发” – 也就是说，触发器始终是针对一条记录的，而不是针对整个SQL语句的。如果变更的数据集非常大的话，效率会很低。 触发器本身的限制： a. 触发器可以掩盖服务器背后的工作。例如SQL影响的记录数翻一倍。 b. 触发器的问题很难排除 c. 触发器可能导致死锁或所等待。 在InnoDB表上的触发器是在同一个事务中完成的，所以它们执行的操作是原子的，原子操作和触发器操作会同时失败或者成功。不过，如果在InnoDB表上的触发器去检查数据的一致性，需要特别小心MVCC，稍不小心，可能会获得错误的结果。 可以使用触发器记录数据变更日志。 定时任务 定时任务类似于Linux的定时任务，不过是完全在MySQL内部实现的。你可以创建事件，执行MySQL在某个时候执行一段SQL代码，或者每隔一段时间执行一段SQL代码。通常，我们会把复杂的SQL都封装到一个存储过程中，这样事件在执行的时候只需要做一个简单的CALL调用。 如果一个定时事件执行需要很长的时间，那么有可能会出现这样的情况，即前面一个事件还未执行完成，下一个时间点的事件又开始了。MySQL本身不会防止这种并发，所以需要用户在自己编写这种情况下防并发代码。你可以使用函数GET_LOCK()来确保当前总是有一个事件在被执行。 虽然事件的执行是和连接无关的，但是它仍然是线程级别的。MySQL中有一个事件调度线程，必须在MySQL配置文件中设置，或者使用下面的命令来设置。SET GLOBAL event_scheduler :=1 在存储过程中保留注释：/! xxxxxxxxxxxxxxxxxxxx / MySQL在服务器端提供只读的、单向的游标，而且只能在存储过程或者更底层的客户端API中使用。因为MySQL游标中指向的对象都是存储在临时表中而不是实际查询到的数据，所以MySQL游标总是只读的。它可以逐行指向查询结果，然后让程序做进一步处理。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql exists与in的区别]]></title>
    <url>%2F2019%2F01%2F03%2Fmysql-exist-in%2F</url>
    <content type="text"><![CDATA[exists exists对外表用loop逐条查询，每次查询都会查看exists的条件语句，当 exists里的条件语句能够返回记录行时(无论记录行是的多少，只要能返回)，条件就为真，返回当前loop到的这条记录，反之如果exists里的条 件语句不能返回记录行，则当前loop到的这条记录被丢弃，exists的条件就像一个bool条件，当能返回结果集则为true，不能返回结果集则为 false，如下： 1select * from user where exists (select 1); 对user表的记录逐条取出，由于子条件中的select 1永远能返回记录行，那么user表的所有记录都将被加入结果集，所以与 select * from user;是一样的，又如下 1select * from user where exists (select * from user where userId = 0); 注意： 可以知道对user表进行loop时，检查条件语句(select * from user where userId = 0),由于userId永远不为0，所以条件语句永远返回空集，条件永远为false，那么user表的所有记录都将被丢弃 not exists与exists相反，也就是当exists条件有结果集返回时，loop到的记录将被丢弃，否则将loop到的记录加入结果集 总的来说，如果A表有n条记录，那么exists查询就是将这n条记录逐条取出，然后判断n遍exists条件 in in查询相当于多个or条件的叠加，这个比较好理解，比如下面的查询 select * from user where userId in (1, 2, 3); 等效于 select * from user where userId = 1 or userId = 2 or userId = 3; not in与in相反，如下 select * from user where userId not in (1, 2, 3); 等效于 select * from user where userId != 1 and userId != 2 and userId != 3; 总的来说，in查询就是先将子查询条件的记录全都查出来，假设结果集为B，共有m条记录，然后在将子查询条件的结果集分解成m个，再进行m次查询 值得一提的是，in查询的子条件返回结果必须只有一个字段，例如 select * from user where userId in (select id from B); 而不能是 select * from user where userId in (select id, age from B); 而exists就没有这个限制 exists和in的性能 考虑如下SQL语句 1: select from A where exists (select from B where B.id = A.id); 2: select * from A where A.id in (select id from B); 查询1.可以转化以下伪代码，便于理解 123456for ($i = 0; $i &lt; count(A); $i++) &#123; $a = get_record(A, $i); #从A表逐条获取记录 if (B.id = $a[id]) #如果子条件成立 $result[] = $a;&#125;return $result; 大概就是这么个意思，其实可以看到,查询1主要是用到了B表的索引，A表如何对查询的效率影响应该不大 假设B表的所有id为1,2,3,查询2可以转换为 select * from A where A.id = 1 or A.id = 2 or A.id = 3; 这个好理解了，这里主要是用到了A的索引，B表如何对查询影响不大 再看not exists 和 not in 1: select from A where not exists (select from B where B.id = A.id); 2: select * from A where A.id not in (select id from B); 看查询1，还是和上面一样，用了B的索引, 而对于查询2，可以转化成如下语句 select * from A where A.id != 1 and A.id != 2 and A.id != 3; 可以知道not in是个范围查询，这种!=的范围查询无法使用任何索引,等于说A表的每条记录，都要在B表里遍历一次，查看B表里是否存在这条记录故not exists比not in效率高 mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in： 例如：表A（小表），表B（大表） 1: select * from A where cc in (select cc from B) 效率低，用到了A表上cc列的索引； select * from A where exists(select cc from B where cc=A.cc) 效率高，用到了B表上cc列的索引。 相反的 2: select * from B where cc in (select cc from A) 效率高，用到了B表上cc列的索引； ​ select * from B where exists(select cc from A where cc=B.cc) 效率低，用到了A表上cc列的索引。 not in 和not exists 如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 in 与 =的区别 select name from student where name in (‘zhang’,’wang’,’li’,’zhao’); 与 select name from student where name=’zhang’ or name=’li’ or name=’wang’ or name=’zhao’]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 查询顺序]]></title>
    <url>%2F2019%2F01%2F03%2Fmysql-select%2F</url>
    <content type="text"><![CDATA[笛卡尔乘积 使用join执行笛卡尔乘积， 并且使用on来对笛卡尔乘积之前进行过滤操作, 这也称作显示的连接操作 使用from连接多个表执行笛卡尔乘积， 并且使用where进行过滤， 但是由于先执行笛卡尔乘积再过滤， 所以有效率问题， 这也称作隐式的连接操作 定义变化的表 可以使用(select @rank := 0) as ranker 来定义一张只有一个字段@rank的表ranker， 并且@rank字段也会作为用户自定义字段而存在， 所以相当于对@rank进行初始化。 笛卡尔乘积给一个表添加一个字段， 并且该字段自增1select p.id, p.name, @rank := @rank + 1 as rank from person as p join (select @rank := 0) as ranker; 查询步骤 查询操作是关系数据库中使用最为频繁的操作，也是构成其他SQL语句（如DELETE、UPDATE）的基础。查询处理的顺序如下: 123456789(7) SELECT (8) DISTINCT (1) FROM (3) JOIN (2) ON (4) WHERE (5) GROUP BY (6) HAVING (9) ORDER BY (10) LIMIT 1）FROM：对FROM子句中的所有的表一次左右执行笛卡儿积（Cartesian product），产生虚拟表VT1。 2，3）JOIN和ON应该是一个整体作为一个连接操作： 如果存在ON就先对VT1和JOIN后面的表进行过滤， 并且执行连接操作, 如果还有JOIN和ON继续接着进行连接， 如果此时又出现表则表示FROM后还有表进行笛卡尔乘积， 定义所有的这一连串操作产生VT3mysql不支持全连接， 并且它的连接与外连接是等价的， 跨连接与内连接是等价的 4）WHERE：对虚拟表VT3应用WHERE过滤条件，只有符合的记录才被插入虚拟表VT4中。此时数据还没有分组，所以不能在where中出现对统计的过滤 where: &lt;，&lt;=，=，!=或者&lt;&gt;，&gt; ，&gt;=， like (% 通配任意字符, _通配一个字符),exist(判空), beteeen(在某范围内)，any/som(集合中任意元素), all(集合中所有的元素), in(在某集合内), not ! 逻辑非, or || 逻辑或, and &amp;&amp; 逻辑与 5）GROUP BY：根据GROUP BY子句中的列，对VT4中的记录进行分组操作，产生VT5。在GROUP BY阶段，数据库认为两个NULL值是相等的，因此会将NULL值分到同一个分组中。 6）CUBE|ROLLUP：对表VT5进行CUBE或ROLLUP操作，产生表VT6。 7）HAVING：对虚拟表VT6应用HAVING过滤器，只有符合的记录才被插入虚拟表VT7中。count(expr) 会返回expr不为NULL的行数，count(1)、count(*)会返回包括NULL值在内的所有数量 8）SELECT：执行SELECT操作，选择指定的列，插入到虚拟表VT8中。 9）DISTINCT：去除重复数据，产生虚拟表VT9。 10）ORDER BY：将虚拟表VT9中的记录按照进行排序操作，产生虚拟表VT10。如果不指定排序，数据并非总是按照主键顺序进行排序的。NULL被视为最小值 11）LIMIT：取出指定行的记录，产生虚拟表VT11，并返回给查询用户。LIMIT n, m的效率是十分低的,一般可以通过在where条件中指定范围来优化 where id&gt; ? limit 10 12) UNION [ALL] (联合两个查询的结果), INTERSECT(交集)， EXCEPT(差集) 注意： 从上面的执行流程可以看出， 上面有很多过滤操作， 如： on where having distinct limit, 而每一个过滤操作都依赖前面的执行结果， 所以我们在前面的结果中如果能够尽量过滤掉不需要的结果那么对于后面的结果操作会简单很多。 所有的这些地方使用表的地方都可以使用子查询来建立新表， 如果数据量比较大尽量不适用子查询， 使用连接来优化。 对于要查询的结果来说， 从上面可以看到， 到第八步才开始进行真正的投影的， 所以不用担心查询出来结果信息。 可以看到limit是在最后执行的， 所以如果前期查询出大量结果， 再进行limit n,m可能会造成性能问题， 因为在limit之前以及进行了投影动作， 也就是所有的结果都以及查询出来了，如果此时我们只需要使用索引先搜索出满足limit的结果的主键id， 再通过主键id去查询结果， 那么此时会优化速度， 因为完全不用先查询出结果， 只需要最终去拿结果就可了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux awk]]></title>
    <url>%2F2019%2F01%2F03%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[awk的基本语法 awk -F&#39; &#39; &#39;BEGAIN {} {Pattern {Action} Pattern {Action} ...} END {}&#39; Pattern为一个判断条件， 如 $2&gt;3 &amp;&amp; $3 ~ /^Hel.*/， 基本比较符号和 ~/!~ 正则匹配 Action为一个动作， 如： print $1, $3 BEGAIN {} 只会在程序执行前执行一次 END {} 只会在程序执行后执行一次 {Pattern {Action} Pattern {Action} …} 会在每一行都会去循环执行 注意： Pattern也可以加上if作为一个判断， 其实awk只是简化了而已， 并且不用写中间的对每一行进行循环 awk存在内置常量， 不需要$来引用， NF字段的数量（列）， NR当前行数（awk读到第几行）， FILENAME正在处理的文件名字 字符串处理 ~/!~ 使用正则表达式匹配 substr( $2,1,length($2)-3) substr($2,length($2)-2) match(buffer,/[0-9]+.c/) split( “hello:world”, array, /[Pp]/) 分组统计​ 分组统计分组个数， 分组总数， 分组平均， 分组最小， 分组最大 按照协议进行分组统计 如： 游戏协议处理时间或者sql语句查询时间， 格式： 时间 协议/查询 时间 123456789102012 1 12012 2 22013 4 42014 3 02014 3 52015 2 22016 3 32017 2 12018 1 42018 4 1 1awk -F' ' '&#123;num[$2]+=1; sum[$2]+=$3; if(min[$2]=="")&#123;min[$2]=$3;max[$2]=$3;&#125; if(min[$2]&gt;$3)&#123;min[$2]=$3;&#125; if($3&gt;max[$2])&#123;max[$2]=$3;&#125;&#125; END &#123;for(i in num) print i, sum[i], num[i], sum[i]/num[i], min[i], max[i]&#125;' log.log 按照时间进行分组统计，格式如上 1awk -F' ' '&#123;num[$1]+=1; sum[$1]+=$3; if(min[$1]=="")&#123;min[$1]=$3;max[$1]=$3;&#125; if(min[$1]&gt;$3)&#123;min[$1]=$3;&#125; if($3&gt;max[$1])&#123;max[$1]=$3;&#125;&#125; END &#123;for(i in num) print i, sum[i], num[i], sum[i]/num[i], min[i], max[i]&#125;' log.log 注意： 从这里可以看到awk中对于未定义的对象使用 UNDEFINE == “” awk允许使用字符串最为数组下标， 所以这里协议可以是字符串协议， 并且使用数组前不需要进行声明 可以看到数组遍历非常简单， for(i in num) print num[i] awk 添加参数， 如， 在每一行前面添加文件名字： 12file_name=hello.logawk -F'|' '&#123;print file_name, $1&#125;' file_name=“$file_name” 添加换行1echo "a,b" | awk -F "," '&#123;for(i=1;i&lt;=NF;i++) &#123;print $i&#125;&#125;']]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell详解]]></title>
    <url>%2F2019%2F01%2F03%2Flinux-shell%2F</url>
    <content type="text"><![CDATA[shell属于弱类型语言, 其默认的变量类型为字符串类型shell命令类型 type命令判断一个命令是否是shell的内置命令 type -a command 打印出command的执行优先级, 因为shell命令可能被 alise 重新命名, function添加名称, 或者是外部命令取相同的名称, 所以必须要一个执行顺序, 顺序如： alias –&gt; function –&gt; builtin –&gt; program 后 which查看命令所在的位置, 如果内置命令不显示位置, whereis也可以用来显示位置, 默认过滤内部命令直接外部去找 shell中有一些命令可能与外部命令有相同的名称, 默认shell会调用内置的命令, 因为这可以避免fork/exec shell执行某个命令后, 如果是内部命令(其实就像是内置的方法一样), shell程序直接调用自己的一段方法就可以了, 如果是外部命令, 那么shell首先fork出一个进程, 再使用exec家族中的方法来执行外部命令, exec家族命令中可以选择是否讲父进程中的环境变量传递给子进程. 启动顺序 引导程序 init进程(初始化操作系统 fork exec) login进程(用于校验用户信息, 并执行exec) bash进程(解释用户输入,并执行fork和exec) 用户进程, 整个启动顺序 命令返回值 所有的命令输出或者echo命令的输出, 都可以赋值给一个具体的变量, 这样非常有用 a=[ 3 -ge 2 ] &amp;&amp; echo &quot;large&quot; || echo &quot;small&quot; 或者使用$(command) 测试命令 test/[ 命令的与或非 与 bash命令中的 &amp;&amp; || !的区别 test 1 &gt; 2 -a 2 &lt; 3 test 1 &gt; 2 -o 2 &lt; 3 test !1 &gt; 2 test 1 &gt; 2 &amp;&amp; test 2 &lt; 3 test 1 &gt; 2 || test 2 &lt; 3 ! test 1 &gt; 2 从上面命令可以看到test命令提供的与或非是在test命令中实现的, 也就是test遇到这样的命令它会默认将其解析成两个命令执行, 而 &amp;&amp; || !则是将其当两个命令来执行的, 先执行前面的命令, 再看是否需要执行后面的命令, 所以这样就可以拼凑出, 三元表达式如:[ 3 -ge 2 ] &amp;&amp; echo “large” || echo “small shell中不存在布尔值的概论, 即使执行test或false命令, 也仅仅只是返回给程序执行结果一个标示, 标示命令是否执行成功, $?, 所以对于shell而言, 所谓的bool值就是命令执行的结果是否成功, $?=0表示程序执行成功, 而非零则表示执行失败 shell的条件判断 [ -n “$1” ] &amp;&amp; echo “exist” || echo “none” shell 语言 shell中使用#表示注释， 首行#!/bin/bash表示当shell fork一个字进程并且使用exec系统调用来替换这段代码时使用什么shell来执行 shell中使用\表示转义， 表示后面字符使用其真实含义， 如：用在行为表示另外启动一行， 也就是\n本身的含义保留， 所以换行才可以使用\ [ 实际上是一个命令， [ -d /etc ] 实际上-d /etc ]都是这个测试命令的参数， 并且它要求最后一个参数必须是 ], 如果测试结果为真，则该命令的Exit Status为0，如果测试结果为假，则命令的Exit Status为1（注意与C语言的逻辑表示正好相反），通过$?获取退出码， 并且可以使用man [ 开查看帮助信息, 它与test命令没有任何区别 算数计算 $(()), $(())中只能用+-*/和()运算符，并且只能做整数运算。 for循环可以直接在命令行输入： for i in $(jps | grep -v ‘Jps’| awk ‘{print $1}’); do echo $i; done for i in $(ls); do echo $i; done for i in `ls`; do echo $i; done for i in hello world welcome; do echo $i; done for i in *; do echo $i; done 表示将当前文件夹下所有的文件都打印出来 注意： in后面的参数是以空格和分行符来确定的， 并且如果参数为一个路径， 或者路径的正则表达式形式， 那么模式是使用 ls 参数 的形式来进行操作的。 in后面的参数都是以字符串的形式表示的， 所以对于bash而言没有数据类型的说法， 一切皆为字符串 每次遍历完成之后， 其实变量 i 并没有销毁，此时变量i保存着最后循环值， 这也表明shell script并非使用传统的局部变量的编译形式。可以手动进行取消 unset i case语句 重定向（在终端下如果，没有指明标准输入、标准输出、标准错误， 那么默认都是屏幕） 标准输入 0， 标准输出 1， 标准错误 2， &gt; 替换， &gt;&gt; 追加 &lt; 将标准出入更改为某个文件 > 默认将标准输入重定向， 和 1&gt; 完全等价 2&gt; 将标准错误重定向 &amp; 表示获取到某种类型管道（如果没有数字表示获取所有管道）， 如 &amp;1表示获取到标准输出 在shell script中可以使用tip return 返回当前脚本或者代码块 exit 退出 printf 格式化打印 脚本中bash和source/.的执行区别 bash是另起一个子进程执行， 原来的父亲进程睡眠， 等待子进程执行完毕再执行后面的操作source/.是在一个进程中执行, 因为source是bash的内建命令， 可以使用man bash-builtins查看所有的bash内建命令 注意： 所以这就就需要我们判断进程的执行区别了，如使用source .bashrc执行已经更改的.bashrc是可以生效的， 而是用bash .bashrc不会生效， 因为子进程更改的环境变量并不会体现到父进程当中。 shell中的变量 局部变量： 只能在当前脚本中使用 环境变量： 所有的脚本中都可以使用 shell变量： shell中特定的变量 字符串， 字符串的定义比较宽松， 单引号、双引号、不用引号 单引号： 单引号中的数据会原封输出， 单引号中变量无效，且不能再存在单引号，不存在转义 双引号： 双引号中可以存在变量， 且可以转义 常用命令 选取： cut cut /etc/passwd -d “:” -f 3,5 grep ps -ef | grep -Ei “mysql|syslog” | grep -v ‘grep’ | grep -B 1 “root” awk less /etc/passwd | awk -F’:’ ‘BEGIN {print “begin”}{print $3} END {print “after”}’ 排序统计： sort less /etc/passwd | cut -d “:” -f 3,7| sort -t : -k 1 -n -r wc less /etc/passwd | wc -l -w -c uniq less /etc/passwd | cut -d : -f 7 | sort | uniq -c -u/d 多重定向： tee less /etc/passwd | tee hello world 将输出重定向到 中端、hello和world的文件当中 字符转换命令： tr 删除、替换、压缩 sed paste paste -d : /etc/passwd /etc/group 案行合并文件 join 按文件相关性合并文件， 类似excel中的lookup函数 字符显示： printf 文件查看： cat(tac, rev)、 more、 less、 head、 tail、 nl、grep、hexdump、 od、 xxd 文件路径： dirname、basename、rename 文件定位： locate 基于数据库， 需要手动维护数据库， 快速 find 时实查找， 相对慢 文件内容搜索 grep 强大的正则表达式 ag 比grep更加快速 多个命令同时执行 所有的命令必须执行成功, 第一个命令执行完成后才会执行下一个命令， 如果中间命令执行出错则会打断后续命令执行， &amp;&amp; ​ 如： less … &gt; a.txt &amp;&amp; less … &gt; b.txt &amp;&amp; less a.txt b.txt | sort | uniq -u 所有的命令至少有一个执行成功, 前面的命令执行错误不会打断命令继续执行， || 如： hello || cat a.txt, 此时第一条命令打印错误信息， 第二条命令执行完成]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 信息]]></title>
    <url>%2F2019%2F01%2F03%2Flinux-info%2F</url>
    <content type="text"><![CDATA[系统​ uname -a # 查看内核/操作系统/CPU信息 ​ head -n 1 /etc/issue # 查看操作系统版本 ​ cat /proc/cpuinfo # 查看CPU信息 ​ cat /proc/cpuinfo | grep ‘physical id’ #查看cpu个数 ​ cat /proc/cpuinfo | grep ‘core id’ #查看cpu核心个数 ​ cat /proc/cpuinfo | grep ‘processor’ #查看cpu线程个数 ​ hostname # 查看计算机名 ​ lspci -tv # 列出所有PCI设备 ​ lsusb -tv # 列出所有USB设备 ​ lsmod # 列出加载的内核模块 ​ env # 查看环境变量 资源​ free -m # 查看内存使用量和交换区使用量 ​ df -h # 查看各分区使用情况 ​ du -sh &lt;目录名&gt; # 查看指定目录的大小 ​ grep MemTotal /proc/meminfo # 查看内存总量 ​ grep MemFree /proc/meminfo # 查看空闲内存量 ​ uptime # 查看系统运行时间、用户数、负载 ​ cat /proc/loadavg # 查看系统负载 磁盘​ mount | column -t # 查看挂接的分区状态 ​ fdisk -l # 查看所有分区 ​ swapon -s # 查看所有交换分区 ​ hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) ​ dmesg | grep IDE # 查看启动时IDE设备检测状况 网络​ ifconfig # 查看所有网络接口的属性 ​ iptables -L # 查看防火墙设置 ​ route -n # 查看路由表 ​ netstat -lntp # 查看所有监听端口 ​ netstat -antp # 查看所有已经建立的连接 ​ netstat -s # 查看网络统计信息 进程​ ps -ef # 查看所有进程 ​ top # 实时显示进程状态 ​ jobs、 fg、 bg、 ctrl+z/c、 启动 &amp; 用户​ w # 查看活动用户 ​ id &lt;用户名&gt; # 查看指定用户信息 ​ last # 查看用户登录日志 ​ cut -d: -f1 /etc/passwd # 查看系统所有用户 ​ cut -d: -f1 /etc/group # 查看系统所有组 ​ crontab -l # 查看当前用户的计划任务]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 用户]]></title>
    <url>%2F2019%2F01%2F03%2Flinux-user%2F</url>
    <content type="text"><![CDATA[shell作用​ 用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。 linux中用户概念 linxu中存在用户的概念， 系统中存在三种用户： root用户： id=0 登陆shell为/bin/bash(表示可以登录) 系统用户： id=(1-1000) 没有登陆shell（/usr/sbin/nologin, 不需要登陆） 普通用户： id=(1000-65534) 登陆shell为/bin/bash(可以登录) 系统中的所有的文件都存在一个所属用户和所属组的概念， 并且存在所属用户、所属组、附属组的权限， 并且每个用户都存在一个用户名（用户id）， 一个所属组（组id）， 0-31个附属组（附属组id）， 并且每个进程都需要以一个用户的身份来运行， 表示当前用户是否存在访问系统资源的权限 查看用户信息 id显示当前用户信息 ​ uid=1000(feng) 用户id（用户名） ​ gid=1000(feng) 用户组id（用户组名） ​ groups=1000(feng) 用户附属组id（附属组名） 可以存在多个附属组 passwd修改用户密码 w/who/whoami who -r 查看运行级别 基本文件 /etc/passwd 用户相关信息 ​ 用户名:密码:uid:gid:描述信息:家目录:登陆shell 123456789root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologin.....mail:x:8:8:mail:/var/mail:/usr/sbin/nologinnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologinuuidd:x:108:112::/run/uuidd:/bin/falsefeng:x:1000:1000:feng,,,:/home/feng:/bin/bashsshd:x:110:65534::/var/run/sshd:/usr/sbin/nologin /etc/shadow 用户密码 /etc/group 用户组信息 创建用户 useradd 在/etc/passwd中添加用户信息 为用户创建一个家目录/home/uname 将/etc/skel中的文件复制到用户的家目录当中(所以如果希望每次创建新用户的相同动作) 建立一个与用户名相同的组， 新建用户默认属于这个组 如果使用passwd命令创建密码， 则将密码保存在/etc/shadow当中 ​ 可以使用以下参数： ​ -d 家目录 ​ -s 登陆shell ​ -u userid ​ -g 主组 ​ -G 附属组（多个， 用“，”分割） 用户其它操作 usermod 修改用户 userdel 删除用户 -r 家目录也一起删除 用户组相关操作（组和用户是独立的概念）​ groupadd 添加组 ​ groupmod 修改组 ​ groupdel 删除组 组的规划案例： 组 用户 training feng、xiang market alice、bob manage steve、john 创建组：​ groupadd training ​ groupadd market ​ groupadd manage 创建用户：​ useradd -G training feng -p 123456 ​ useradd -G training xiang -p 123456 ​ useradd -G market alice -p 123456 ​ useradd -G market bob -p 123456 ​ useradd -G manage steve -p 123456 ​ useradd -G manage john -p 123456 权限(文件夹必须用户可执行权限， 不然无法查看文件夹中的内容)​ 文件类型|文件所属组|其它 链接数 所属用户 所属组 大小 最后修改时间 文件名 ​ drwxr-xr-x 6 feng feng 4096 Aug 4 15:22 ./ ​ drwxr-xr-x 3 root root 4096 Jan 2 2017 ../ ​ -rw——- 1 feng feng 15478 Aug 4 14:34 .bash_history ​ 所有的权限命令都是-R来递归修改目录下的权限 更改文件所属用户 ​ chown feng Hello.java 更改文件所属组 ​ chgrp feng Hello.java 更改文件权限 ​ chmod 模式 文件 ​ 模式： ​ u、g、o、a 表示用户、组、其它、所有 ​ +、-表示添加或者减少 ​ r、w、x代表三种权限 ​ 777用三个数字来表示三组权限 ​ 数字权限： r = 2^2 = 4, w = 2^1 = 2 , x = 2^0 = 1 ​ 如：rw = 4+2 = 6 ​ 实例： 1234chmod u+rw Hello.javachmod g-x hellochmod go+r Hello.javachmod a-x Hello.class 登陆中断的umask值， 表示当前登录用户创建文件的权限​ 注意：对于普通用户的默认umask值为0002， 管理员用户的默认umask值为0002 ​ 前面1位：特殊权限， 有三个（suid、sgid、sticky） ​ 后面3位： ugo模型的基本权限 ​ 特殊权限： 权限 对文件的影响 对目录的影响 suid 以文件的所属用户身份执行，而非执行文件的用户 无 sgid 以文件所属组身份执行 在该目录中创建的文件的所属组与该目录的所属组相同 sticky 无 对目录拥有些权限的用户仅能删除拥有文件，无法删除其它 suid（通常都是设置给可执行文件的）： 当操作系统需要暴露出一个命令可以给普通用户使用， 但是这个命令需要root用户的权限， 此时将此命令的O权限设置r_x给普通用户， 并添加器suid权限， 此时就满足需求。 12feng@ubuntu:~$ ll /usr/bin/passwd -rwsr-xr-x 1 root root 54256 May 17 07:37 /usr/bin/passwd* 可以看出此时命令的U权限位的x权限设置成了s， 并且O权限位为r_x， 表示：其它用户可以执行当前命令， 但是此时我们知道密码保存在/etc/shadow文件当中， 但是此时西面可以看出shadow文件必须只有root用户才可以访问的， 所以此时passwd命令必须让其以root用户的身份运行。 feng@ubuntu:~$ ll /etc/shadow -rw-r—– 1 root shadow 998 Aug 4 17:33 /etc/shadow mysql程序虽然是root用户启动， 但是此时实际运行的用户为mysql用户 sticky： drwxrwxrwt 8 root root 4096 Aug 4 18:20 tmp/ ​ 可以看到对于O权限位的x权限变成了t， 对于文件夹而言必须要x权限才能读取， 此时变成了 ​ t权限位， 表示目录下的文件只有当前用户才能删除用户的文件， 其它的文件无法删除 sgid： 一般使用在多人团队的项目当中（系统中使用很少）， 为文件夹设置一个sgid权限， 并设置 ​ 所属组都存在访问操作权限， 这样更加方便管理 ​ ​]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gopass安装]]></title>
    <url>%2F2019%2F01%2F03%2Flinux-gopass%2F</url>
    <content type="text"><![CDATA[gopass各组件说明 gopass 用于密码管理的工具， 是pass的go语言版本， 提供了多人的密码管理命令行接口 gnupg 用于真正的加密， gnupg用于生成非对称秘钥， 使用公钥进行机密， 私钥进行解密 git 用于将gopass需要管理的密码（被gnupg生产的公钥加密过的密码）上传到git中便于管理 http://www.voidcn.com/article/p-gngvxvlm-nx.html 安装gopass 安装基本工具 apt-get install gnupg git rng-tools 说明： gnupg通过非对称加密算法对密码进行加密 git用于将加密后的密码上传到git远程仓库中 初始化GPG key pair gpg --gen-key 安装gopass 12wget -q -O- https://api.bintray.com/orgs/gopasspw/keys/gpg/public.key | sudo apt-key add -echo "deb https://dl.bintray.com/gopasspw/gopass trusty main" | sudo tee /etc/apt/sources.list.d/gopass.list 12sudo apt-get updatesudo apt-get install gopass 添加自动完成脚本到.zshrc中 12source &lt;(gopass completion zsh | head -n -1 | tail -n +2)compdef _gopass gopass 初始化gopass gopass init 安装passmenu 安装dmemu sudo apt install dmenu 安装passmenu脚本到path中 12345678910111213141516171819202122232425#!/usr/bin/env bashshopt -s nullglob globstartypeit=0if [[ $1 == "--type" ]]; then typeit=1 shiftfiprefix=$&#123;PASSWORD_STORE_DIR-~/.password-store&#125;password_files=( "$prefix"/**/*.gpg )password_files=( "$&#123;password_files[@]#"$prefix"/&#125;" )password_files=( "$&#123;password_files[@]%.gpg&#125;" )password=$(printf '%s\n' "$&#123;password_files[@]&#125;" | dmenu "$@")[[ -n $password ]] || exitif [[ $typeit -eq 0 ]]; then pass show -c "$password" 2&gt;/dev/nullelse pass show "$password" | &#123; IFS= read -r pass; printf %s "$pass"; &#125; | xdotool type --clearmodifiers --file -fi 添加系统快捷键 路径： System Settings --&gt; keyboard --&gt; Shortcuts --&gt; Custom Shortcuts 添加： 上面脚本的路径 配置git远程仓库 在github中创建个人私有仓库 讲gopass的git仓库添加到远程仓库当中 git remote add origin https://github.com/xxxx/gopass.git gopass sync 配置gpg 创建回收key， 用于在私钥发送泄露或者忘记私钥密码时回收公钥 gpg --armor --output revoke-key.txt --gen-revoke keyname 导出公钥和私钥 gpg --armor --output public-key.txt --export keyname gpg --armor --output private-key.txt --export-secret-keys keyname 备份自己的gpg秘钥对到google云盘当中 说明： gpg的秘钥和gopass保护的密码都以及保存起来了， 用于以后的密码找回 直接查看gopass中密码 由于gopass中的密码以及被上面生成的密钥对的公钥加密过， 所以只需要通过上面导出的私钥就可以解密出正真的密码明文 gpg -o hello.txt -d ~/.password-store/storm.gpg 注意：此时需要输入我们生成密钥时候的密钥密码， 所以从这里可以看到即使别人拿到我们的密钥如果没有私钥密码那么他们照样是无法进行解密的 恢复gopass的环境 git clone 密码库到本地 git clone https://github.com/xxxx/gopass.git ~/.password-store gpg 导入公钥和私钥 12gpg --import ~/public-key.txt gpg --allow-secret-key-import --import ~/private-key.txt gopass配置使用公钥用户进行加密 gopass init 总结 从上面的说明可以看出github中的密码和google的密码也是非常重要的， 但是通常情况下绑定了手机可以直接找回， 所以不用担心， 直接使用gopass自动生成的密码即可 私钥是一定不能忘记的， 所以如果这三个密码如果没有全部丢失那么通常情况下自己的gopass密码管理就是没有问题的。 但是存在一个问题， 那就是机器丢失的问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 服务器监控]]></title>
    <url>%2F2019%2F01%2F02%2Flinux-server-monitor%2F</url>
    <content type="text"><![CDATA[top top -H -p free free -m vmstat vmstat 1 10 pmstat pmstat -P -ALL 2 3 pidstat pidstat -p pid 1 5 -l/d/r/w/t iostat iostat -x 1 3 ifstat dstat crontab 定时任务， 注意： 添加PATH，或者直接执行source /etc/profile， 添加文件权限，写定时任务时输出错误日志，脚本中所有的命令和文件必须在path中找到，不然就需要全路径，如果有问题查看日志/var/log/cron netstat 显示服务器监听状态， 和正在被其它机器连接以及连接到其它的机器的连接 -a -t -u： 表示查看正在被其他机器连接或者连接到其它机器， -t tcp, -u udp, -a tcp/udp -l: 表示服务器监听状态 -n 拒绝显示别名，能显示数字的全部转化成数字。 -p 显示建立相关链接的程序名和端口号 -e 显示扩展信息 常用案例： 查看某个端口的连接数： netstat -an | grep 9991 | wc -l 查看某个程序监听的端口 netstat -lp | grep mysql 查看某个程序对外连接数在ip上的分布： netstat -an | grep &quot;192.168.1.15:22&quot; |awk &#39;{print $5}&#39;|awk -F: &#39;{print $1}&#39;|sort|uniq -c|sort -nr|head -20 192.168.1.15:22 服务端监听的程序 netstat -c实时监控连接的建立 ps -ef | grep name 可以先通过netstat查看到程序进程id， 再通过ps来查看相关信息 lsof 列出打开的文件(list open file), 这是个非常重要的命令， 非常方便我们查看文件相关，我们知道在linux当中所有的硬件设备都是以文件的形式存在， 所以我们可以通过查看文件的信息查看我们的硬件信息， 如： 网络设备， 磁盘等等 常用参数： lsof -p pid： 列出指定进程号所打开的文件 ​ lsof -p 2 列出程序打开的文件 lsof -i 条件： 列出符合条件的进程。 （4， 6， 协议， :端口， @ip） ​ sudo lsof -i 4 列出ipv4的进行 ​ sudo lsof -i 6:2181 列出ipv4并且端口为2181的进程 lsof +d/D 目录： 列出目录下被打开的文件/递归 ​ lsof -d . 当前目录 ​ lsof -D . 递归显示 ​ 可以指定多个条件，但默认是OR关系的，如果需要AND关系，必须传入-a参数，比如查看22端口并且使用Ipv6连接的进程： ​ sudo lsof -c sshd -i 6 -a -i :22]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark注意点]]></title>
    <url>%2F2019%2F01%2F02%2Fhadoop-spark%2F</url>
    <content type="text"><![CDATA[spark transform 和 action 动作中的闭包序列化，由于在闭包中，spark只是负责把闭包进行序列化成对象，也就是说从Driver到Executor中，spark拿到的都是一个个序列化对象，并通过rdd的组合对这些序列化对象进行调用，那么对于闭包来说，spark没有任何的特例，它就是完全使用scala语言闭包的语义，也就是说在创建闭包的时候，对于闭包的可见性对象， 它会将其打包成类的属性， 并对闭包进行序列化，那么通常来说它仅仅只是会打包闭包可见性并且需要的对象，对于闭包中的可见性对象如果它的方法依赖与另外一个对象（并且这个对象，不是这个可见对象的属性，那么spark是不会序列化这个对象的，如： 123456789101112131415//业务调用import CacheHelper.cachedataset.filter&#123;log =&gt; cache(log .....)&#125;// rdd闭包中使用这个单例中的方法， object CacheHelper&#123;cache(log ....) &#123;Sington.name&#125;&#125;// 程序启动的时候， 初始化这个单例对象name=&quot;init name&quot;class Sington(name: String)object Sington &#123;private var instance = new Sington(&quot;default name&quot;)val getInstance = instancedef init(name: String) instance.name = name&#125; 注意： 从上面的分析可以看到， 闭包中的调用的方法依赖一个Sington的全局对象, 对于普通的本地运行master = local[n]此时的Sington的name为”init name”, 对于集群中运行的时候master = yarn此时Sington的那么为”default name” 分析： 现在通过上面的规则可以非常明显的看出此时Executor中的Sington对象没有被序列化， 为默认值， 所以对于spark而言要非常注意这种序列化问题， 其实规则非常简单， 但是通常会由于对象图谱而造成混乱， 所以通常必须要非常清晰的全局对象。 处理： 更改这个错误也非常简单， 只需要在dataset.filter中直接传入这个Sington对象， 或者只传入它需要的参数就可以， 也就是说将这个单例对象暴露到闭包的可见范围之内， 这样闭包就会被序列化。 缺点： 当然这里非常明显的代码结构发生了变化， 但这也是无赖之举， 也就是说我们一定要缩小闭包的可见范围。 1234567891011121314import CacheHelper.cachedataset.filter&#123;log =&gt; cache(log ...., Singtong.getInstance.name)&#125;//将这个Sington暴露到闭包可见范围之内， rdd闭包中使用这个单例中的方法， object CacheHelper&#123;cache(log ...., name) &#123;Sington.name&#125;&#125;// 程序启动的时候， 初始化这个单例对象name=&quot;init name&quot;class Sington(name: String)object Sington &#123;private var instance = new Sington(&quot;default name&quot;)val getInstance = instancedef init(name: String) instance.name = name&#125;]]></content>
      <categories>
        <category>hadoop</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hortonwork 使用]]></title>
    <url>%2F2019%2F01%2F02%2Fhadoop-hortonwork%2F</url>
    <content type="text"><![CDATA[安装 hortonwork sandbox 参考 sandbox安装页面。 安装的过程中会出现端口冲突，直接修改 sandbox/proxy/proxy-deploy.sh 配置就可了， 如： -p 31080 1080 \ 修改docker 的root密码, 直接登录进入docker就会提示更改密码 更改ambari admin 密码， 终端下执行 ambari-admin-password-reset mysql无法启动 mv /usr/sbin/mysqld /usr/bin/mysqld ln -nfs /usr/bin/mysqld /usr/sbin/mysqld 修改hdfs的yarn日志用户, Ambari &gt; HDFS &gt; Configurations&gt;custorm core-site &gt; Add Property hadoop.http.staticuser.user = yarn 修改ranger admin UI密码登入， 保持下面密码全部都是一样的， 并且密码长度大于等于9位 注意： 设置ranger的rangerdba连接的时候要明确指定不需要SSL，如下： jdbc:mysql://hdp1.ilivoo.com:3306?useSSL=false 更改ambari admin 密码， 终端下执行 ambari-admin-password-reset 更改ranger admin for Ambari密码， ranger -&gt; config -&gt; advance -&gt; Ranger Admin user’s password for Ambari(Admin Settings下) 更改ranger Admin 密码， ranger -&gt; config -&gt; advance -&gt; Ranger Admin user’s password(Ranger-env下) 更改mysql root用户以方便以后对mysql的更改， 在my.conf中加入skip-grant-tables跳过授权，更改mysql root用户密码 update mysql.user set authentication_string=password(&#39;mypasswd&#39;) where user=&#39;root&#39;; 更改ranger需要的mysql root账户密码， ranger -&gt; config -&gt; ranger admin -&gt; Database Administrator (DBA) password 更改ranger admin账户的登录密码 update ranger.x_portal_user set password = &#39;ceb4f32325eda6142bd65215f4c0f371&#39; where login_id= &#39;admin&#39;; 此password是有用户名和密码进行MD5求取出来的 echo -n &#39;password{admin}&#39; | md5sum 使用同样(f步骤)的方法更改amb_ranger_admin的登录密码 修改 Atlas admin密码 Configs-&gt; Advanced -&gt; Advanced atlas-env -&gt; Admin password mysql 相关配置 max_connect_errors=10000 启动kerberos 安装并配置kerberos 参考: https://blog.csdn.net/Dr_Guo/article/details/53534927 导出kerberos创建的admin/admin用户的keytab到需要的环境中 12#kadmin.localxst -k admin.keytab -norandkey admin/admin 配置firefox kerberos认证 firefox中输入about:config 搜索并修改选项network.negotiate-auth.trusted-uris 内容如下： .hortonworks.com 配置chrome认证，直接添加命令行参数就可以 google-chrome --auth-server-whitelist=&quot;*.hortonworks.com&quot; --auth-negotiate-delegate-whitelist=&quot;*.hortonworks.com&quot; 修改sandbox-proxy， 让其暴露出kerberos认证需要的三个端口 修改 sandbox/proxy/proxy-deploy.sh 添加如下内容： 123-p 749:749 \-p 464:464 \-p 88:88 \ 修改 sandbox/proxy/conf.stream.d/tcp-hdp.conf 添加如下内容： 123456789101112server &#123; listen 749; proxy_pass sandbox-hdp:749;&#125;server &#123; listen 464; proxy_pass sandbox-hdp:464;&#125;server &#123; listen 88; proxy_pass sandbox-hdp:88;&#125; 注意： 由于kerberos需要的是tcp端口， 所以我们只需要该tcp的配置就可以了， 如果需要http端口则可以修改http配置 权限管理 通常情况下来说为了方便， 我们会在kerberos中创建一个admin用户， 以后使用admin账户就可以访问所有的hadoop服务， 而在hadoop中也建立一个admin用户，并加入到hadoop组当中， 让其和kerberos中的admin用户对应， 但是实际上它们没有任何的关系。 kerberos权限仅仅用来判定AB问题， 也就是说如何证明自己是自己所申称的人， 所以它需要引入一个受信的第三方来作为验证。 但是对于hadoop中的各组件来说， 通常都是每个组件一个用户， 并且把所有的组件用户加入到一个hadoop用户组当中。所以对于kerberos的admin用户仅仅只是能够声明我是一个管理员用户，它可以访问所store the credentials有的启动kerberos的hadoop服务， 但是hadoop通常来说每个用户都有自己的用户访问权限， 也就是说kerberos只是作为第一个访问这个服务的权限， 而服务自身的权限就和kerberos没有任何关系了。如hdfs当中， 所有的用户权限是根据本地linux模型来管理的， 所以必须要有linux本地用户的权限才可以进行访问。 yarn 队列有自己的提交权限策略， 需要配置才能提交任务 yarn.scheduler.capacity.root.acl_administer_queue=yarn,spark,hive,admin,zeppelin spark的历史日志查看也需要进行权限设置， 通常关闭 spark.history.ui.acls.enable 因为日志通常记录在hdfs中， 所以还必须拥有hdfs的文件访问权限 问题汇总 ranger 页面无法登录， 如果显示无法访问DB这是因为使用Mysql的时候必须明确指定是否使用SSL， 所以必须在配置连接的时候使用 jdbc:mysql://hdp1.ilivoo.com:3306?useSSL=false curl -H “X-Requested-By:ambari” -u “admin:tepia@2018” -X GET http://hdp1:8080/api/v1/clusters/c1/credentials/kdc.admin.credential注意： 任何的报错都应该去查看日志才能精确定位错误，不用假想和凭经验断定。 为集群创建feng用户后， feng用户无法提交任务到yarn中， 这需要修改yarn的队列配置， 直接搜索queue就可了。 Invalid KDC administrator credentials. Please enter admin principal and pass, 当开启kerberos的时候需要选择KDC store the credentials 存储策略， temporary 或者 persisted， 默认为临时的，如果我们重启amberi server的时候， 再次去添加服务或删除服务时候就会出现上面的错误。解决办法： 查看是否存在 credentials 1curl -H &quot;X-Requested-By:ambari&quot; -u &quot;admin:password&quot; -X GET http://hdp1:8080/api/v1/clusters/c1/credentials/kdc.admin.credential 删除 credentials 1curl -H &quot;X-Requested-By:ambari&quot; -u &quot;admin:password&quot; -X DELETE http://hdp1:8080/api/v1/clusters/c1/credentials/kdc.admin.credential 如果不存在添加添加 1curl -H &quot;X-Requested-By:ambari&quot; -u &quot;admin:password&quot; -X POST -d &apos;&#123;&quot;Credential&quot; : &#123;&quot;principal&quot; : &quot;admin/admin&quot;, &quot;key&quot; : &quot;addpassword&quot;,&quot;type&quot; : &quot;temporary&quot;&#125;&#125;&apos; http://hdp1:8080/api/v1/clusters/c1/credentials/kdc.admin.credential 注意： -u &quot;admin:password&quot; 是 ambari ui的登录用户名和密码， &quot;principal&quot; : &quot;admin/admin&quot; 是需要添加的用户名，&quot;key&quot; : &quot;addpassword&quot; 是需要添加的密码， 最后使用用户名和命名登录就可以了。 重点：由于添加机器或者服务都是需要知道admin/admin的信息的， 这样就可以通过它在KDC中添加机器和服务的principal， 但是由于KDC的密码通常来说都是有时效的， 基本上几分钟就开始过期了，所以我们在使用的时候， 必须先修改KDC的密码， 再迅速添加上amberi server的KDC密码，这样再去添加机器或者服务的时候就可以连接上KDC。 修改KDC密码, kadmin.local 1change_password admin/admin@ILIVOO.COM 最后， 由于amberi server的密码是可以直接persisted，所以我们可以开启这个持久化功能，具体参照 https://community.hortonworks.com/articles/42927/adding-kdc-administrator-credentials-to-the-ambari.html， 开启之后以后添加服务就可以不用添加amberi server密码了，只需要执行修改KDC的密码就可以，但是注意KDC密码必须是amberi server保存的密码]]></content>
      <categories>
        <category>hadoop</category>
        <category>hortonwork</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hortonwork</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 安装]]></title>
    <url>%2F2019%2F01%2F02%2Flinux-centos-install%2F</url>
    <content type="text"><![CDATA[journalctl 启动过程中遇到任何问题都可通过这里解决 安装完成后配置网卡自动启动 修改配置文件 vi /etc/sysconfig/network-scripts/ifcfg-ent33 1ONBOOT=YES 重启网卡 service network restart 安装网络工具 yum install net-tools 更换阿里云的源 安装wget yum install wget 获取阿里云的源 wget http://mirrors.aliyun.com/repo/Centos-7.repomv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS-7.repo CentOS-Base.repo 更新缓存存并更新 yum clean allyum makecacheyum -y update 设置ssh， 参考ssh使用教程 设置固定ip 编辑配置文件，添加或修改如下类容 vim /etc/sysconfig/network-scripts/ifcfg-eth0 123456BOOTPROTO=&quot;static&quot; #dhcp改为static ONBOOT=&quot;yes&quot; #开机启用本配置IPADDR=192.168.7.106 #静态IPGATEWAY=192.168.7.2 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.7.2 #DNS 配置 重启网络 service network restart 注意： 通常情况下，安装虚拟机后使用NAT网络， 那么主机的地址为192.168.7.1, 虚拟机网关地址 192.168.7.2, 剩下的地址才是虚拟机地址 关闭防火墙 systemctl stop firewalld.service systemctl disable firewalld.service 关闭SELINUX vim /etc/selinux/config` 1SELINUX=disabled 安装本地源 挂载U盘 mount -t vfat /dev/sdc4 /mnt/Centos7 步骤参考 [u盘挂载]: https://blog.csdn.net/leshami/article/details/78133716 安装无线网卡参考 设置开机启动 /etc/init.d/中添加可执行脚本, 开头可以参考这个目录下其它的脚本, 如： 添加 sslocal 1234567#!/bin/bash## sshd Start up the OpenSSH server daemon## chkconfig: 2345 55 25# description: sslcal shadowsocks/usr/bin/sslocal -c /etc/shadowsocks.json 添加到启动服务当中 chkconfig sslocal on 添加交换分区 添加swap分区dd if=/dev/zero of=/var/swapfile bs=1024 count=4096k 执行完毕，对交换文件格式化并转换为swap分区：mkswap /var/swapfile 挂载并激活分区：swapon /var/swapfile 赋权限chmod -R 0600 /var/swapfile 设置开机自动挂载该分区：在文件/etc/fstab文件末尾追加如下内容后/var/swapfile swap swap defaults 0 0 安装vmware 1234sudo yum updatesudo yum upgradesudo yum install &quot;kernel-devel-uname-r == $(uname -r)&quot; gccsudo sh ./VMware-Workstation-Full-14.1.1-7528167.x86_64.bundle 挂载ntfs磁盘 123yum -y install epel-releaseyum -y install ntfs-3g fuseyum -y install ntfsprogs]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux wifi配置]]></title>
    <url>%2F2019%2F01%2F02%2Flinux-wifi%2F</url>
    <content type="text"><![CDATA[首先下载iw工具。 yum -y install iw 获取无线网卡的名称 执行iw dev，假设获得名称为 wlp3s0（示例） 激活无线网络接口 执行ip link set wlp3s0 up 扫描当前环境中的无线网络 执行iw wlp3s0 scan|grep SSID，假设你能够连接的网络名称是TP-LINK-1（示例） 登录指定网络 执行wpa_supplicant -B -i wlp3s0 -c &lt;(wpa_passphrase “TP-LINK-1” “此网络的密码”) 主动请求动态地址 dhclient wlp3s0 查看获取的网络地址 执行ip addr show wlp3s0 设置自动启动 设置NetworkManager自动启动 chkconfig NetworkManager on 安装NetworkManager-wifi yum -y install NetworkManager-wifi 开启WiFi nmcli r wifi on 测试（扫描信号） nmcli dev wifi 连接 nmcli dev wifi connect wifi_name password wifi_password]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摄影]]></title>
    <url>%2F2019%2F01%2F02%2Flive-photo%2F</url>
    <content type="text"><![CDATA[摄影的三要素 曝光： 光线通过光圈到照片中接受的多少。 审美： 照片通过各种元素给人的视觉不同的冲击力。 其它： 照片体现的各种主题、人文等等。 曝光的三要素： 光圈： 小孔成像中小孔的描述， 光圈也存在大小。 快门： 光线通过小孔传递到照片， 通过一个开关控制光圈打开的时间。 ISO：由于场景中光线的强度有限， 如果通过大光圈慢快门还是无法达到我们想要的效果，那么通常需要调大ISO， 从而使得照片增加曝光。ISO也叫感光度， 如果ISO增大那么照片的曝光也会增大， 但是同样带来了另外一个问题， 噪音也会变大，所以通常情况下光线良好的都是低ISO。 景深的三要素： 景深定义：对于定焦物体后面的景色清晰程度， 也就是焦点后面物体多远的保持清晰 光圈：通过小孔成像的原理我们知道， 小孔越大照片越模糊， 小孔越小照片越清晰， 所以通常情况下拍摄人物会选择大光圈， 而拍摄景色使用小光圈， 这样也会使得人物定焦后， 人物清晰而人物后面的景色比较朦胧， 而景色整个的都比较清晰。 焦距：当拍摄时的光圈大小不变，被摄体的位置也不改变时，使用的镜头焦距越短，景深就越大;镜头的焦距越长，景深就越小。也就是在光圈不变的条件下，使用广角镜头时，景深的清晰范围就要相对大一些，使用中、长焦距镜头时，景深的清晰范围就相对要小得多。 焦点与相机的距离：而当拍摄时的光圈大小不变，所使用的镜头焦距也不改变时，被摄体越远，画面中的前后清晰范围就越大；反之，被摄体越近，前后的清晰范围也就相对越小。这就提醒我们，在拍摄一些特定和近景的画面时，调焦应该特别仔细，稍有疏忽，使主体景物越出景深范围，整个画面就都虚了。]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql性能解析]]></title>
    <url>%2F2019%2F01%2F02%2Fmysql-profile%2F</url>
    <content type="text"><![CDATA[profiling使用： 性能剖析一般分为两步， 测量任务所花费的时间， 然后对结果进行统计和排序。 New Relic, xhprof 可以通过设置long_query_time为0来捕获所有的查询 1234slow_query_log = onslow_query_log_file = /var/log/mysql/mysql-slow.loglong_query_time = 0#set global slow_query_log=on;通常情况下，服务器可以通过随时打开来进行开启慢查询 pt-query-digest, mysqldumpslow等等来分析满查询 性能剖析步骤： 使用慢查询， 直接开启慢查询或者使用pt-query-digest获取(show full processlist实现)或者直接使用tcpdump将网络包数据保存到磁盘， 然后使用pt-query-digest –type=tcpdump来进行解析并分析查询 使用pt-query-digest来分析慢查询日志， 并分析出结果 使用explain分析执行的语句 使用profiling来分析单条语句 1234set profiling=1select * show profilesshow profile for query n 通常使用show status、show processlist和show innodb status开销是非常低的 通过show profile for query n来获取到查询主要的耗时在哪个地方， 如： 123Copying to temp table 0.090623Sorting result 0.011555Sending result 0.045931 注意：可以看到此处最大的消耗是在临时表中， 所有此时我们考虑的是如何去掉临时表， 但经常可能会误导我们的是结果排序， 它占比非常低， 而我们如果简单通过查询语句很容易掉入到这个陷阱当中， 所以一般原则是”不建议优化排序缓冲区”。 show status通常是一个很有用的工具， 但是它不是一款剖析工具， show status大多都是一个计数器。 mysql在5.6.6中正是加入了performance_schema数据库（它的存储引擎也是performace_schema, 内存式数据引擎）， 用来记录系统中需要记录的性能相关的数据， 并且是默认开启的]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql分区]]></title>
    <url>%2F2019%2F01%2F02%2Fmysql-partitions%2F</url>
    <content type="text"><![CDATA[修改已经存在的表为其添加分区 1234567891011121314151617ALTER TABLE `device_model_report_copy1` PARTITION BY RANGE (to_days(time)) (PARTITION p201811 VALUES LESS THAN (to_days('2018-12-01')), PARTITION p201812 VALUES LESS THAN (to_days('2019-01-01')), PARTITION p201901 VALUES LESS THAN (to_days('2019-02-01')), PARTITION p201902 VALUES LESS THAN (to_days('2019-03-01')), PARTITION p201903 VALUES LESS THAN (to_days('2019-04-01')), PARTITION p201904 VALUES LESS THAN (to_days('2019-05-01')), PARTITION p201905 VALUES LESS THAN (to_days('2019-06-01')), PARTITION p201906 VALUES LESS THAN (to_days('2019-07-01')), PARTITION p201907 VALUES LESS THAN (to_days('2019-08-01')), PARTITION p201908 VALUES LESS THAN (to_days('2019-09-01')), PARTITION p201909 VALUES LESS THAN (to_days('2019-10-01')), PARTITION p201910 VALUES LESS THAN (to_days('2019-11-01')), PARTITION p201911 VALUES LESS THAN (to_days('2019-12-01')), PARTITION p201912 VALUES LESS THAN (to_days('2020-01-01')), PARTITION pmax VALUES LESS THAN (MAXVALUE)) 添加分区和差分分区 12ALTER TABLE `device_model_report_copy1` DROP PARTITION pmax;ALTER TABLE `device_model_report_copy1` ADD PARTITION (PARTITION p202001 VALUES LESS THAN (to_days('2020-02-01'))); 注意 : 由于分区表通常会带有一个最大的分区用来控制忘记添加分区， 所以我们添加分区的时候一定要知道最大分区中是否存在数据， 如果存在那么必须先修改最大分区， 所以通常情况下我们不会使用上面的语句进行添加分区， 而是使用Reorganize关键字， 并且Reorganize关键字不会导致数据丢失 1234ALTER TABLE `device_model_report_copy1` reorganize partition pmax into(PARTITION p202001 VALUES LESS THAN (to_days('2020-02-01')), PARTITION p202002 VALUES LESS THAN (to_days('2020-03-01')), PARTITION pmax VALUES LESS THAN (MAXVALUE)); 修改多个分区，在into关键字之前或之后都指定多个分区 1ALTER TABLE `device_model_report_copy1` reorganize partition p201901, p201902 ... p201912 into(Partition p2019 values less than(to_days('2020-01-01'))); 修改整个表的分区 1234ALTER TABLE `device_model_report_copy1` reorganize partition into(Partition p2019 values less than(to_days('2020-01-01'))，Partition p2020 values less than(to_days('2021-01-01'))，Partition pmax values less than(MAXVALUE)); 子分区 123456789101112create table orders_range(id int auto_increment primary key,customer_surname varchar(30),store_id int,salesperson_id int,order_Date date,note varchar(500)) engine=myisam partition by range(id) subpartition by hash(store_id) subpartitions 2(partition p0 values less than(5),partition p1 values less than(10),partition p3 values less than(15)); 注意 : 只允许对range和list类型的分区再进行分区，子分区的类型只允许是hash或key. 查看分区信息 123456789select partition_name part, partition_expression expr, partition_description descr, table_rows, data_length/1024/1024from information_schema.partitions where table_schema = schema() and table_name='device_model_report'; 创建分区存储过程 12345678910111213141516171819202122232425262728293031323334353637drop procedure if exists p_time_range_partition_add;DELIMITER // CREATE DEFINER=`root`@`%` PROCEDURE `p_time_range_partition_add`(IN `sch_name` varchar(50),IN `tab_name` varchar(50),IN `patition_date` date)b_lable:BEGINdeclare pmax_name VARCHAR(50) default "pmax"; -- max patition name must be pmaxdeclare p_name varchar(50) default concat('p', date_format(patition_date,'%Y%m'));declare p_time varchar(50) default concat(date_format(date_add(patition_date,interval 1 month),'%Y-%m'), '-01');declare pmax_table_rows integer default 1;-- check paramif (SELECT table_name FROM information_schema.TABLES WHERE table_schema = sch_name and table_name = tab_name) is null then leave b_lable;end if;-- test pmaxselect table_rows into pmax_table_rows from information_schema.partitions where table_schema = sch_name and table_name = tab_name and partition_name = pmax_name;if pmax_table_rows &gt; 0 then leave b_lable; -- need to manual alterend if;-- test exist pnameif (select partition_name from information_schema.partitions where table_schema = sch_name and table_name = tab_name and partition_name = p_name) = p_name then leave b_lable; -- has patitionend if;-- add patitionset @p_ps=concat("ALTER TABLE ", tab_name, " reorganize partition pmax into(PARTITION ", p_name, " VALUES LESS THAN (to_days('", p_time, "')), PARTITION pmax VALUES LESS THAN (MAXVALUE));");prepare stm from @p_ps;execute stm;END//DELIMITER ; 设置事件 1234-- 设置开启(数据库默认是关闭的，这个需要在my.cnf中设置下，否则数据库重启后会自动关闭)set global event_scheduler = 1;-- 查看任务调度启动状态show global VARIABLES like 'event_scheduler'; 1234567-- 创建任务调度create event event_p_time_range_partition_addon schedule every 1 month starts sysdate()on completion preserveenabledocall p_time_range_partition_add('test', 'device_model_report',date_add(curdate(),interval 1 month));]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 命令行工具]]></title>
    <url>%2F2018%2F07%2F19%2Fkafka-shell-command%2F</url>
    <content type="text"><![CDATA[kafka-console-producer.sh用来往topic中生产消息1bin/kafka-console-producer.sh --broker-list hadoop3.feng.com:9092 --property &quot;parse.key=true&quot; --property &quot;key.separator=:&quot; --topic kafka 说明 –broker-list –topic 指定broker列表和生产消息的topic –property 指定console读取消息的kafka.tools.ConsoleProducer$LineMessageReader的属性， 也可以–line-reader指定默认的行读取器， 可以指定三个参数： parse.key true 表示是否解析key， 如果不解析默认key为null key.separator 指定key分隔符， 默认为 \t ignore.error true 指定是否忽略错误 如果不解析key那么， 生产的消息会均匀的发送到所有的Partition当中 –producer.config –producer-property 通过配置文件或者直接指定key=value去指定生产者的属性，用于在设置命令行中没有提供的参数 –key-serializer –value-serializer –compression-codec 指定key value的序列化和压缩方式 kafka-console-consumer.sh用来消费topic中的消息1bin/kafka-console-consumer.sh --bootstrap-server hadoop2.feng.com:9092 --topic kafka --property print.key=true --property &quot;key.separator=:&quot; --from-beginning 说明 –broker-list –topic 指定broker列表和消费的topic –property 指定console读取消息的kafka.tools.DefaultMessageFormatter的属性， 也可以–formatter指定默认的行读取器， 可以指定三个参数： print.key true 打印key print.value true 打印value key.separator 指定key分隔符 line.separator 指定行分隔符 –from-beginning 指定从日志头开始消费 –group 指定消费者组， 如果不指定那么默认会自动生产一个唯一的消费者组， 如果想要多个不同的consumer消费某个topic那么此处必须指定 –Partition 指定消费的topic的分组， 如果不指定默认消费全部的分组 –offset 指定消费者的位置非负整数或者earliest和latest， 默认为latest –consumer.config –consumer-property 通过配置文件或者直接指定key=value去指定消费者的属性，用于在设置命令行中没有提供的参数 –isolation-level read_uncommitted/read_committed 指定隔离级别， 用于支持读取消息的一致性语义 kafka-consumer-groups.sh用来管理消费者组1bin/kafka-consumer-groups.sh --bootstrap-server hadoop1.feng.com:9092 --group console-consumer-38042 --describe 说明 此命令主要存在四个功能–list, –describe, –delete, –reset-offsets –list 显示所有的消费者组 –describe 显示某个消费者组的详细信息 –delete 删除某个消费者组 –reset-offsets 重新设置消费者组的偏移量 kafka-log-dirs.sh用来查看broker的日志保存信息1bin/kafka-log-dirs.sh --bootstrap-server hadoop1.feng.com:9092 --describe --topic-list kafka --broker-list 2 说明 –topic-list 需要显示的topic –broker-list 需要显示的brokers kafka-delete-records.sh用来删除记录123456789bin/kafka-delete-records.sh --bootstrap-server hadoop1.feng.com:9092 --offset-json-file myconfig/kafka-delete-records.json&#123; &quot;partitions&quot;: [ &#123;&quot;topic&quot;: &quot;kafka&quot;, &quot;partition&quot;: 0,&quot;offset&quot;: 20&#125; ], &quot;version&quot;:1&#125; 说明 对于kafka来说默认存在两种删除策略， 一种基于日志量， 二种基于日志保存的时间， 但是有时候我们需要手动的去指定删除到具体的位置， 这个脚本就比较有用了 日志可能未必立即删除， 但是以及无法消费删除的日志了 kafka-preferred-replica-election.sh用来对某个topic的分区进行选举prefered replica， 分区的leader用来读写的， 从而平衡kafka集群1bin/kafka-preferred-replica-election.sh --zookeeper hadoop1.feng.com:2181 --path-to-json-file myconfig/kafka-preferred-replica-election.json 说明 此处的选举主要是为了kafka能够平衡负载， 并且kafka也保留了auto.leader.rebalance.enable来自动进行分区leader选举， 造成不平衡的原理是因为broker添加或者是broker失败恢复 kafka-reassign-partitions.sh生产分配方案12345678910111213141516bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --generate --topics-to-move-json-file myconfig/kafka-reassign-partitions-generate.json --broker-list 0,1,2Current partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125;Proposed partition reassignment configuration&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125;kafka-reassign-partitions-generate.json&#123; &quot;topics&quot;: [ &#123;&quot;topic&quot;: &quot;kafka&quot;&#125; ], &quot;version&quot;:1&#125; 说明 –generate 生成重新分区方案， –topic-to-move-json-file指定需要移动的topic信息， –broker-list 指定移动到的broker 此处打印出当前的分配信息， 以及建议的分配信息， 可以讲建议信息作为–reassignment-json-file的参数进行手动的执行 这里不需要指定bootstrap-server是因为当重新进行分配分区的时候实际上仅仅只是在zookeeper中更改状态， 当controller获取到状态的时候就异步的去执行当前分配策略 执行分配123bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --execute --reassignment-json-file myconfig/kafka-reassign-partitions-execute.json&#123;&quot;version&quot;:1,&quot;partitions&quot;:[]&#125; 在Partition移动的过程中会有大量的io操作， 所以对于集群中网络带宽这种稀有资源应该加以限制， 使用throttle进行设定 检查分配1bin/kafka-reassign-partitions.sh --zookeeper hadoop1.feng.com:2181 --verify --reassignment-json-file myconfig/kafka-reassign-partitions-execute.json 检查移动分区的执行情况 移除当前移动分区时设置的网络配额]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell debug]]></title>
    <url>%2F2018%2F07%2F15%2Flinux-shell-debug%2F</url>
    <content type="text"><![CDATA[bash -x教程 设置PS4 1export PS4=&apos;($&#123;BASH_SOURCE&#125;:$&#123;LINENO&#125;): $&#123;FUNCNAME[0]&#125; - [$&#123;SHLVL&#125;,$&#123;BASH_SUBSHELL&#125;, $?]&apos; bashdb教程安装1sudo apt install bashdb 命令详解 原始代码 bashdb.sh sub_bashdb.sh bashdb.sh 12345#!/bin/bashecho &quot;hello world&quot;name=&quot;feng xiang&quot;age=28. sub_bashdb.sh $name $age sub_bashdb.sh 12345#!/bin/bashecho &quot;welcome &quot;name=&quot;wang pei&quot;age=27echo &quot;welcome&quot; &gt;&gt; hello.txt 断点 12345678910111213141516#断点, 指定行号或者当前行号break [LINENO]#瞬时断点, 指定行号或者当前行号， 断点一次后失效tbreak#条件断点, 断点编号， 断点条件condition BRKPT-NUM condition#删除一个或多个断点delete &#123;BRKPT-NUM&#125;...#清除某行所有断点clear LINENO#开启一个或多个断点enable BPNUM1 [BPNUM2 ...]#关闭一个或多个断点disable BPNUM1 [BPNUM2 ...]#显示断点info b[reakpoints] 12345678#继续执行到断点continue [LINENO | - ]#1. 执行到下一个断点continue#2. 执行到指定的行(首先给指定的行加零时断点，再执行continue命令)continue LINENO#关闭debug并执行脚本continue - 12345#前进1. 前进一步或者多步， 不进入方法next [COUNT]2. 前进一步或者多步， 进入方法或者source(.)命令执行的文件step [COUNT] 12#跳过一行或多行skip [COUNT] 1234567891011#执行完当前方法finish#从当前方法中返回return#向当前debug的进行发送信号kill [SIGNAL]signal SIGNAL#退出debuggerquit [EXIT-CODE [SHELL-LEVELS]]#重新运行或者指定重新运行的命令, 如果指定args必须为完整的命令， 不指定则使用原先的命令run [args] 查看变量 1234#打印出表达式值print EXPRESSIONprint $@ $name 123456#每次debug停止到一行的时候显示或者直接执行命令display [EXPRESSION]display $@ $name#删除display的命令undisplay NUM [NUM2 ...] 显示源代码, list显示的时候有个移动往下翻页的概念， 并记录当前浏览到了哪一个位置 12345678910list[&gt;] [LOC|.|-] [NUMBER]list . # 显示到当前栈的行list # 显示当前翻页的行前后list - # 显示当前翻页的行前面list&gt; . # 显示到当前栈的行list 10 3 # 显示10附近的三行 9-11list&gt; 10 3 # 显示是后面的三行 10-12list 10 13 # 显示10-13行list 10 -5 # 显示10行到倒数第五行 显示和设置当前debugger的设置信息 1show/set [param] 查看当前信息 123456789bashdb&lt;1&gt; infoInfo subcommands are: args display functions line signals stack watchpoints breakpoints files handle program source warranty #args 显示参数#display 显示display表达式#line 显示当前行号#breakpoints 显示所有断点... debug其它脚本， 并保留当前现场 12debug [SCRIPT]#不指定脚本， 执行当前行的命令 进入一个嵌套的shell， 不是子shell 12345678910shell [options]options: --no-fns | -F : don&apos;t copy in function definitions from parent shell --no-vars | -V : don&apos;t copy in variable definitions --shell SHELL_NAME --posix : corresponding shell option --login | l : corresponding shell option --noprofile : corresponding shell option --norc : corresponding shell option 执行一个命令 1eval CMD 设置环境变量 1export VAR1 [VAR2 ...] ##### 通过shell来]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs架构]]></title>
    <url>%2F2018%2F04%2F29%2Fhadoop-hdfs-architecture%2F</url>
    <content type="text"><![CDATA[介绍 hdfs是被设计运行在廉价的机器上的分布式文件系统， 它和现有的文件系统有这很多相似之处， 然而也存在这最为重要的区别， hdfs具有高容错性并且有非常高的吞吐量， 用于处理大量的数据集， hdfs遵循posix使用流的方式访问存储在hdfs中的文件， 它最初设计目标是nurch搜索引擎的底层基础设施， 现在作为hadoop中重要的组成部分。 假设和目标 硬件的错误总是存在的， hdfs被设计运行在成百上千的机器上， 每一个存储都作为hdfs的一个部分，事实上如此大型的系统， 每一个组件都可能存在错误， 所以自动检测并恢复错误就是hdfs设计的核心目标。 从另一个方面来说， 分布式的存储远比单硬件提供持久化要可靠地多。 应用需要以流的形式访问数据， 它和通常的文件系统设计目标不同， hdfs被设计成批处理而不是与用户交互的处理， 它的主要目标是大的吞吐量而不是访问延迟。 应用运行在hdfs上都有非常大的数据集， 通常一个文件都是GB到TB级， 它应该提供高聚合的数据带宽并且在单个集群容易扩展上百个节点， 它应该能在单个实例中支持上亿个文件。 hdfs的文件需要一个一次写入多次读取的访问模型， 一个文件一旦创建、写入并且关闭， 那么它就不能改变， 除非append或者truncate， Append内容到文件的结尾被允许， 但是不能在任意位置修改， 这种简单的数据模型能够支撑非常高的吞吐量， MapReduce应用或者Web crawler应用相当适合这种模型。 移动计算比移动数据更加的廉价， 这意味着， 应用的计算数据与计算操作离得更近会更加有效， 这对于大型数据来说就非常正确了， 通过小量的网路带宽来增加系统的吞吐量， 这个假设是聚合计算到距离数据更近的节点上而不是移动数据， hdfs为应用提供了这种接口移动计算到距离数据更近位置的接口。 hdfs被设计成平台无关的， 并且非常简单的从一个平台迁移到另外一个平台。 NameNode和DataNodes]]></content>
      <categories>
        <category>hadoop</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F2018%2F04%2F28%2Ftools-git%2F</url>
    <content type="text"><![CDATA[概念 git存在工作区， 暂存区， 本地仓库， 远程仓库 工作区: 当前项目root目录 暂存区: 新文件通过git add添加， 或者已经关联的文件都在暂存区当中 本地仓库： 本地的仓库， ！，通过commit将暂存区中的内容提交到本地仓库 远程仓库： 远程的仓库， 通过push将本地的commit的版本push到远程 对于已经新添加的文件或者已经关联的文件修改， 都必须通过git add将其添加到暂存区当中， 再通过commit提交到本地，再通过push到远程仓库当中 git checkout123git checkout file 放弃工作空间中改变的文件git checkout branch 切换到指定分支, 可以添加 -b参数添加一个新的分支git checkout -b newbranch branch 从branch分支创建一个新的分支， 并切换到新的分支 git add1git add . 将工作空间中的改变文件添加到暂存区 git reset1234git reset用户回退一个已经commit但是并没有push的代码， 常见的参数： --hard --merge --mixed --patch --softgit reset --mixed version_hash 会保留源码， 只是将git commit和index信息回退到某个版本 --mixed是git reset的默认模式， git reset --mixed 等价于 git resetgit reset --soft 会保留源码， 只是回退到commit信息到某个版本，不涉及index的回退， 如果还需要提交直接提交就可以了git reset --hard 源代码也会回退到某个版本，commit和index都会回退到某个版本，如果没有指定版本那么就会放弃当前所有的更改回退当上一个commit 注意： 上面reset只是更改本地仓库的代码， 远程仓库代码还没有改变， 所以如果index发生变化， 而线上的index没有发生变化， 那么必定会导致不发进行push， 所以必须pull下来， 再进行push， 此时代码还是回变为线上index的版本， 所以总的来说这种方式不适合回退到线上index版本之前的， 它无法做到， 所以我们经常将其作为本地代码回滚的策略。 git revert12git revert 用于反正提交， 执行时必须要求工作目录必须是干净的git revert 用一个新提交来消除一个历史提交的所做的任何修改， 所以它的index不会回退， 而是继续向前， revert之后本地的代码也会回滚到指定的历史版本， 这个时候再push是不会发生任何冲突的 git reset/revert总结git reset是直接删除制定的commit（不会影响远程机器上的代码， index直接被删除）， git revert是用一次新的commit来回滚之前的commit（会影响远程机器上的代码， index会继续前进） git fetch [远程仓库 [远程仓库分支]]123git fetch origin mastergit merge origin/master， 此时可能需要解决冲突等等， 合并之后我们就可以在当前分支上工作并提交到远程分支如果不希望合并当前分支， 但是希望在远处分支上工作， 可以checkout出来一个新的分支git checkout -b newremotebranch origin/master 可以看到checkout可以直接从一个分支上创建一个新的分支，并不需要工作在origin/master分支上 注意： 下载远程分支， 但并不会合并远程分支到当前分支， 也就是说当前分支可能比下载下来的远程分支滞后几个分支， 所以如果我们希望在远处分支上工作， 那么我们必须将远程分支合并到当前分支 it remote某些场合下， 一个git项目需要同时使用两个甚至更多的远程仓库， 比如国外+国内、测试环境+生成环境+审核环境等等， 这样我们就需要对不同的远程仓库分别提交等操作。git remote 参数： add remove set-branches set-url update prune rename set-head show添加仓库， git remote add remote_name remote_url…..123456案例： 当我们发现github上面存在一个项目我们无法修改， 但是我们希望完全保留其所有的版本信息。操作步骤如下：a. git clone出项目到本地b. 在github上面创建一个空的仓库， 地址如下： https://github.com/ilivoo/spring.gitc. 在本地项目下直接删除原来的仓库， git remote remove remote_named. 将自己的远程仓库添加到项目当中， git remote add origin https://github.com/ilivoo/spring.gite. 将本地的项目直接上传到自己的远程项目当中， git push origin master 注意： 此处不仅仅限制与将其导入到github中， 也可以导入到国内的码云当中， 而且推荐使用这种方式。 并且码云当中还有一个更好的办法， 来完成这个动作， 直接在码云当中创建一个仓库， 创建过程中直接克隆github上的项目， 这样就是一步到位， 而且速度非常快， 再直接从码云中clone到本地， 关机是码云中还可以将其设置为private另外： git存在fork的概念， 可以直接fork项目再自己对其进行修改， 类似上面的操作 git branch123git branch branchName 创建一个新的分支git checkout branchName 切换到新的分支git checkout -b branchName 创建并切换到一个新的分支 git checkoutcheckout出某个分支， 注意如果此时暂存区还有修改没有提交， 如果此时checkout的某个分支可以合并， 那么可以切换到对应的分支， 如果有冲突， 那么此时不允许切换到某个分支， 所以在我们切换分支的时候最好是将工作空间清空。 git merge合并两个分支， 此时被合并的分支必定是commit状态， 而当前分区可以是未commit状态， 但是如果此时合并时存在冲突， 那么此时是不能merge成功的， 所以此时必须要对当前分支commit再合并， 合并后可能存在冲突， 此时将冲突解决就可以了， 重新提交。 也可以在合并冲突之后使用命令git mergetool来调用mergetool来处理冲突问题， 所以可以设置当前mergetool, 如p4merge1234567891011121314C:\Users\ilivoo&gt;cat .gitconfig[user] name = ilivoo email = 316488140@qq.com[merge] tool = p4merge[mergetool &quot;p4merge&quot;] cmd = p4merge \&quot;$BASE\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$MERGED\&quot;[mergetool] trustExitCode = false[diff] tool = p4merge[difftool &quot;p4merge&quot;] cmd = p4merge \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; 注意： 合并是先执行git merge， 合并可能没有成功存在冲突， 此时再调用git mergetool自动调用合并工具来手动合并， 而git difftool则是直接使用就可以, 参数可以添加任意两个版本如果不想合并， 那么可以直接通过 git merge –abort取消当前合并 git clean12345678910# 删除 untracked filesgit clean -f# 连 untracked 的目录也一起删掉git clean -fd# 连 .gitignore文件中untrack 文件/目录也一起删掉 （如：idea生成的.idea，日志logs等等）git clean -xfd# 在用上述 git clean 前，墙裂建议加上 -n 参数来先看看会删掉哪些文件，防止重要文件被误删git clean -nxfd# 注意： 真正的删除是参数f d x， n只是用来查看当前参数可能会删除的那些文件， 如：git clean -nf查看那些未跟踪的文件会被删除， 但是并不会真正执行删除， git clean -f 才是真正删除未跟踪的文件 git 设置代理12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; git 设置无密码提交 使用保存密码方式 git config –global credential.helper store .gitconfig文件中会多出 [credential]​ helper = store 当输入过一次密码之后会在.gitconfig文件的相同目录下生成.git-credentials https://ilivoo:password@github.com 使用秘钥方式 添加秘钥到github当中 修改.git/config配置文件 123[remote &quot;origin&quot;]url = git@github.com:ilivoo/project.gitfetch = +refs/heads/*:refs/remotes/origin/*]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 日志]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-log%2F</url>
    <content type="text"><![CDATA[mysql 日志 主要包括： 错误日志、查询日志、慢查询日志、事务日志、二进制日志、回放日志 错误日志： 错误日志是用来记录mysql系统在运行的过程中所发生的错误， 如启动过程中存在的问题， 系统在配置主从是报的错误等待 错误日志默认是开启的，并且错误日志无法被禁止， 默认情况下错误日日志存储在mysql数据库的数据文件目录当中， 以主机名作为名字， 并且以.err结尾， 一般情况下 建议更改日志的名字，这样在统一维护的时候会更加方便 配置方式: log_error=error.log 查询日志 查询日志是用来记录用户的所有操作， 其中包括增删改查等所有信息 查询日志默认是关闭的， 一般如果不是为了调试数据库都不会开启查询日志， 会产生大量的磁盘io 配置方式： general_log=log.log 慢查询日志 慢查询日志是用来记录执行时间超过指定的时间的查询语句， 通过慢查询可以找出运行效率低下需要优化的语句 慢查询默认关闭的， 一般它对服务器的性能微乎其微， 所以建议线上根据需要开启 配置方式 123slow_query_log=onslow_query_log_file=slow.loglong_query_time=0.000001 手动开启： 12set global slow_query_log=on;set global long_query_time=0.000001; 可以看到虽然这里设置了， 但是严格它对于当前的session是无法生效的， 如果仅仅只是想对当前session生效， 可以设置， 12set global slow_query_log=on;set long_query_time=0.000001; 如果想对所有的连接有效， 包括已经启动的连接和后面加入的连接有效， 可以设置， 12set global slow_query_log=on;set global long_query_time=0.000001; 查看当前session的配置情况 1select * from performance_schema.variables_by_thread as a, (select THREAD_ID,PROCESSLIST_ID,PROCESSLIST_USER,PROCESSLIST_HOST,PROCESSLIST_COMMAND,PROCESSLIST_STATE from performance_schema.threads where PROCESSLIST_USER&lt;&gt;&apos;NULL&apos;) as b where a.THREAD_ID = b.THREAD_ID and a.VARIABLE_NAME = &apos;long_query_time&apos;; 可以查看所有的连接是否生效， 再通过kill PROCESSLIST_ID来杀手不生效的连接（一般都是使用连接池来连接的所以问题不大）， 也可以通过笨的方法， 对每一个现有的连接执行一次 注意： 通过这里设置全局变量我们可以看到， mysql设置全局变量未必会对现有的连接生效， 所以我们如果想要进行验证， 最简单的办法就是通过上面的语句进行查询。 设置线上慢查询日志时最好指定毫秒数， 这样可以用于长期线上开启 事务日志 事务日志（InnoDB特有的日志）可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把改修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。事务日志采用追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢的刷回到磁盘。目前大多数的存储引擎都是这样实现的，我们通常称之为预写式日志，修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。 1234innodb_flush_log_at_timeout=1 事务日志如果没有每一次从缓冲区中刷新到磁盘， 那么每隔一秒刷新一次innodb_flush_log_at_trx_commit=1 为1表示事务一提交就刷新到磁盘中， 为2表示只有在事务提交时才同步， 有可能丢失整个事务innodb_log_files_in_group=2 至少两个日志组innodb_log_group_home_dir=./ 日志组的位置 二进制日志 二进制日志是MySQL服务器用来记录数据修改事件的，比如INSERT、UPDATE、DELETE等会导致数据发生变化的语句，SELECT语句不会被记录在内。MySQL必须先执行完一条语句才能知道它是否修改了数据，因此写入二进制日志文件的时间是语句执行完成的时间。写入顺序是按语句执行完成的先后顺序，事务中的语句会先被缓存起来，成功提交后才会被写入，回滚则不会被写入。非事务的存储引擎，所有的修改会立刻写入到二进制日志中。 二进制日志顾名思义不是文本而是一种更有效率的二进制格式，它比文本占用更少的空间，但是可读性就很差了，必须使用mysqlbinlog工具才能转换为可读的文本。二进制日志主要用于数据库备份和故障时恢复数据，配置MySQL主从复制必须启用二进制日志。 二进制日志索引文件中会列出所有二进制日志文件，此文件是文本的因此可以直接查看，文件的最后一行就是当前正在使用的二进制日志文件。 配置 12345678910111213141516log_bin=onlog_bin_basename=bin.log //指定bin log的名字， 后面会自动生成xxxxx的数字log_bin_index=bin.log.index //所有bin log文件名的索引// 通过flush logs会导致当前bin log写完， 并刷新出一个新的log// 并且在 index中记录这个文件的名字binlog_do_db //表示只记录指定数据库的二进制日志binlog_ignore_db //表示不记录指定的数据库的二进制日志//不建议使用 do/ignore来进行控制, 安全的替换方案是 在 slave上配置过滤,//使用基于查询中真正涉及到的表的选项, 例如replicate-wild-* 选项replicate-wild-do-table=ljzxdb.%sync_binlog //直接影响mysql的性能和完整性sync_binlog=0：当事务提交后，Mysql仅仅是将binlog_cache中的数据写入Binlog文件，但不执行fsync之类的磁盘 同步指令通知文件系统将缓存刷新到磁盘，而让Filesystem自行决定什么时候来做同步，性能最好。sync_binlog=n，在进行n次事务提交以后，Mysql将执行一次fsync之类的磁盘同步指令，同志文件系统将Binlog文件缓存刷新到磁盘。binglog_format //值有statement（记录逻辑sql语句）、row（记录表的行更动情况）、mixed混合使两种，// row的存在是因为存在很多不确定性的函数， 如果直接使用， 会导致数据不一致， 如 current_time(), 所以直接将值记录下来就更正确 回放日志 bin log从master发送到slave的时候， slave就变成了reply log， 用于回放master中数据发生的变化]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql索引导出]]></title>
    <url>%2F2018%2F04%2F28%2Fmysql-index-dump%2F</url>
    <content type="text"><![CDATA[打印删除所有的索引的语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印删除所有的索引的语句，不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;DROP &apos;, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, &apos;PRIMARY KEY&apos;, CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;`&apos;))) SEPARATOR &apos;, &apos;), &apos;;&apos;) FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; and UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME ORDER BY TABLE_NAME ASC; 打印出所有的所有创建语句， 包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印出所有的创建语句， 不包括Primary key 1SELECT CONCAT(&apos;ALTER TABLE `&apos;,TABLE_NAME,&apos;` &apos;, &apos;ADD &apos;, IF(NON_UNIQUE = 1, CASE UPPER(INDEX_TYPE) WHEN &apos;FULLTEXT&apos; THEN &apos;FULLTEXT INDEX&apos; WHEN &apos;SPATIAL&apos; THEN &apos;SPATIAL INDEX&apos; ELSE CONCAT(&apos;INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE)END, IF(UPPER(INDEX_NAME) = &apos;PRIMARY&apos;, CONCAT(&apos;PRIMARY KEY USING &apos;, INDEX_TYPE), CONCAT(&apos;UNIQUE INDEX `&apos;, INDEX_NAME, &apos;` USING &apos;, INDEX_TYPE))),&apos;(&apos;, GROUP_CONCAT(DISTINCT CONCAT(&apos;`&apos;, COLUMN_NAME, &apos;`&apos;) ORDER BY SEQ_IN_INDEX ASC SEPARATOR &apos;, &apos;), &apos;);&apos;) AS &apos;Show_Add_Indexes&apos; FROM information_schema.STATISTICS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND UPPER(INDEX_NAME) != &apos;PRIMARY&apos; GROUP BY TABLE_NAME, INDEX_NAME ORDER BY TABLE_NAME ASC, INDEX_NAME ASC; 打印自增的所有创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)),&apos;) UNSIGNED NOT NULL AUTO_INCREMENT;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印删除自增的创建语句 1SELECT CONCAT(&apos;ALTER TABLE `&apos;, TABLE_NAME, &apos;` &apos;, &apos;MODIFY COLUMN `&apos;, COLUMN_NAME, &apos;` &apos;, IF(UPPER(DATA_TYPE) = &apos;INT&apos;, REPLACE(SUBSTRING_INDEX(UPPER(COLUMN_TYPE), &apos;)&apos;, 1), &apos;INT&apos;, &apos;INTEGER&apos;), UPPER(COLUMN_TYPE)), &apos;) UNSIGNED NOT NULL;&apos;) FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = &apos;dfh_data&apos; AND EXTRA = UPPER(&apos;AUTO_INCREMENT&apos;) ORDER BY TABLE_NAME ASC; 打印没有主键的表 1SELECT DISTINCT t.table_schema, t.table_name FROM information_schema.tables AS t LEFT JOIN information_schema.columns AS c ON t.table_schema = c.table_schema AND t.table_name = c.table_name AND c.column_key = &quot;PRI&quot; WHERE t.table_schema NOT IN (&apos;information_schema&apos;, &apos;mysql&apos;, &apos;performance_schema&apos;) AND c.table_name IS NULL AND t.table_type != &apos;VIEW&apos;;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks代理]]></title>
    <url>%2F2018%2F04%2F28%2Flinux-shadowsocket%2F</url>
    <content type="text"><![CDATA[proxychain 安装proxychain， 也可以直接使用apt安装 123456git clone https://github.com/rofl0r/proxychains-ng.gitcd proxychains-ng./configuremake &amp;&amp; make installcp ./src/proxychains.conf /etc/proxychains.confcd .. &amp;&amp; rm -rf proxychains-ng 配置proxychain 123sudo vi /etc/proxychains.conf# 将socks4 127.0.0.1 9095改为socks5 127.0.0.1 1080 //1080改为你自己的端口 使用proxychain 1proxychains chrome #在需要进行代理的程序前加proxychains 确保本地的代理已经打开 shadowsocks 安装shadowsocks, apt直接安装可能版本太低无法支持更多的加密算法 12sudo apt-get install python-pipsudo pip install shadowsocks 配置shadowsocks 123456789101112sudo vi /etc/shadowsocks.json&#123; &quot;server&quot;:&quot;服务器地址&quot;, &quot;server_port&quot;:服务器端口号, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;密码&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;加密方式&quot;, &quot;fast_open&quot;: true, #需要服务端开启 &quot;workers&quot;: 2&#125; 启动shadowsocks， 可以直接配置成开机启动启动 1sudo sslocal -c /etc/shadowsocks.json 开启服务端开启fast_open 1https://github.com/shadowsocks/shadowsocks/wiki/TCP-Fast-Open SwitchyOmega 安装SwitchyOmega， chrome如果没有代理可能无法访问商店， 可以开启shadowsocks并使用proxychain代理chrome再安装 配置SwitchyOmega 1https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList shadowsocks服务端搭建 建议使用搬瓦工， 并选择手机运营商对应的直连机房 安装步骤： 安装Centos 7 x86_64 bbr 使用谷歌网络优化技术 安装shadowsocks1234yum install wgetwget — no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.shchmod +x shadowsocks.sh./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 优化,参照https://github.com/shadowsocks/shadowsocks/wiki/TCP-Fast-Open： 服务端优化tcp_fastopen 12345678910&#123;&quot;server&quot;:&quot;0.0.0.0&quot;,&quot;server_port&quot;:443,&quot;local_address&quot;:&quot;127.0.0.1&quot;,&quot;local_port&quot;:1080,&quot;password&quot;:&quot;xxxxxxx&quot;,&quot;timeout&quot;:300,&quot;method&quot;:&quot;chacha20&quot;,&quot;fast_open&quot;:true&#125; 服务端系统开启tcp_fastopen 12#创建文件vi /etc/sysctl.d/local.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 配置如下# max open filesfs.file-max = 51200# max read buffernet.core.rmem_max = 67108864# max write buffernet.core.wmem_max = 67108864# default read buffernet.core.rmem_default = 65536# default write buffernet.core.wmem_default = 65536# max processor input queuenet.core.netdev_max_backlog = 4096# max backlognet.core.somaxconn = 4096# resist SYN flood attacksnet.ipv4.tcp_syncookies = 1# reuse timewait sockets when safenet.ipv4.tcp_tw_reuse = 1# turn off fast timewait sockets recyclingnet.ipv4.tcp_tw_recycle = 0# short FIN timeoutnet.ipv4.tcp_fin_timeout = 30# short keepalive timenet.ipv4.tcp_keepalive_time = 1200# outbound port rangenet.ipv4.ip_local_port_range = 10000 65000# max SYN backlognet.ipv4.tcp_max_syn_backlog = 4096# max timewait sockets held by system simultaneouslynet.ipv4.tcp_max_tw_buckets = 5000# turn on TCP Fast Open on both client and server sidenet.ipv4.tcp_fastopen = 3# TCP receive buffernet.ipv4.tcp_rmem = 4096 87380 67108864# TCP write buffernet.ipv4.tcp_wmem = 4096 65536 67108864# turn on path MTU discoverynet.ipv4.tcp_mtu_probing = 1# for high-latency networknet.ipv4.tcp_congestion_control = hybla# for low-latency network, use cubic instead# net.ipv4.tcp_congestion_control = cubic 12#启动sysctl --system 客户端优化tcp_fastopen 1234567891011 &#123; &quot;server&quot;:&quot;xx.xx.xx.xx&quot;, &quot;server_port&quot;:443, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;xxxxxx&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;chacha20&quot;, &quot;fast_open&quot;:true, &quot;workers&quot;:2&#125; 客户端系统开启tcp_fastopen 1234# 当前生效echo 3 &gt; /proc/sys/net/ipv4/tcp_fastopen# 永久生效,在/etc/sysctl.conf中添加net.ipv4.tcp_fastopen = 3 全局代理设置 先需要安装genpac工具（基于gfwlist的代理自动配置(Proxy Auto-config)文件生成工具，支持自定义规则， chrome的代理也是使用这种。 123sudo pip install genpacmkdir -p ~/opt/autoproxy &amp;&amp; cd sudo pip install genpacgenpac --pac-proxy &quot;SOCKS5 127.0.0.1:1080&quot; --gfwlist-proxy=&quot;SOCKS5 127.0.0.1:1080&quot; --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output=&quot;autoproxy.pac&quot; 配置系统代理，成功之后会在该目录下生成一个autoproxy.pac文件。然后打开System Settings-&gt;Hardware-&gt;Network-&gt;Network proxy，将Method设置为Automatic，然后Configuration URL中填写autoproxy.pac文件路径 1file:///home/feng/opt/autoproxy/autoproxy.pac 注意： 安装完之后此时使用浏览器就可以进行使用全局代理了 设置命令行下所有的其它程序使用socks代理，安装一个privoxy代理工具，实现终端内socks5转换为http/https，进而将http/https请求转发给ss，实现终端内的代理 1sudo apt-get install privoxy 配置/etc/privoxy/config文件 123#保证存在下面两行存在, forward-socks5t 如果不行就改成 forward-socks5 listen-address localhost:8118forward-socks5t / 127.0.0.1:1080 . 配置/etc/profile 1234# 在末尾添加以下三行：export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export ftp_proxy=http://127.0.0.1:8118 #ftp的代理可以根据需要添加 注意： source /etc/profile 启动privoxy 1sudo privoxy --user privoxy /etc/privoxy/config 注意：之后每次修改了配置文件后，都要执行sudo service privoxy restart来重启服务]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux安装配置]]></title>
    <url>%2F2018%2F04%2F28%2Flinux-ubuntu-setup%2F</url>
    <content type="text"><![CDATA[联想r720安装ubuntu 安装的时候一定要使用网络安装， 用iphone提供网络， 安装网卡驱动 安装完成后更改显卡驱动为最新的 123sudo add-apt-repository ppa:xorg-edgers/ppa #添加ppa源sudo add-apt-repository ppa:graphics-drivers/ppa #添加ppa源sudo apt-get update #更新apt-get 然后进入：系统设置-&gt;软件和更新-&gt;附件驱动，选择更新的显卡驱动。 安装bumblebee自动管理集显和独显 设置nvidia-settings-&gt;PRIME Profiles-&gt;Intel(Power Save Mode) 安装bumblebee 和 bbswitch电源管理 1234sudo add-apt-repository ppa:bumblebee/testingsudo apt updatesudo apt install bumblebee-nvidia bumblebee primussudo apt-get install acpidump iasl dmidecode 配置/etc/bumblebee/bumblebee.conf 1234Driver=nvidiaKernelDriver=nvidia-396LibraryPath=/usr/lib/nvidia-396:/usr/lib32/nvidia-396XorgModulePath=/usr/lib/nvidia-396/xorg,/usr/lib/xorg/modules 检测安装 1optirun --status 显示Bumblebee status: Ready (3.2.1). X inactive. Discrete video card is off.就没什么问题 打开nvidia设置 1optirun -b none nvidia-settings -c :8 测试GPU性能 123sudo apt-get install phoronix-test-suitephoronix-test-suite benchmark pts/gputestoptirun phoronix-test-suite benchmark pts/gputest 测试发现使用GPU和不使用的区别就表示完全通过测试 设置desktop文件帮助网站bumlebee、csdn、arch)、FAQ 安装软件使用依赖管理工具, 安装软件更加简单 12sudo apt-get install gdebisudo gdebi netease-cloudmusic_1.0.0_amd64_ubuntu16.04.deb 也可以使用dpkg安装软件 1234567891011# 使用dpkg安装.deb格式文件，这里会提示缺少依赖文件sudo dpkg -i name.deb# 这里使用 apt-get install -f 安装上面一条指令的的依赖文件sudo apt-get install -f# 依赖文件已经安装好了，重新安装一遍sudo dpkg -i name.deb# 如果卸载 -P=--purge, -r=--removeesudo dpkg -P/-r name 安装ubuntu-restricted-extras 获取字体 1sudo apt-get install ubuntu-restricted-extras ubuntu-restricted-addons 配置快捷键 123sudo apt-get install compizconfig-settings-managersettings -&gt; keyboard -&gt; shortcuts#仅仅配置shortcuts有时候可能不生效，需要使用compizconfig来修改 最终快捷键， 保留几个常用的 12345678910Ctrl + Alt + t 打开终端Ctrl + Super + up 放大应用Ctrl + Super + down 缩小应用Super + d 关闭/显示桌面Super + l 锁住桌面Super + w 显示打开的应用Super 显示左边栏并打开搜索Super + Tab 显示左边栏并上下切换打开应用Alt [+ SHift] + [向前]向后切换应用Alt + F4 关闭应用 oh-my-zsh 安装zsh 12sudo apt-get install zsh #安装zshchsh -s /bin/zsh #不需要sudo，重启 安装oh-my-zsh 1https://github.com/robbyrussell/oh-my-zsh 安装zsh插件 123456789vi-modeautojump 需要安装软件zsh-autosuggestions https://github.com/zsh-users/zsh-autosuggestionslast-working-dir d 在终端输入d 显示最近频繁进入的路径，然后输入路径前对应的序号可快速进入对应路径history 用法：在终端输入h即可zsh-syntax-highlighting https://github.com/zsh-users/zsh-syntax-highlightingweb-search 如：google StackOverflow， 指定搜搜引擎就可以了sublime 需要安装软件， [s]st [|文件|文件夹] [管理员]打开 [当前目录|文件|文件夹] vim 更新vim到最新版本 1sudo add-apt-repository ppa:jonathonf/vim 安装vim 1https://github.com/amix/vimrc 插件快捷键讲解 12345ack.vim 内容搜索 &lt;leader&gt; + gctrlp.vim 文件搜索 &lt;leader&gt; + jbufexplorer.zip 缓存查看 &lt;leader&gt; + omru.vim 最近文件 &lt;leader&gt; + fNERD Tree 文件目录 &lt;leader&gt; + nn tmux 安装tmux 1https://github.com/gpakosz/.tmux 设置tmux 1234567set-option -g allow-rename off每一次窗口执行不同的命令窗口的名字就会发生变化， 此处是关闭自动命名set-window-option -g pane-base-index 1设置窗口pane的下标从1开始# on Linux, this requires xsel or xcliptmux_conf_copy_to_os_clipboard=false设置tmux剪切板与系统剪切板同步 使用tmux tmux使用来管理回话并提供分屏等功能的软件， 当tmux开启时， 就会启动一个tmux-server的程序，用来管理上面所有的session会话， 每个session会话中可以存在多个windows（窗口）， 每个窗口又存在多个panel（面板） tmux命令 123456789tmux + tab 列出所有tmux可以使用的命令tmux new-session -s name 创建sessiontmux list-session 查看sessiontmux attach-session name attach到某个sessiontmux kill-session name 杀死某个sessiontmux info 查看session, window, pane, 运行的进程号tmux list-keys 列出所有可以的快捷键和其运行的 tmux 命令tmux list-commands 列出所有的 tmux 命令及其参数tmux kill-server 关闭所有 session 注意 所有的上面这些命令在没进入某个session之前直接使用tmux command args来调用， 当进如了tmux之后直接在任何的panel中通过 &lt;pre&gt; + : +command args来激活命令行操作， 此时所有的命令只需要使用 command args就可以调用了 tmux 基本操作 1234567? 列出所有快捷键；按q返回d 脱离当前会话,可暂时返回Shell界面s 选择并切换会话；在同时开启了多个会话时使用: 进入命令行模式；此时可输入支持的命令，例如 kill-server 关闭所有tmux会话, rename 重命名当前session[ 复制模式，光标移动到复制内容位置，空格键开始，方向键选择复制，回车确认，q/Esc退出， 可以设置vi模式复制] 进入粘贴模式，粘贴之前复制的内容，按q/Esc退出t 显示当前的时间 tmux 窗口操作 12345678c 创建新窗口&amp; 关闭当前窗口[0-9] 数字键切换到指定窗口ctrl + h/l 左右切换窗口（安装ctrl， 并快速的按h/l）w 通过窗口列表切换窗口, 重命名当前窗口，便于识别. 修改当前窗口编号，相当于重新排序f 在所有窗口中查找关键词，便于窗口多了切换 tmux 面板操作 123456789101112131415&quot;/- 将当前面板上下分屏%/_ 将当前面板左右分屏x 关闭当前分屏! 将当前面板置于新窗口,即新建一个窗口,其中仅包含当前面板z 最大化当前所在面板ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小H/J/K/L 以1个单元格移动边缘以调整当前面板大小q 显示面板编号o 选择当前窗口中下一个面板方向键/h/j/k/l 移动光标选择对应面板&#123; 向前置换当前面板&#125; 向后置换当前面板alt+o 逆时针旋转当前窗口的面板ctrl+o 顺时针旋转当前窗口的面板 tmux 滚屏操作，由于tmux接管如果输出大量的数据无法看到上面屏幕的字， 如果此时鼠标操作关闭，那么当使用鼠标滑轮的时候显示的执行过命令的显示， 而不能屏幕翻页， 要想达到这个目的有两个办法： 12打开鼠标操作， 使用滑轮上下翻页 &lt;pre&gt; + m打开复制模式， 使用vi的翻滚策略来操作， 此时又会碰到一个问题， 如果前缀是ctrl + b那么不好往下整页翻滚， 建议使用半屏翻滚 ctrl + u / d(f) tmuxinator 安装tmuxinator 1234567891011121314https://github.com/tmuxinator/tmuxinatorsudo apt-get install ruby-fullsudo gem install tmuxinator设置编辑器， 由于tmuxinator编辑项目$EDITOR = vimtmuxinator默认的存放项目位置~/.config/tmuxinator配置zsh命令简写和自动完成 TAB键a. 在.zshrc中添加source ~/.bin/tmuxinator.zshb. 拷贝自动完成脚本cp ~/.bin/tmuxinator.fish ~/.config/fish/completions/注意：如果不存在~/.bin目录，那么自己下载到这个文件夹文件路径 https://github.com/tmuxinator/tmuxinator/tree/master/completion/ 基本命令 1234567tmuxinator new/edit/open project 创建/编辑/打开项目， 可以使用n/e/o简洁表示tmuxinator start [PROJECT] [ARGS] 启动一个项目tmuxinator stop [PROJECT] 停止一个项目tmuxinator list 显示所有项目tmuxinator delete [PROJECT1] [PROJECT2] 删除一个或者多个项目tmuxinator copy [EXISTING] [NEW] 复制一个项目tmuxinator debug [PROJECT] [ARGS] debug一个项目 示例 123456789101112131415161718192021222324252627282930# 项目名称name: sample# 项目主目录root: ~/# 所有的窗口创建之前执行的命令pre_window: echo &quot;hello world&quot;# 所有的windowswindows: # windows 1 editor, 指定名字为myeditor - editor: myeditor # 指定当前window的主目录 root: ~/projects/editor # layout 指定当前window的排版方式， 默认有5种 # 也可以自己定制， 方法在下面 layout: main-vertical # 当前windows下所有的panes， 使用layout指定的排版方式排版 panes: # 面板一， 此面板只有命令vim， 所以这样写就可以了 - vim # 面板二， 此面板存在多一个命令， 将多个命令回车添加 # 必须上一个命令执行成功才会执行下一个命令 - logs: - ssh logs@example.com - cd /var/logs - tail -f development.log # window 2， 可以看到单个窗口一个命令， 直接写就可以了 - server: bundle exec rails s # window 3， 同上 - logs: tail -f log/development.log 使用环境变量 1root: &lt;%= ENV[&quot;MY_CUSTOM_DIR&quot;] %&gt; 使用参数传递 位置参数 123tmuxinator start project foo# 此处的args[0]就是fooroot: ~/&lt;%= @args[0] %&gt; 命名参数 123tmuxinator start project workspace=workspace/todo# 此处的workspace 就是上面设置的值workspace/todoroot: ~/&lt;%= @settings[&quot;workspace&quot;] %&gt; tmux默认5中layout 12345even-horizontaleven-verticalmain-horizontalmain-verticaltiled tmux自定义layout 打印layout 12$ tmux list-windows1: bash* (4 panes) [211x47] [layout 9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125;] @3 (active) 截取layout 19a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125; 配置layout 1layout:9a0a,211x47,0,0&#123;110x47,0,0,12,100x47,111,0[100x23,111,0,13,100x23,111,24&#123;49x23,111,24,14,50x23,161,24,15&#125;]&#125; 安装fusuma控制触摸板 安装 1234sudo gpasswd -a $USER input #需要管理员权限执行，配置开机启动也可以不用sudo apt-get install libinput-toolssudo apt-get install xdotoolgem install fusuma #使用ruby安装 配置 12345#确保开启多点触控gsettings set org.gnome.desktop.peripherals.touchpad send-events enabled#创建配置文件mkdir -p ~/config/fusum &amp;&amp; touch ~/.config/fusuma/config.yml#vi ~/.config/fusuma/config.yml 1234567891011121314151617181920212223242526272829303132swipe: 3: left: command: &apos;xdotool key alt+Left&apos; right: command: &apos;xdotool key alt+Right&apos; up: command: &apos;xdotool key ctrl+t&apos; down: command: &apos;xdotool key ctrl+w&apos; 4: left: command: &apos;xdotool key super+Left&apos; right: command: &apos;xdotool key super+Right&apos; up: command: &apos;xdotool key super+a&apos; down: command: &apos;xdotool key super+s&apos;pinch: in: command: &apos;xdotool key ctrl+plus&apos; out: command: &apos;xdotool key ctrl+minus&apos;threshold: swipe: 1 pinch: 1interval: swipe: 1 pinch: 1 配置开机启动， 参考下面的shadowsocks配置 安装网易云音乐 安装 123sudo apt-get install gdebisudo gdebi netease-cloudmusic_1.0.0_amd64_ubuntu16.04.debsudo apt-get install sqlite3 配置 123netease-cloud-music.desktopExec=netease-cloud-music --no-sandbox %U 中在%U 前面添加--no-sandbox 即可解决cd ~/.cache &amp;&amp; sudo chmod -R 777 netease-cloud-music wifi驱动安装后无法启动 查看网络信息 12345678910111213$ rfkill list 0:ideapad_wlan: Wireless LAN Soft blocked: no Hard blocked:yes 表示硬件阻塞没有开启 1:ideapad_bluetooth: Bluetooth Soft blocked: no Hard blocked: yes 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes 表示软件阻塞没有安装驱动 Hard blocked: no 移除ideapad模块， 后后面的没有阻塞的模块可以启动 12345678$ modprobe -r ideapad_laptop$ rfkill list 2:phy0: Wireless LAN Soft blocked: no Hard blocked:no 3:hci0: Bluetooth Soft blocked: yes Hard blocke 配置开机启动禁止ideapad模块 12创建/etc/modprobe.d/my_blacklist.conf文件block掉ideapad模块，这样没有重启就不用手动blockblacklist ideapad_laptop 开机自动启动方式一 以systemctl创建shadowsockets开机启动为例子 1sudo vim /etc/systemd/system/shadowsocks.service 编写文件类容 123456789[Unit]Description=Shadowsocks Client ServiceAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/bin/sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json[Install]WantedBy=multi-user.target 把/home/xx/Software/ShadowsocksConfig/shadowsocks.json修改为你的shadowsocks.json路径，如：/etc/shadowsocks.json 生效配置 1sudo systemctl enable /etc/systemd/system/shadowsocks.service 方式二 直接启动Startup Applications程序， 填写需要执行的命令和参数 系统问题修复 查看dmesg日志， 并且google错误信息 1sudo dmesg 查看系统日志 系统卡死现象 12Ctrl + Alt + F1 ~ F6 进入tty1 ~ tty6模式Ctrl + Alt + F7 恢复到桌面模式 idea快捷键冲突问题 设置fcitx 网易云音乐 更改系统快捷键， 不常用的建议都去掉 安装compizconfig-settings-manager继续修改快捷键 关闭关闭tty防止ctrl + alt + F1-12的影响 1234# sudo vi /usr/share/X11/xorg.conf.d/50-novtswitch.confSection &quot;ServerFlags&quot; Option &quot;DontVTSwitch&quot; &quot;true&quot;EndSection 配置idea 博客 其它可选选项 安装字体 12345wget http://ftp.de.debian.org/debian/pool/contrib/m/msttcorefonts/ttf-mscorefonts-installer_3.7_all.deb -P ~/Downloadssudo gdebi ~/Downloads/ttf-mscorefonts-installer_3.7_all.debsudo mkfontscale （创建雅黑字体的fonts.scale文件，它用来控制字体旋转缩放）sudo mkfontdir （创建雅黑字体的fonts.dir文件，它用来控制字体粗斜体产生）sudo fc-cache -fv （建立字体缓存信息，也就是让系统认识雅黑） 12345678wget https://github.com/fangwentong/dotfiles/raw/master/ubuntu-gui/fonts/Monaco.ttfsudo mkdir -p /usr/share/fonts/customsudo mv Monaco.ttf /usr/share/fonts/customsudo chmod 744 /usr/share/fonts/custom/Monaco.ttfsudo mkfontscale #生成核心字体信息sudo mkfontdirsudo fc-cache -fv 安装crossover]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2018%2F04%2F26%2Fhadoop-map-reduce%2F</url>
    <content type="text"><![CDATA[概述 MapReduce是一个简单的计算处理框架, 它能够在一个廉价的大型集群上以一种容错的方式并行的处理TB级的数据。 一个MapRecude任务通常讲输入数据集进行分块, 并且以完全并行的方式处理map任务, 框架对map任务输出进行。排序, 并作为reduce任务的输入, 通常任务的输入和输出都存储在文件系统, 框架负责处理协调、监控并重运行失败的任务。 通常情况下计算节点和存储节点都是在同一个HDFS(如果MapReduce以HDFS作为文件系统)的datenode中， 这种配置允许框架能够有效的协调任务在已经存在数据的节点中运行，这通常在集群中对网络带宽非常友好。 MapReduce也可以通过yarn（分布式资源协调工具）来运行， 并且为每一个MapReduce任务创建一个MRAppMaster负责从yarn的ResourceManager申请协调资源和对MapReduce任务进行监控和管理。 最小的一个MapReduce任务由一个输入输出、实现特定接口的map和reduce方法、以及一个job配置， job客户端提交任务， ResouceManager对任务进行协调监控并提供诊断信息给客户端。 虽然Hadoop框架是使用java运行， 但是MapReduce应用却不要求一定使用java编写， 如Hadoop Streaming允许用户使用任何可执行的shell（能够在shell命令中执行） 创建并运行mapper或者reducer， Hadoop Pipes以一个SWIG兼容的C++ API来实现MapReduce任务（而不是基于JNI） 输入和输出 MapReduce框架操作唯一的&lt;key, value&gt;对，框架输入&lt;key, value&gt;对， 并产生&lt;key, value&gt;对作为输出，key和value需要在框架中通过RPC或者文件的方式进行传输， 所以必须序列化， 框架提供Writable接口作为序列化的基础， 另外框架需要通过key来对&lt;key, value&gt;对进行排序， 所以必须实现WritableComparable接口， 虽然key没有要求必须实现hashcode方法， 但是map任务后的reduce任务通常需要对&lt;key, value&gt;行partition， 而通过hash来进行partition却是默认的方式。 简单的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; //创建单实例的对象， 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; //输入的key其实是一个相对于文件的偏移量， value为每一行的数据 //所以读取文件必须有一个格式， 并且讲文件split给每个map任务，这里由InputFormat提供 StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one);//将split的单词和数量输出&lt;workd, count&gt; &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, "word count"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); //指定Combiner进行本地Reducer任务执行， 主要是这里的reducer任务具有操作无序性质， 所以 //本地的reducer一般用来减小mapper到reducer的带宽， 对于速度上可能并不明显 job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //等待任务完成， 返回job客户端， 也可以执行返回 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 运行应用1$ bin/hadoop jar wc.jar WordCount /user/joe/wordcount/input /user/joe/wordcount/output 为MapReduce任务提供ClassPath和work dir提供资源1$ bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -files dir1/dict.txt#dict1,dir2/dict.txt#dict2 -archives mytar.tgz#tgzdir input output 说明： -files 提供文件到任务work dir中， 通过逗号分割， 使用#号取别名, -archives 提供文件并且解压缩到work dir， 通过都好分割， 使用#号取别名， 两者指定的文件可以在MapReduce任务中获取 Mapper Hadoop MapReduce通过InputFormat将输入的files进行split并创建一任意个InputSplit，并为每个InputSplit创建一个map任务用来执行这个InputSplit表示的逻辑文件， map任务通过RecordReader来读取InputSplit这个逻辑文件， 并生成map的输入对&lt;key1, value1&gt;, InputSplit和RecordReader都是由InputFormat创建。 Mapper类是执行map任务的核心， 它循环的执行RecordReader产生的&lt;key1, value1&gt;对， 并且在执行任何的&lt;key1, value1&gt;对之前可以对Mapper进行初始化操作，如下： 1234Mapper.setup(Context)Mapper.run(Context) # 用于提供map操作的模板方法， Mapper调用的方法Mapper.map(key, value, Context)Mapper.cleanup(Context) MapReduce 输出对不需要和输入对有任何的联系， 输入对可以map出零个或者多个输出对， 通过context.write()输出。 用户可以指定combiner对mapper产生的输出进行聚合， 这样可能会加快速度和减少网络带宽， 其实combiner就是一个reducer， 只是它单纯的对当前map任务的所有输出进行reduce任务操作， 操作完成之后再传给总的reduce来进行处理， 所以可以看到此处的reduce任务必须具有无序性， 其次reducer也可以接受并非只是mapper的中间结果&lt;key2, value2&gt;, 也可以接受&lt;key2, [value2, vlaue3…]&gt; map任务的个数通常是通过输入文件的总大小，也就是总的输入文件的块， 正确的并发map任务通常每个节点10-100个， 通常对于轻量级cpu任务开启300个map任务， 因为一般情况下map需要从磁盘获取数据， 并进行简单操作， 所以通常都是io密集型而对cpu不敏感， map任务的创建需要时间， 所以最好map任务执行超过一分钟。 Partitioner Partitioner是用map产生的中间结果&lt;key1, value1&gt;的key空间来对每个map任务的的中间结果进行分区的， 通常情况下是通过hash来进行分区， 总的分区数量和reduce任务的分区数量是一样的。 ReducerReducer 用来reduce Mapper任务产生的中间结果， 当Mapper产生结果后， 通过Partitioner将具体&lt;key2, value2&gt;或者是&lt;key2, [vlaue2, value3…]&gt;(经过combiner之后的结果)传输到具体的Reducer当中， 具体的某个Reducer收到所有Mapper需要传输到自己的&lt;key2, value2/[value2, value3…]&gt;之后， 对其进行shuffle、sort和最终的reduce Shuffle 讲所有从Mapper中获取的结果进行shuffle成&lt;value2, [value2, value3…]&gt; Sort 对shuffle结果进行排序，shuffle和sort是同时进行的， 排序方式可以通过Job.setSortComparatorClass(Class)来提供， 此处是对key进行排序 Secondary Sort 如果需要对相同key的values进行排序可以指定Job.setGroupingComparatorClass(Class)来对value进行排序 Reduce 对当前Reducer收到并处理的&lt;key2, [value2, value3…]&gt;进行reduce操作， 用来产生&lt;key3, value3&gt;输出到文件系统中，reduce输出没有排序。reduce在操作之前和之后可以和map进行相同的初始化或者结束回调。 reduce的数量通常在[0.95~1.75] 之间， 当maps任务执行完后， 指定倍数为0.95时所有的reducers可以立即启动并且执行传输map的输出， 当为1.75时最快的节点将会完成它们第一轮并且执行第二轮， 有助于负载均衡。 增加的reduces的数量虽然增加了负载， 但是增加了负载均衡的能力， 并且减小了失败造成的消耗。 CounterCounter 是一个用来报告统计的基础设施， Mapper和Reducer实现可以使用Counter来报告统计信息。 Job Configuration Job代表一个MapReduce的任务配置， 通常用来配置InputFormat、Mapper、combiner（如果有）、Partitioner、Reducer、OutputFormat的实现 FileInputFormat和FileOutpuFormat配置输入和输出格式 12FileInputFormat.setInputPaths(Job, Path…) FileInputFormat.addInputPath(Job, Path)FileInputFormat.setInputPaths(Job, String…) FileInputFormat.addInputPaths(Job, String) Job通常需要指定先进的参数， 如Comparator、文件put到DistributedCache、是否使用compressed、是否speculative执行以及map和reduce最大的attempts 当然Job还可以使用Configuration的get/set来获取和设置更多的参数1Configuration.set(String, String) 任务执行和环境变量 MRAppMaster使用隔离jvm的一个子进程去执行Mapper/Reducer任务， 子进程继承父进程的环境变量， 用户可以指定额外的子进程虚拟机启动参数，如下：12345678910111213141516&lt;!-- @taskid@ 表示MapReduce任务id--&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; 任务提交和监控 Job是一个原始的接口， 用于用户job与ResourceManager交互， Job提供的基本功能是提交任务、跟踪任务进程、访问任务的报告和日志和获取MapRecude集群状态等信息， job提交任务涉及到下面几个方面： 检查任务的输入输出 计算InputFormat的InputSplit 为任务分布式缓存设置必要的账号信息 复制任务jar和配置信息到MapReduce系统目录 提交任务到ResourceManager并选择是否监控它的状态 任务历史日志记录在两个指定的目录当中， 如下： 12mapreduce.jobhistory.intermediate-done-dirmapreduce.jobhistory.done-dir 用户可以查看任务运行历史日志总览 1$ mapred job -history [all] output.jhist Job 输入 InputFormat 描述MapReduce任务的输入，主要功能如下： 验证任务输入 split 所有的输入文件为逻辑的InputSplit实例， 每一个InputSplit相当于一个Mapper任务 提供RecordReader实现， 用于Mapper任务启动时从InputSplit中获取单个&lt;key1, value1&gt;记录 默认基于文件的输入是FileInputFortmat， 它通过总文件大小来split出逻辑的InputSplit， 显然通过总大小来分割有时候是无效的， 因为RecordReader无法获取记录的边界 Job输出 OutputFormat 描述MapReduce任务的输出， 主要功能如下： 验证任务输出， 如：检查输出文件是否存在 提供RecorderWriter实现， 用于写入任务输出， Output文件存储到文件系统 OutputCommiter 描述MapReduce任务输出提交， OutputCommiter的主要功能如下： 初始化设置， 如：为任务创建临时输出目录， 任务的初始化设置是在单独的任务中完成的， 初始化之前任务处在PREP状态， 初始化完成后处在RUNNING状态 任务完成后的清理动作， 如：删除零时输出目录， 任务的清理工作也是在单独的任务中， 清理工作完成后任务可能出现SUCCEDED/FAILED/KILLED状态 检查任务是否需要提交 一旦任务完成就提交任务的输出 如果任务失败， 任务输出将会被清理， 并且丢弃任务提交， 如果任务不能清理， 一个相同attempt-id的任务将会单独启动并执行清理动作 RecordWriter 负责讲Reducer的输出&lt;key3, value3&gt;输出到文件系统， 并且负责讲job的outputs输出到文件系统 提交Job到队列 用户提交任务到队列当中， 队列作为一个任务集合允许系统提供指定的功能， 如队列使用ACLs控制那个用户可以提交任务到队列， 在Hadoop中默认就是使用队列作为调度器来调度任务。 Hadoop中存在一个叫做default的队列用于用户默认的任务提交队列， 用户也可以通过Configuration.set(MRJobConfig.QUEUE_NAME, String)指定将任务提交到特定名字的队列当中， 并且Hadoop还提供了一个可插拔的Capacity Scheduler用来支持多租户， 提供资源的隔离， 所以它存在多个队列。 计数器 Counters代表全局的计数器， 可以被MapReduce框架或者应用定义， 每一个Counter可以是一个Enum， 特定的Enum的Counter被集中到特定的Counters.Group当中， 应用可以定义任意的Enum并且Counter.incrCounter(Enum, long)或者Counter.incrCounter(String, String, long)到map或者reduce方法当中， 这些counters被框架自动聚合。 分布式缓存 分布式的缓存用于有效的存储特定应用大型只读的文件， 它是被MapReduce框架提供的基础设施用于缓存应用需要的文本、归档和jar文件等等 Job中的应用通过hdfs://指定被缓的文件， 分布式的缓存假设被指定hdfs://的文件已经存在于文件系统当中 框架将会在任何任务被执行之前，复制必须的文件到worker节点， 事实上它是相当有效的， 文件只会被复制一次， 并且un-archive到work节点 分布式的缓存跟踪缓存文件的修改时间戳， 显然分布式的缓存文件是不能被任何的修改 文件、归档、jar/native lib都可以被缓存， 可以通过property设置， 或者通过Job添加， 如：12Job.addCacheFile(URI) / Job.addCacheArchive(URI)Job.AddArchiveToClassPath(Path) / JOb.addFileToClassPath(Path) URI: hdfs://host:port/absolute-path#link-name 分布式的缓存文件可以是私有或公用， 这决定了它们是怎样被worker节点所共享的， 主要是通过分布式文件的用户和文件的可见性来决定 #### Debugging MapReduce框架提供当任务失败时执行用户指定脚本的功能， 脚本可以访问任务的标准输出、标准错误、系统日志和任务配置， 脚本处理访问的信息并输出错误信息到控制台或者任务UI Debugging脚本的使用步骤： 提交脚本到分布式缓存中 提交脚本到map或者reduce当中， 使用property或者Configuration 脚本访问 $script $stdout $stderr $syslog $jobconf 数据压缩 MapReduce提供map和reduce应用输出压缩功能， 为了性能它使用本地实现 跳过错误的记录 Hadoop可以通过ShipBadRecords类来控制map输入的错误记录， 对于某些应用来说如果少量的错误记录是可以接受的那么可以开启此功能， 使用SkipBadRecords.set[Mapper|Reducer]MaxSkipGroups(Configuration, long)来设置最大的错误记录数量。 复杂的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Counter;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.StringUtils;public class WordCount2 &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; // 提供counter功能 static enum CountersEnum &#123; INPUT_WORDS &#125; // 减少对象的创建 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); // 提供单词大小写敏感和模式忽略功能 private boolean caseSensitive; private Set&lt;String&gt; patternsToSkip = new HashSet&lt;String&gt;(); private Configuration conf; private BufferedReader fis; // 在setup中处理Mapper的初始化动作 @Override public void setup(Context context) throws IOException, InterruptedException &#123; // 获取配置 conf = context.getConfiguration(); // 获取Job中设置的环境变量 caseSensitive = conf.getBoolean("wordcount.case.sensitive", true); // 解析配置 if (conf.getBoolean("wordcount.skip.patterns", false)) &#123; //获取并解析缓存 URI[] patternsURIs = Job.getInstance(conf).getCacheFiles(); for (URI patternsURI : patternsURIs) &#123; Path patternsPath = new Path(patternsURI.getPath()); String patternsFileName = patternsPath.getName().toString(); parseSkipFile(patternsFileName); &#125; &#125; &#125; private void parseSkipFile(String fileName) &#123; try &#123; fis = new BufferedReader(new FileReader(fileName)); String pattern = null; while ((pattern = fis.readLine()) != null) &#123; patternsToSkip.add(pattern); &#125; &#125; catch (IOException ioe) &#123; System.err.println("Caught exception while parsing the cached file '" + StringUtils.stringifyException(ioe)); &#125; &#125; @Override public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; // 处理单词大小写敏感和模式忽略功能 String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase(); for (String pattern : patternsToSkip) &#123; line = line.replaceAll(pattern, ""); &#125; StringTokenizer itr = new StringTokenizer(line); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); // counter计数， 会在日志中打印 Counter counter = context.getCounter(CountersEnum.class.getName(), CountersEnum.INPUT_WORDS.toString()); counter.increment(1); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 配置解析 Configuration conf = new Configuration(); GenericOptionsParser optionParser = new GenericOptionsParser(conf, args); String[] remainingArgs = optionParser.getRemainingArgs(); if ((remainingArgs.length != 2) &amp;&amp; (remainingArgs.length != 4)) &#123; System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt; [-skip skipPatternFile]"); System.exit(2); &#125; Job job = Job.getInstance(conf, "word count"); // 基本设置 job.setJarByClass(WordCount2.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); List&lt;String&gt; otherArgs = new ArrayList&lt;String&gt;(); for (int i=0; i &lt; remainingArgs.length; ++i) &#123; if ("-skip".equals(remainingArgs[i])) &#123; // 设置缓存 job.addCacheFile(new Path(remainingArgs[++i]).toUri()); // 设置环境变量 job.getConfiguration().setBoolean("wordcount.skip.patterns", true); &#125; else &#123; otherArgs.add(remainingArgs[i]); &#125; &#125; // 设置输入与输出 FileInputFormat.addInputPath(job, new Path(otherArgs.get(0))); FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1))); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 命令行执行123456789101112131415161718192021222324$ bin/hadoop fs -ls /user/joe/wordcount/input//user/joe/wordcount/input/file01/user/joe/wordcount/input/file02$ bin/hadoop fs -cat /user/joe/wordcount/input/file01Hello World, Bye World!$ bin/hadoop fs -cat /user/joe/wordcount/input/file02Hello Hadoop, Goodbye to hadoop.$ bin/hadoop fs -cat /user/joe/wordcount/patterns.txt\.\,\!to$ bin/hadoop jar wc.jar WordCount2 -Dwordcount.case.sensitive=false /user/joe/wordcount/input /user/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt$ bin/hadoop fs -cat /user/joe/wordcount/output/part-r-00000bye 1goodbye 1hadoop 2hello 2horld 2]]></content>
      <categories>
        <category>hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2F2018%2F04%2F26%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[原理 docker 每一条命令都会创建一层， 并且会使用上一层的镜像运行一个docker容器， 再执行本层的命令，执行完成之后就提交停止容器， 并提交当前容器的修改为一个镜像 当容器开始构建的时候， 第一步会将指定的context目录下的所有的文件都传递给docker守护进程（除了.dockerignore文件标识的文件除外), 第二步启动FROM image指定的镜像的容器， 并执行下面一条命令，关闭当前容器并提交一层镜像， 以后各层都是如此，以上一层的镜像运行一个容器并执行命令后提交镜像所以我们可以看到命令越多则层数越多， 建议尽量使用最少的层， 并且将相关的动作描述清晰使其容易维护 注意 妥善使用下面点： 基础镜像选择， 使用两种环境变量， 组合命令 设置基础镜像1FROM ubuntu 设置LABELLABEL用来标示当前镜像， 作为一种元数据存在与镜像当中， 作为一种描述123LABEL com.ilivoo.game=&quot;fengxiang&quot; \ version=&quot;1.0&quot; \ description=&quot;this is my first game!&quot; 设置环境变量设置docker容器运行时的环境变量， 并且可以在构建的过程中使用这些命令， 其实道理很简单， 以后的构建过程中都是使用上一个镜像层作为基础， 而运行的时候启动容器这个环境变量就已经生成了， 所以后面的RUN 等命令当然也是可以获取到这个环境变量的， 所以脚本引用方式与正查的脚本完全一样, 可以看到巧妙的设计环境变量可以使得构建过程非常灵活， 并且运行时容器也非常灵活12ENV VERSION=1.0 \ DEBUG=on 设置ARG参数ARG命令也是用来设置环境变量的， 但是ARG设置的环境变量不会保留到最终容器运行的层当中, 其实实现也非常简单当运行容器的时候设置环境变量， 并且在容器退出的时候就将环境变量删除就可以达到此目的了, 并且此参数可以通过–build-arg参数在构建的时候进行覆盖，这样就可以达到非常灵活的效果了虽然ARG环境变量不会保存到运行是环境变量， 当时也不能使用铭感数据， 因为使用history会查看到其值1ARG REDIS_VERSION=3.2.10 环境变量与ARG参数结合使用 通过在构建的时候覆盖当前–build-arg NAME=FENGXIANG此时后面设置的环境变量FENG就变成了FENGXIANG, 可以看到虽然环境变量不能通过参数的形式进行改变但是这里确提供了灵活的方式来达到环境变量的不同， 所以通过脚本来构建就可以达到非常灵活的效果， 所以在我们的代码当中需要进行灵活变化的变量应该使用环境变量来进行设置， 这样就可以达到任何代码都不需要更改确有一个灵活的生产 测试 线上环境， 这个非常重要 12ARG NAME=XIANGENV FENG=$&#123;NAME&#125; 建议将那些需要进行覆盖的ARG使用这种方式给出， 并且在使用的时候通过字符串获取的方式来获得， 此处标示如果COMPANY不存在那么默认值就是ilivoo通过这种方式能够非常明显标示当前ARG用来进行占位的， 真正使用的时候可以进行指定， 并且最好是通过注释的方式标示可以指定的值 12ARG COMPANYENV COMPANY $&#123;COMPANY:-ilivoo&#125; 指定工作目录指定docker构建的以后的各层当中都使用指定的这个目录作为工作目录， 也就是以后命令执行的当前目录都是这个目录，如：CMD [“./start.sh”], 这个命令默认就去查找这个目录下对应的脚本， 如果不存在则报错， 并且每次设置都会更改以后的所有层， 可以覆盖。 所以建议再构建镜像的时候只有在最终执行命令的时候再去指定这个目录， 构建最终层镜像的时候显示的指定每个工作的目录是一个非常不错的选择1WORKDIR /app 指定执行用户USER和WORKDIR命令类似， 都是改变环境状态并影响以后的层， 而USER则是更改RUN CMD ENTRYPOINT这类动作命的身份， 用户必须事先建立好。如果以root执行脚本， 在执行容器的时候希望使用特定的身份执行容器， 不建议使用sudo命令， 这样会非常麻烦，而且容易出错，建议使用gosu来控制， 借鉴redis的做法。1USER root ADD命令当使用ADD添加本地文件时， 如果是单个文件才会进行解压缩(如果多个文件不会解压缩)如： ADD hello.tar.gz /app如果原路径是一个URL那么docker会试图区下载这个文件， 下载后权限自动设置为600，并且不会解压文件，如果需要更改还需要添加可以看到ADD命令其实限制非常多， 并且语义也并不明确， 如果需要从网络下载还不如使用RUN命令，再使用curl gzip chmod等一次更改当前修改， 这样只需要创建一层就可以完成了。所以在项目中尽量不要使用这个命令来添加文件， 仅当需要解压时才使用， 如将所有的项目文件压缩为一个zip包， 再通过一条ADD命令将其添加到项目目录当中12ADD http://download.redis.io/releases/redis-$&#123;REDIS_VERSION&#125;.tar.gz /appADD hello.tar.gz /app COPY命令注意docker构建的context， 也就是docker构建的上下文再docker构建的过程中是没有当前目录的概念的，比如：不能指定../clear.sh， 因为docker引擎不知道这个目录是哪里，所以获取不到这个文件当使用docker build进行构建的时候， 会设置Dockerfile和Context上下文对于的路径， 如果没有设置Dockerfile的位置， 那么默认会在context中查找Dockerfile, 如果没有那么构建失败而当指定Context的时候，docker客户端会将context指定的目录下所有的文件都传送给docker引擎， docker引擎再根据dockerfile中指定的需要的文件从context中去查找， 如果找不到构建失败由此可见Dockerfile中指定的原路径都是指定当前docker引擎的context路径docker引擎每向下构建一层需要的依赖文件就从docker引擎的context路径传送到启动的docker容器当中， 再提交容器生成一层镜像1COPY . /app RUN命令在构建的过程中运行当前命令， 存在两种形式shell格式和exec形式, shell格式就像直接运行shell命令一样执行 而exec格式则是只按照当前给出的命令和参数的形式进行执行此处必须写成./start.sh， 而不能直接写成start.sh， 因为在shell下直接写成start.sh也是不能运行的RUN ./start.sh而此处exec格式，必须添加上执行的shell1RUN [&quot;/bin/bash&quot;, &quot;start.sh&quot;] CMD命令指定容器启动的主进程启动命令， 也就是操作系统挂载文件系统之后启动的init进程， 而此处的主进程就是类似init进程, 当且仅当主进程推出那么此时容器内所有的进程都退出， 并且我们知道主进程init是所有的进程的父进程，当产生僵尸进程的时候当父进程死后，而子进程没有死那么为了防止僵尸进程，此时就需要init进程来寄养子进程。这里完全和linux操作系统是一样的。在使用shell格式的时候， 系统会默认为其添加sh -c的参数形式执行容器在其启动的时候可以重新覆盖CMD命令，从而启动其他的命令1CMD echo $HOME ENTRYPOINT命令ENTRYPOINT, 当使用了ENTRYPOINT之前的CMD和ENTRYPOINT就被覆盖， 而定义在ENTRYPOINT后面的CMD则作为ENTRYPOINT的参数, 而且在容器运行的时候还可以覆盖CMD参数， 这样就可以达到一种默认命令， 并且可以更改参数的用途， 这样某些场景会非常有用， 比如redis的启动， ENTRYPOINT指定启动脚本， 并且在真正的主进程启动之间进行一些环境的修改， 如执行用户等等，并且可以设置参数。 第二可以把容器当作命令来执行， 并且可以为其指定参数。容器启动的时候也可以使用 –entrypoint 来覆盖这里的entrypoint命令12ENTRYPOINT [&quot;/bin/bash&quot;, &quot;start.sh&quot;]CMD [&quot;3&quot;] 存储卷VOLUME定义匿名卷， 容器运行时应该尽量（必须）保证容器存储层不发生写操作， 任何的数据要么保存到卷当中， 要么通过数据库服务保存在其它的地方为了防止用户忘记运行时将需要使用的目录挂载为卷， 我们可以在Dockerfile中定义匿名的卷， 如果用户不挂载那么运行是容器自动挂载当前卷， 使用docker守护进程在本地创建一个目录作为此匿名卷的挂载目录， 这样就可以保证容器的存储层不会被写入了. 所以任何时候我们都应该为一个需要写入数据的容器声明卷， 而在docker中， 最终这个卷被挂载到具体那个位置是未知的，docker提供了一个灵活的处理方式， docker的卷可以使用任何第三方实现的插件， 将其保存在分布式文件系统或者云或者本地系统等等， 这样卷就可以作为一种资源来供容器使用了， 这也swarm kubernetes等工具的实现方式。运行时挂载 -v mydata:/data12VOLUME /dataVOLUME [&quot;/pass1&quot;, &quot;/pass2&quot;] 端口映射EXPOSE 声明运行时容器提供服务的端口， 仅仅只是一个声明而已， 运行是并不会因为这个声明就会开启这个端口的服务， 其实容器也是完全做不到这一点的， 主要有两个意图， 第一帮助镜像使用者理解这个镜像的服务的守护端口， 方便配置映射， 另一个用户则是运行时使用随机端口映射， 也就是docker run -P时， 会自动随机映射EXPOSE的端口， 不管当前端口是否有服务在监听1EXPOSE 9092]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 使用教程]]></title>
    <url>%2F2018%2F04%2F25%2Fhexo%2F</url>
    <content type="text"><![CDATA[常见问题分类和标签统计错误1234cd $HEXO_HOMErm db.jsonhexo cleanhexo generate markdown转义12345678910111213141516171819202122! &amp;#33; — 惊叹号 Exclamation mark” &amp;#34; &amp;quot; 双引号 Quotation mark# &amp;#35; — 数字标志 Number sign$ &amp;#36; — 美元标志 Dollar sign% &amp;#37; — 百分号 Percent sign&amp; &amp;#38; &amp;amp; Ampersand‘ &amp;#39; — 单引号 Apostrophe( &amp;#40; — 小括号左边部分 Left parenthesis) &amp;#41; — 小括号右边部分 Right parenthesis* &amp;#42; — 星号 Asterisk+ &amp;#43; — 加号 Plus sign&lt; &amp;#60; &amp;lt; 小于号 Less than= &amp;#61; — 等于符号 Equals sign&gt; &amp;#62; &amp;gt; 大于号 Greater than? &amp;#63; — 问号 Question mark@ &amp;#64; — Commercial at[ &amp;#91; --- 中括号左边部分 Left square bracket\ &amp;#92; --- 反斜杠 Reverse solidus (backslash)] &amp;#93; — 中括号右边部分 Right square bracket&#123; &amp;#123; — 大括号左边部分 Left curly brace| &amp;#124; — 竖线Vertical bar&#125; &amp;#125; — 大括号右边部分 Right curly brace]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 正则表达式]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-re%2F</url>
    <content type="text"><![CDATA[定义 正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 正则表达式的强大之处在于引入特殊字符来定义字符集、字符集位置、匹配子组、重复模式和扩展表示法。 字符集定义: 符号 描述 正则表达式 匹配案例 literal 匹配字符串的字面值 foo foo re1&#124;re2 匹配正则表达式re1或者re2 foo&#124;bar foo或者bar . 匹配任何字符(除了\n之外) b.b bab […] 匹配来自字符集的任意单个字符 [aeiou] a [^…] 匹配非来自字符集的任何单个字符 [^aeiou] 0 [x-ym-n] 匹配任意来自x~y和m~n范围内的单个字符 [A-Za-z] b \d 匹配任何十进制数字，与[0-9]相同(与\D相反) data\d.txt data1.txt \w 匹配字母数字数字字符,与[A-Za-z0-9]相同(与\w相反) data\w.txt dataa.txt \s 匹配任何空格字符, 与[\n\t\r\v\f]相同(与\S相反) of\sthe of the \x 对特殊字符进行转移 \. . 字符集位置： 符号 描述 正则表达式 匹配 ^ 匹配字符串起始位置 ^Dear Dear $ 匹配字符串终止位置 /bin/*sh$ /bin/bash \A(\Z) 匹配字符串起始(结束)位置 \ADear Dear \b 匹配任何单词边界(与\S相反) \bthe\b the 重复模式： 符号 描述 正则表达式 匹配 * 匹配０次或者多次 [A-Za-z0-9]* abcde + 匹配１次或者多次 [a-z]+.com baidu.com ? 匹配０次或者多次 goo? go {N} 精确匹配Ｎ次 [0-9]3 333 {M,N} 匹配M~N次 [0-9]{5,9} 123456 (*&#124;+&#124;?&#124;{}) 匹配上面重复出现的非贪婪版本 .*?[a-z] abcc 匹配子组： 符号 描述 正则表达式 匹配 (…) 匹配封闭的正则表达式， 然后另存为子组 ([0-9]{3}-)?[0-9]{7-8} 020-888888 \N 配皮上面已经保存的分组 ([0-9]{3})-(\d{7-8}) post is \1 020-12345678 post is 020 扩展表示法： 符号 描述 正则表达式 匹配 贪婪模式正则表达式的默认为贪婪模式，表示竟可能多的匹配， 非贪婪是如果后面正则表达式能够匹配则尽可能少的匹配， 给后面表达式匹配的机会， 如：1234&gt;&gt;&gt; re.match(r&apos;^(\d+)(0*)$&apos;, &apos;102300&apos;).groups() #贪婪(&apos;102300&apos;, &apos;&apos;)&gt;&gt;&gt; re.match(r&apos;^(\d+?)(0*)$&apos;, &apos;102300&apos;).groups() #非贪婪(&apos;1023&apos;, &apos;00&apos;) re 模块123456789101112131415161718192021基础方法compile(pattern, flags = 0) 用任何可选的标记来编译正则表达式，然后返回正则表达式对象match(pattern, string, flags = 0) 尝试使用带可选标记的正则表达式的模式来匹配字符串，如果匹配成功返回匹配对象，否则返回Nonesearch(patter, string, flags = 0) 使用可选的标记搜索字符串中第一次出现正则表达式模式，如果匹配成功返回匹配对象，否则返回None，search是逐个逐个字符的往下匹配findall/finditer(pattern, string[,flags]) 查找字符串中所有(非重复)出现的正则表达式模式，并返回匹配列表split(pattern, string, max = 0) 使用正则表达式分割模式分割字符串，并返回最大max次操作的列表sub(pattern, repl, string, count = 0) 使用repl替换正则表达式的模式在字符串中出现的位置，除非定义count，否则替换所有的位置purge() 清楚隐式编译的正则表达式模式标记re.I | re.IGNORECASE 不区分大小写匹配re.L | re.Locale 根据所使用的本地语言环境通过\w、\W、\b、\B、\s、\S实现匹配re.M | re.MULTILINE ^和$分别匹配目标字符串中行的起始和结束，而不是严格匹配整个字符串本身的起始和结尾re.S | re.DOTALL .号可以匹配所有的字符，包括\nre.X | re.VERBOSE 所有的空格加上#都会被忽略re.T | re.TEMPLATE ...匹配对象的方法group(num = 0) 返回整个匹配对象或者编号为num的特定子组groups(default = None) 返回包含所有匹配子组的元组groupdict(defalt = None) 返回包含所有匹配子组的字典，子组名称作为键]]></content>
      <categories>
        <category>python</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 帮助命令]]></title>
    <url>%2F2018%2F04%2F25%2Fpython-help%2F</url>
    <content type="text"><![CDATA[获取内建信息123dir() 打印当前scopedir(__builtins__) 打印内建scopedir(object) 打印对象scope 获取帮助1help(object) 获取类继承层次1Class.mro() 此方法是type中的方法， 每个Class都是type的实例 获取所有的模块1help('modules') 判断对象类型123456789&gt;&gt;&gt; import types&gt;&gt;&gt; type('abc')==types.StringTypeTrue&gt;&gt;&gt; type(u'abc')==types.UnicodeTypeTrue&gt;&gt;&gt; type([])==types.ListTypeTrue&gt;&gt;&gt; type(str)==types.TypeTypeTrue 获取对象中的常量1234567891011121314151617181920212223242526272829303132import typesbase_type = (types.BooleanType, types.FloatType, types.IntType, types.LongType, types.NoneType,types.StringType, types.TupleType, types.ListType, types.DictType, types.ComplexType)def dir_ck(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value print sorted(_constant.keys())def dir_ok(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value print sorted(_object.keys())def dir_c(obj): _constant = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) in base_type: _constant[key] = value for key in sorted(_constant.keys()): print key + "\t = \t" + _constant.get(key)def dir_o(obj): _object = &#123;&#125; for key, value in obj.__dict__.iteritems(): if type(value) not in base_type: _object[key] = value for key in sorted(_object.keys()): print key + "\t = \t" + str(_object.get(key)) 查找对象中的方法12def dir_f(obj, key): print filter(lambda x: x.lower().find(key) != -1, dir(obj))]]></content>
      <categories>
        <category>python</category>
        <category>帮助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
